"""
ğŸ¯ Real Data Signal Quality Engine - ç²¾ç¢ºæ·±åº¦åˆ†æå·¥å…·
å°ˆé–€åˆ†æ real_data_signal_quality_engine.py èˆ‡ JSON è¦ç¯„çš„100%åŒ¹é…åº¦
"""

import json
import ast
import re
from typing import Dict, List, Any, Tuple
from pathlib import Path

class RealDataPrecisionAnalyzer:
    """Real Data ä¿¡è™Ÿè³ªé‡å¼•æ“ç²¾ç¢ºåˆ†æå™¨"""
    
    def __init__(self):
        self.json_spec_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase2_pre_evaluation/real_data_signal_quality_engine/real_data_signal_quality_engine.json"
        self.python_file_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase2_pre_evaluation/real_data_signal_quality_engine/real_data_signal_quality_engine.py"
        
        self.analysis_results = {
            "compliance_issues": [],
            "missing_components": [],
            "unused_code": [],
            "logic_mismatches": [],
            "data_flow_breaks": [],
            "performance_issues": []
        }
    
    def load_specifications(self) -> Tuple[Dict, str]:
        """è¼‰å…¥ JSON è¦ç¯„å’Œ Python ä»£ç¢¼"""
        # è¼‰å…¥ JSON è¦ç¯„
        with open(self.json_spec_path, 'r', encoding='utf-8') as f:
            json_spec = json.load(f)
        
        # è¼‰å…¥ Python ä»£ç¢¼
        with open(self.python_file_path, 'r', encoding='utf-8') as f:
            python_code = f.read()
        
        return json_spec, python_code
    
    def analyze_strategy_compliance(self, json_spec: Dict, python_code: str):
        """åˆ†æç­–ç•¥è¦ç¯„ç¬¦åˆåº¦"""
        engine_spec = json_spec["real_data_signal_quality_engine_dependency"]
        
        # 1. æª¢æŸ¥ç­–ç•¥åŸºæœ¬ä¿¡æ¯
        required_version = engine_spec["version"]  # "2.1.0"
        required_role = engine_spec["role"]  # "parallel_monitoring_not_blocking_main_flow"
        
        if required_version not in python_code:
            self.analysis_results["compliance_issues"].append({
                "category": "VERSION_MISMATCH",
                "severity": "HIGH",
                "details": f"JSON è¦ç¯„ç‰ˆæœ¬ {required_version} åœ¨ä»£ç¢¼ä¸­æœªæ‰¾åˆ°"
            })
        
        if "parallel" not in python_code.lower() or "monitoring" not in python_code.lower():
            self.analysis_results["compliance_issues"].append({
                "category": "ROLE_MISMATCH", 
                "severity": "CRITICAL",
                "details": f"ä»£ç¢¼æœªé«”ç¾æ‰€éœ€è§’è‰²: {required_role}"
            })
        
        # 2. æª¢æŸ¥æ¨¡çµ„é¡å‹
        module_type = engine_spec["module_type"]  # "enhanced_quality_monitoring_engine"
        if "enhanced" not in python_code.lower() or "quality_monitoring" not in python_code.lower():
            self.analysis_results["compliance_issues"].append({
                "category": "MODULE_TYPE_MISMATCH",
                "severity": "HIGH", 
                "details": f"ä»£ç¢¼æœªé«”ç¾æ¨¡çµ„é¡å‹: {module_type}"
            })
    
    def analyze_dependency_compliance(self, json_spec: Dict, python_code: str):
        """åˆ†æä¾è³´è¦ç¯„ç¬¦åˆåº¦"""
        dependencies = json_spec["real_data_signal_quality_engine_dependency"]["strategy_dependency_graph"]["core_dependencies"]
        
        # 1. æª¢æŸ¥å¢å¼·ç›£æ§ç³»çµ±
        enhanced_monitoring = dependencies["enhanced_monitoring_systems"]
        
        # Phase1B ä¾è³´æª¢æŸ¥
        phase1b_spec = enhanced_monitoring["phase1b_volatility_adaptation"]
        if "phase1b_volatility_adaptation" not in python_code:
            self.analysis_results["missing_components"].append({
                "component": "phase1b_volatility_adaptation",
                "severity": "CRITICAL",
                "required_data_type": phase1b_spec["data_type"],
                "update_frequency": phase1b_spec["update_frequency"]
            })
        
        # Phase1C ä¾è³´æª¢æŸ¥
        phase1c_spec = enhanced_monitoring["phase1c_signal_standardization"]
        if "phase1c_signal_standardization" not in python_code:
            self.analysis_results["missing_components"].append({
                "component": "phase1c_signal_standardization", 
                "severity": "CRITICAL",
                "required_data_type": phase1c_spec["data_type"],
                "update_frequency": phase1c_spec["update_frequency"]
            })
        
        # ç³»çµ±è² è¼‰ç›£æ§æª¢æŸ¥ - é€™æ˜¯ JSON ä¸­çš„æ–°éœ€æ±‚
        system_load_spec = enhanced_monitoring.get("system_load_monitor")
        if system_load_spec:
            if "system_load" not in python_code.lower() or "cpu_usage" not in python_code.lower():
                self.analysis_results["missing_components"].append({
                    "component": "system_load_monitor",
                    "severity": "CRITICAL", 
                    "required_data_type": system_load_spec["data_type"],
                    "thresholds": system_load_spec["thresholds"],
                    "details": "JSON è¦ç¯„è¦æ±‚ç³»çµ±è² è¼‰ç›£æ§ä½†ä»£ç¢¼ä¸­ç¼ºå¤±"
                })
        
        # Phase3 ä¾è³´æª¢æŸ¥
        phase3_spec = enhanced_monitoring["phase3_market_analyzer"]
        if "phase3_market_analyzer" not in python_code:
            self.analysis_results["missing_components"].append({
                "component": "phase3_market_analyzer",
                "severity": "HIGH",
                "required_data_type": phase3_spec["data_type"],
                "update_frequency": phase3_spec["update_frequency"]
            })
        
        # pandas-ta ä¾è³´æª¢æŸ¥
        pandas_ta_spec = enhanced_monitoring["pandas_ta_indicators"] 
        if "pandas_ta" not in python_code.lower():
            self.analysis_results["missing_components"].append({
                "component": "pandas_ta_indicators",
                "severity": "HIGH", 
                "required_data_type": pandas_ta_spec["data_type"],
                "update_frequency": pandas_ta_spec["update_frequency"]
            })
    
    def analyze_enhanced_capabilities(self, json_spec: Dict, python_code: str):
        """åˆ†æå¢å¼·åŠŸèƒ½ç¬¦åˆåº¦"""
        enhanced_capabilities = json_spec["real_data_signal_quality_engine_dependency"]["strategy_dependency_graph"]["core_dependencies"]["enhanced_monitoring_capabilities"]
        
        # 1. å¾®ç•°å¸¸æª¢æ¸¬
        micro_anomaly_spec = enhanced_capabilities["micro_anomaly_detection"]
        if "micro_anomaly" not in python_code.lower() and "anomaly_detection" not in python_code.lower():
            self.analysis_results["missing_components"].append({
                "component": "micro_anomaly_detection",
                "severity": "MEDIUM",
                "required_data_type": micro_anomaly_spec["data_type"],
                "monitoring_scope": micro_anomaly_spec["monitoring_scope"],
                "details": "JSON è¦ç¯„è¦æ±‚å¾®ç•°å¸¸æª¢æ¸¬åŠŸèƒ½"
            })
        
        # 2. å»¶é²è§€å¯Ÿè¿½è¹¤
        delayed_obs_spec = enhanced_capabilities["delayed_observation_tracking"]
        if "delayed_observation" not in python_code.lower() and "performance_tracking" not in python_code.lower():
            self.analysis_results["missing_components"].append({
                "component": "delayed_observation_tracking",
                "severity": "MEDIUM", 
                "required_data_type": delayed_obs_spec["data_type"],
                "tracking_duration": delayed_obs_spec["tracking_duration"],
                "details": "JSON è¦ç¯„è¦æ±‚å»¶é²è§€å¯Ÿè¿½è¹¤åŠŸèƒ½"
            })
        
        # 3. å‹•æ…‹é–¾å€¼ç›£æ§
        dynamic_threshold_spec = enhanced_capabilities["dynamic_threshold_monitoring"]
        if "dynamic_threshold" not in python_code.lower() and "adaptive_threshold" not in python_code.lower():
            self.analysis_results["missing_components"].append({
                "component": "dynamic_threshold_monitoring",
                "severity": "MEDIUM",
                "required_data_type": dynamic_threshold_spec["data_type"],
                "update_frequency": dynamic_threshold_spec["update_frequency"],
                "details": "JSON è¦ç¯„è¦æ±‚å‹•æ…‹é–¾å€¼ç›£æ§åŠŸèƒ½"
            })
    
    def analyze_computation_flow(self, json_spec: Dict, python_code: str):
        """åˆ†æè¨ˆç®—æµç¨‹ç¬¦åˆåº¦"""
        try:
            # æª¢æŸ¥ä¸åŒå¯èƒ½çš„çµæ§‹ä½ç½®
            if "computation_flow" in json_spec:
                flow_spec = json_spec["computation_flow"]
            elif "computation_flow" in json_spec.get("real_data_signal_quality_engine_dependency", {}):
                flow_spec = json_spec["real_data_signal_quality_engine_dependency"]["computation_flow"]
            else:
                self.analysis_results["logic_mismatches"].append({
                    "category": "MISSING_COMPUTATION_FLOW",
                    "severity": "CRITICAL",
                    "details": "JSON è¦ç¯„ä¸­æ‰¾ä¸åˆ° computation_flow é…ç½®"
                })
                return
            
            processing_layers = flow_spec["processing_layers"]
        except KeyError as e:
            self.analysis_results["logic_mismatches"].append({
                "category": "JSON_STRUCTURE_ERROR",
                "severity": "CRITICAL", 
                "details": f"JSON è¦ç¯„çµæ§‹éŒ¯èª¤ï¼Œç„¡æ³•æ‰¾åˆ° computation_flow: {e}"
            })
            return
        
        # 1. æª¢æŸ¥è™•ç†å±¤
        layer_0_spec = processing_layers["layer_0_signal_intake"]
        if "signal_intake" not in python_code.lower() and "real_data_quality_validation" not in python_code.lower():
            self.analysis_results["logic_mismatches"].append({
                "layer": "layer_0_signal_intake",
                "severity": "CRITICAL",
                "expected_input": layer_0_spec["input"], 
                "expected_processing": layer_0_spec["processing"],
                "expected_output": layer_0_spec["output"],
                "expected_time": layer_0_spec["expected_processing_time"],
                "details": "Layer 0 ä¿¡è™Ÿæ¥æ”¶å’Œé©—è­‰é‚è¼¯ç¼ºå¤±"
            })
        
        layer_1_spec = processing_layers["layer_1_priority_classification"]
        if "priority_classification" not in python_code.lower() and "signal_priority_scoring" not in python_code.lower():
            self.analysis_results["logic_mismatches"].append({
                "layer": "layer_1_priority_classification",
                "severity": "CRITICAL",
                "expected_input": layer_1_spec["input"],
                "expected_processing": layer_1_spec["processing"], 
                "expected_output": layer_1_spec["output"],
                "expected_time": layer_1_spec["expected_processing_time"],
                "details": "Layer 1 å„ªå…ˆç´šåˆ†é¡é‚è¼¯ç¼ºå¤±"
            })
        
        layer_2_spec = processing_layers["layer_2_quality_control"]
        if "quality_control" not in python_code.lower() and "comprehensive_quality_assessment" not in python_code.lower():
            self.analysis_results["logic_mismatches"].append({
                "layer": "layer_2_quality_control", 
                "severity": "CRITICAL",
                "expected_input": layer_2_spec["input"],
                "expected_processing": layer_2_spec["processing"],
                "expected_output": layer_2_spec["output"],
                "expected_time": layer_2_spec["expected_processing_time"],
                "details": "Layer 2 è³ªé‡æ§åˆ¶é‚è¼¯ç¼ºå¤±"
            })
        
        # 2. æª¢æŸ¥æ€§èƒ½è¦æ±‚
        total_time = flow_spec["total_expected_processing_time"]  # "40ms (enhanced monitoring)"
        if "40ms" not in python_code and "enhanced monitoring" not in python_code.lower():
            self.analysis_results["performance_issues"].append({
                "category": "PROCESSING_TIME_COMPLIANCE",
                "severity": "HIGH",
                "expected": total_time,
                "details": "ä»£ç¢¼ä¸­æœªé«”ç¾ç¸½è™•ç†æ™‚é–“è¦æ±‚"
            })
        
        # 3. æª¢æŸ¥ä½µç™¼è¦æ±‚
        concurrency = flow_spec["concurrency_level"]  # "multi_threaded_async"
        if "multi_threaded" not in python_code.lower() and "async" not in python_code.lower():
            self.analysis_results["performance_issues"].append({
                "category": "CONCURRENCY_COMPLIANCE",
                "severity": "HIGH", 
                "expected": concurrency,
                "details": "ä»£ç¢¼æœªé«”ç¾å¤šç·šç¨‹ç•°æ­¥è™•ç†è¦æ±‚"
            })
    
    def analyze_unused_code(self, python_code: str):
        """åˆ†ææœªä½¿ç”¨çš„ä»£ç¢¼"""
        # è§£æ AST
        try:
            tree = ast.parse(python_code)
        except SyntaxError as e:
            self.analysis_results["unused_code"].append({
                "category": "SYNTAX_ERROR",
                "severity": "CRITICAL",
                "details": f"ä»£ç¢¼èªæ³•éŒ¯èª¤: {e}"
            })
            return
        
        # æ”¶é›†å®šç¾©çš„é¡å’Œå‡½æ•¸
        defined_classes = []
        defined_functions = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                defined_classes.append(node.name)
            elif isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):
                defined_functions.append(node.name)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœªä½¿ç”¨çš„å…ƒç´ 
        for class_name in defined_classes:
            if python_code.count(class_name) <= 2:  # åªå‡ºç¾åœ¨å®šç¾©è™•ï¼Œæœªè¢«ä½¿ç”¨
                self.analysis_results["unused_code"].append({
                    "category": "UNUSED_CLASS",
                    "severity": "LOW",
                    "name": class_name,
                    "details": f"é¡ {class_name} å¯èƒ½æœªè¢«ä½¿ç”¨"
                })
        
        # æª¢æŸ¥å°å…¥ä½†æœªä½¿ç”¨çš„æ¨¡çµ„
        imports = re.findall(r'from\s+(\S+)\s+import|import\s+(\S+)', python_code)
        for imp in imports:
            module = imp[0] or imp[1]
            if module and python_code.count(module) <= 1:
                self.analysis_results["unused_code"].append({
                    "category": "UNUSED_IMPORT",
                    "severity": "LOW", 
                    "name": module,
                    "details": f"å°å…¥çš„æ¨¡çµ„ {module} å¯èƒ½æœªè¢«ä½¿ç”¨"
                })
    
    def analyze_data_flow_integrity(self, json_spec: Dict, python_code: str):
        """åˆ†ææ•¸æ“šæµå®Œæ•´æ€§"""
        # æª¢æŸ¥ä¸Šæ¸¸æ¨¡çµ„é€£æ¥
        try:
            if "dependencies" in json_spec:
                deps_spec = json_spec["dependencies"]
            elif "dependencies" in json_spec.get("real_data_signal_quality_engine_dependency", {}):
                deps_spec = json_spec["real_data_signal_quality_engine_dependency"]["dependencies"]
            else:
                self.analysis_results["data_flow_breaks"].append({
                    "category": "MISSING_DEPENDENCIES_CONFIG",
                    "severity": "HIGH",
                    "details": "JSON è¦ç¯„ä¸­æ‰¾ä¸åˆ° dependencies é…ç½®"
                })
                return
            
            upstream_modules = deps_spec.get("upstream_modules", [])
        except KeyError as e:
            self.analysis_results["data_flow_breaks"].append({
                "category": "DEPENDENCIES_STRUCTURE_ERROR",
                "severity": "HIGH",
                "details": f"ä¾è³´çµæ§‹éŒ¯èª¤: {e}"
            })
            return
        
        for upstream in upstream_modules:
            module_name = upstream["module"]
            if module_name not in python_code:
                self.analysis_results["data_flow_breaks"].append({
                    "category": "MISSING_UPSTREAM_CONNECTION",
                    "severity": "CRITICAL" if upstream["critical"] else "HIGH",
                    "module": module_name,
                    "type": upstream["type"],
                    "timeout": upstream["timeout"],
                    "details": f"ç¼ºå°‘èˆ‡ä¸Šæ¸¸æ¨¡çµ„ {module_name} çš„é€£æ¥"
                })
        
        # æª¢æŸ¥ä¸‹æ¸¸æ¨¡çµ„é€£æ¥
        downstream_modules = deps_spec.get("downstream_modules", [])
        
        for downstream in downstream_modules:
            module_name = downstream["module"]
            if module_name not in python_code:
                self.analysis_results["data_flow_breaks"].append({
                    "category": "MISSING_DOWNSTREAM_CONNECTION",
                    "severity": "HIGH",
                    "module": module_name,
                    "type": downstream["type"],
                    "delivery_guarantee": downstream["delivery_guarantee"],
                    "details": f"ç¼ºå°‘èˆ‡ä¸‹æ¸¸æ¨¡çµ„ {module_name} çš„é€£æ¥"
                })
    
    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """åŸ·è¡Œå…¨é¢åˆ†æ"""
        print("ğŸ¯ é–‹å§‹ Real Data Signal Quality Engine ç²¾ç¢ºæ·±åº¦åˆ†æ...")
        
        # è¼‰å…¥è¦ç¯„å’Œä»£ç¢¼
        json_spec, python_code = self.load_specifications()
        
        # åŸ·è¡Œå„é …åˆ†æ
        self.analyze_strategy_compliance(json_spec, python_code)
        self.analyze_dependency_compliance(json_spec, python_code)
        self.analyze_enhanced_capabilities(json_spec, python_code)
        self.analyze_computation_flow(json_spec, python_code)
        self.analyze_unused_code(python_code)
        self.analyze_data_flow_integrity(json_spec, python_code)
        
        # è¨ˆç®—ç¸½é«”ç¬¦åˆåº¦
        total_issues = sum(len(issues) for issues in self.analysis_results.values())
        critical_issues = sum(1 for issues in self.analysis_results.values() 
                            for issue in issues if issue.get("severity") == "CRITICAL")
        high_issues = sum(1 for issues in self.analysis_results.values()
                         for issue in issues if issue.get("severity") == "HIGH")
        
        compliance_percentage = max(0, 100 - (critical_issues * 20 + high_issues * 10 + (total_issues - critical_issues - high_issues) * 5))
        
        return {
            "overall_compliance": f"{compliance_percentage:.1f}%",
            "total_issues": total_issues,
            "critical_issues": critical_issues,
            "high_issues": high_issues,
            "detailed_analysis": self.analysis_results,
            "recommendation": "REWRITE_REQUIRED" if compliance_percentage < 60 else 
                            "MAJOR_FIXES_NEEDED" if compliance_percentage < 80 else
                            "MINOR_ADJUSTMENTS" if compliance_percentage < 95 else "COMPLIANT"
        }

if __name__ == "__main__":
    analyzer = RealDataPrecisionAnalyzer()
    results = analyzer.run_comprehensive_analysis()
    
    print(f"\nğŸ¯ Real Data Signal Quality Engine ç²¾ç¢ºåˆ†æå ±å‘Š")
    print(f"=" * 60)
    print(f"ğŸ“Š ç¸½é«”ç¬¦åˆåº¦: {results['overall_compliance']}")
    print(f"ğŸ” ç¸½å•é¡Œæ•¸: {results['total_issues']}")
    print(f"ğŸ”´ åš´é‡å•é¡Œ: {results['critical_issues']}")
    print(f"ğŸŸ¡ é«˜ç´šå•é¡Œ: {results['high_issues']}")
    print(f"ğŸ“‹ å»ºè­°: {results['recommendation']}")
    
    print(f"\nğŸ“‹ è©³ç´°å•é¡Œåˆ†æ:")
    for category, issues in results['detailed_analysis'].items():
        if issues:
            print(f"\nğŸ” {category.upper()}:")
            for i, issue in enumerate(issues, 1):
                severity = issue.get('severity', 'UNKNOWN')
                emoji = "ğŸ”´" if severity == "CRITICAL" else "ğŸŸ¡" if severity == "HIGH" else "ğŸŸ " if severity == "MEDIUM" else "ğŸŸ¢"
                print(f"  {emoji} {i}. [{severity}] {issue.get('details', issue.get('name', str(issue)))}")
