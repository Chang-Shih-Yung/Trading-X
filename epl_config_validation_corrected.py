"""
ğŸ” EPL Decision History Tracking JSON é…ç½®é©—è­‰ (ä¿®æ­£ç‰ˆ)
=======================================================

åŸºæ–¼å¯¦éš›é…ç½®çµæ§‹çš„é©—è­‰è…³æœ¬
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

logger = logging.getLogger(__name__)

class EPLConfigValidatorCorrected:
    """EPL æ±ºç­–æ­·å²è¿½è¹¤é…ç½®é©—è­‰å™¨ (ä¿®æ­£ç‰ˆ)"""
    
    def __init__(self):
        self.config_path = Path("/Users/henrychang/Desktop/Trading-X/X/backend/phase4_output_monitoring/3_epl_decision_history_tracking/epl_decision_history_tracking_config.json")
        
    def load_and_analyze_config(self) -> Dict[str, Any]:
        """è¼‰å…¥ä¸¦åˆ†æé…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            return {
                "config_loaded": True,
                "config_data": config,
                "file_size": self.config_path.stat().st_size,
                "structure_analysis": self._analyze_real_structure(config)
            }
            
        except Exception as e:
            return {
                "config_loaded": False,
                "error": str(e),
                "file_exists": self.config_path.exists()
            }
    
    def _analyze_real_structure(self, config: Dict) -> Dict[str, Any]:
        """åˆ†æå¯¦éš›é…ç½®çµæ§‹"""
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        
        return {
            "main_sections": list(epl_config.keys()),
            "has_decision_tracking": "decision_tracking_architecture" in epl_config,
            "has_analytics": "analytics_and_reporting" in epl_config,
            "has_data_management": "data_management_and_retention" in epl_config,
            "has_api_integration": "api_integration_points" in epl_config,
            "decision_lifecycle": "decision_lifecycle_monitoring" in epl_config.get("decision_tracking_architecture", {}),
            "outcome_measurement": "outcome_measurement" in epl_config.get("decision_tracking_architecture", {}),
            "historical_analysis": "historical_analysis_capabilities" in epl_config.get("analytics_and_reporting", {})
        }
    
    def validate_phase_integration(self, config: Dict) -> Dict[str, Any]:
        """é©—è­‰ Phase æ•´åˆ (åŸºæ–¼å¯¦éš›çµæ§‹)"""
        integration_score = 0
        max_score = 40
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        decision_arch = epl_config.get("decision_tracking_architecture", {})
        
        # Phase1 ä¿¡è™Ÿè¿½è¹¤ (10åˆ†)
        phase1_score = 0
        decision_creation = decision_arch.get("decision_lifecycle_monitoring", {}).get("decision_creation", {})
        if "original_signal_candidate" in decision_creation.get("input_data_capture", {}):
            phase1_score += 5
        if "pre_evaluation_result" in decision_creation.get("input_data_capture", {}):
            phase1_score += 5
        
        details["phase1_signal_integration"] = f"{phase1_score}/10"
        integration_score += phase1_score
        
        # Phase2 é è©•ä¼°æ•´åˆ (10åˆ†)
        phase2_score = 0
        if "pre_evaluation_result" in decision_creation.get("input_data_capture", {}):
            phase2_score += 5
        if "decision_engine_process" in decision_creation:
            phase2_score += 5
        
        details["phase2_pre_evaluation"] = f"{phase2_score}/10"
        integration_score += phase2_score
        
        # Phase3 åŸ·è¡Œæ”¿ç­–è¿½è¹¤ (10åˆ†)
        phase3_score = 0
        execution_tracking = decision_arch.get("decision_lifecycle_monitoring", {}).get("decision_execution_tracking", {})
        if "execution_initiation" in execution_tracking:
            phase3_score += 5
        if "execution_monitoring" in execution_tracking:
            phase3_score += 5
        
        details["phase3_execution_tracking"] = f"{phase3_score}/10"
        integration_score += phase3_score
        
        # Phase4 å…§éƒ¨æ•´åˆ (10åˆ†)
        phase4_score = 0
        outcome_measurement = decision_arch.get("decision_lifecycle_monitoring", {}).get("outcome_measurement", {})
        if "immediate_outcomes" in outcome_measurement:
            phase4_score += 5
        if "long_term_performance" in outcome_measurement:
            phase4_score += 5
        
        details["phase4_outcome_tracking"] = f"{phase4_score}/10"
        integration_score += phase4_score
        
        return {
            "integration_score": integration_score,
            "max_score": max_score,
            "percentage": (integration_score / max_score) * 100,
            "details": details,
            "status": "excellent" if integration_score >= 32 else "good" if integration_score >= 24 else "needs_improvement"
        }
    
    def validate_data_structures(self, config: Dict) -> Dict[str, Any]:
        """é©—è­‰æ•¸æ“šçµæ§‹ç›¸å®¹æ€§"""
        compatibility_score = 0
        max_score = 30
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        
        # æ±ºç­–è¨˜éŒ„çµæ§‹ (10åˆ†)
        decision_structure = 0
        decision_lifecycle = epl_config.get("decision_tracking_architecture", {}).get("decision_lifecycle_monitoring", {})
        if "decision_creation" in decision_lifecycle:
            decision_structure += 5
        if "decision_execution_tracking" in decision_lifecycle:
            decision_structure += 5
        
        details["decision_record_structure"] = f"{decision_structure}/10"
        compatibility_score += decision_structure
        
        # æ•¸æ“šç®¡ç† (10åˆ†)
        data_management = 0
        data_mgmt = epl_config.get("data_management_and_retention", {})
        if "retention_policies" in data_mgmt:
            data_management += 5
        if "data_archival" in data_mgmt:
            data_management += 5
        
        details["data_management"] = f"{data_management}/10"
        compatibility_score += data_management
        
        # API æ•´åˆ (10åˆ†)
        api_integration = 0
        api_config = epl_config.get("api_integration_points", {})
        if "data_export_endpoints" in api_config:
            api_integration += 5
        if "historical_query_endpoints" in api_config:
            api_integration += 5
        
        details["api_integration"] = f"{api_integration}/10"
        compatibility_score += api_integration
        
        return {
            "compatibility_score": compatibility_score,
            "max_score": max_score,
            "percentage": (compatibility_score / max_score) * 100,
            "details": details,
            "status": "excellent" if compatibility_score >= 24 else "good" if compatibility_score >= 18 else "needs_improvement"
        }
    
    def validate_monitoring_features(self, config: Dict) -> Dict[str, Any]:
        """é©—è­‰ç›£æ§åŠŸèƒ½"""
        monitoring_score = 0
        max_score = 30
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        
        # åˆ†æèƒ½åŠ› (10åˆ†)
        analytics_score = 0
        analytics = epl_config.get("analytics_and_reporting", {})
        if "historical_analysis_capabilities" in analytics:
            analytics_score += 5
        if "performance_analytics" in analytics:
            analytics_score += 5
        
        details["analytics_capabilities"] = f"{analytics_score}/10"
        monitoring_score += analytics_score
        
        # æ±ºç­–æˆæ•ˆè¿½è¹¤ (10åˆ†)
        effectiveness_score = 0
        outcome_measurement = epl_config.get("decision_tracking_architecture", {}).get("decision_lifecycle_monitoring", {}).get("outcome_measurement", {})
        if "immediate_outcomes" in outcome_measurement:
            effectiveness_score += 5
        if "long_term_performance" in outcome_measurement:
            effectiveness_score += 5
        
        details["decision_effectiveness"] = f"{effectiveness_score}/10"
        monitoring_score += effectiveness_score
        
        # å­¸ç¿’ç³»çµ± (10åˆ†)
        learning_score = 0
        learning_system = epl_config.get("decision_learning_system", {})
        if "pattern_recognition" in learning_system:
            learning_score += 5
        if "strategy_optimization" in learning_system:
            learning_score += 5
        
        details["learning_system"] = f"{learning_score}/10"
        monitoring_score += learning_score
        
        return {
            "monitoring_score": monitoring_score,
            "max_score": max_score,
            "percentage": (monitoring_score / max_score) * 100,
            "details": details,
            "status": "excellent" if monitoring_score >= 24 else "good" if monitoring_score >= 18 else "needs_improvement"
        }
    
    def generate_validation_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆé©—è­‰å ±å‘Š"""
        print("ğŸ” EPL Decision History Tracking JSON é…ç½®é©—è­‰ (ä¿®æ­£ç‰ˆ)")
        print("=" * 70)
        
        # è¼‰å…¥é…ç½®
        config_analysis = self.load_and_analyze_config()
        
        if not config_analysis.get("config_loaded"):
            return {
                "validation_status": "failed",
                "error": config_analysis.get("error")
            }
        
        config = config_analysis["config_data"]
        structure = config_analysis["structure_analysis"]
        
        print(f"âœ… é…ç½®è¼‰å…¥æˆåŠŸ ({config_analysis['file_size']} bytes)")
        print("ğŸ“Š å¯¦éš›é…ç½®çµæ§‹åˆ†æ:")
        for key, value in structure.items():
            icon = "âœ…" if value else "âŒ"
            print(f"  {icon} {key}: {value}")
        
        # å„é …é©—è­‰
        phase_integration = self.validate_phase_integration(config)
        data_compatibility = self.validate_data_structures(config)
        monitoring_features = self.validate_monitoring_features(config)
        
        # è¨ˆç®—ç¸½åˆ†
        total_score = (
            phase_integration["integration_score"] +
            data_compatibility["compatibility_score"] +
            monitoring_features["monitoring_score"]
        )
        max_total_score = (
            phase_integration["max_score"] +
            data_compatibility["max_score"] +
            monitoring_features["max_score"]
        )
        
        overall_percentage = (total_score / max_total_score) * 100
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            "validation_timestamp": datetime.now().isoformat(),
            "overall_score": {
                "total_points": f"{total_score}/{max_total_score}",
                "percentage": f"{overall_percentage:.1f}%",
                "grade": self._get_grade(overall_percentage)
            },
            "detailed_scores": {
                "phase_integration": phase_integration,
                "data_compatibility": data_compatibility,
                "monitoring_features": monitoring_features
            },
            "structure_analysis": structure,
            "recommendations": self._generate_recommendations(
                phase_integration, data_compatibility, monitoring_features, overall_percentage
            )
        }
        
        # æ‰“å°çµæœ
        self._print_results(report)
        
        return report
    
    def _get_grade(self, percentage: float) -> str:
        """ç²å–è©•ç´š"""
        if percentage >= 90:
            return "A+ (å„ªç§€)"
        elif percentage >= 80:
            return "A (è‰¯å¥½)" 
        elif percentage >= 70:
            return "B (å¯æ¥å—)"
        elif percentage >= 60:
            return "C (éœ€æ”¹é€²)"
        else:
            return "D (éœ€é‡æ§‹)"
    
    def _generate_recommendations(self, phase_integration, data_compatibility, monitoring_features, overall_percentage) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        if overall_percentage >= 85:
            recommendations.append("é…ç½®å“è³ªå„ªç§€ï¼Œå»ºè­°ç¶­æŒç¾ç‹€")
        elif overall_percentage >= 70:
            recommendations.append("é…ç½®è‰¯å¥½ï¼Œå¯é€²è¡Œå°å¹…å„ªåŒ–")
        else:
            if phase_integration["percentage"] < 70:
                recommendations.append("éœ€è¦æ”¹é€² Phase é–“æ•´åˆé…ç½®")
            if data_compatibility["percentage"] < 70:
                recommendations.append("éœ€è¦å„ªåŒ–æ•¸æ“šçµæ§‹è¨­è¨ˆ")
            if monitoring_features["percentage"] < 70:
                recommendations.append("éœ€è¦å®Œå–„ç›£æ§åŠŸèƒ½")
        
        return recommendations
    
    def _print_results(self, report: Dict):
        """æ‰“å°çµæœ"""
        print(f"\nğŸ“Š ç¸½é«”è©•åˆ†: {report['overall_score']['percentage']} - {report['overall_score']['grade']}")
        
        print("\nğŸ“‹ è©³ç´°è©•åˆ†:")
        for category, details in report["detailed_scores"].items():
            percentage = details["percentage"]
            status_icon = "âœ…" if percentage >= 70 else "âš ï¸" if percentage >= 50 else "âŒ"
            print(f"  {status_icon} {category}: {percentage:.1f}% ({details['status']})")
            
            for detail_key, detail_value in details["details"].items():
                print(f"    - {detail_key}: {detail_value}")
        
        print("\nğŸ’¡ å»ºè­°:")
        for rec in report["recommendations"]:
            print(f"  â€¢ {rec}")

def main():
    """ä¸»å‡½æ•¸"""
    validator = EPLConfigValidatorCorrected()
    report = validator.generate_validation_report()
    
    print(f"\nğŸ¯ EPL Decision History Tracking JSON é©—è­‰å®Œæˆ")
    print(f"ğŸ“Š ç¸½åˆ†: {report['overall_score']['percentage']}")

if __name__ == "__main__":
    main()
