"""
üéØ Phase1B Á≤æÁ¢∫Ê∑±Â∫¶ÂàÜÊûêÂ∑•ÂÖ∑
Ê™¢Êü• phase1b_volatility_adaptation.py ÊòØÂê¶ÂÆåÂÖ®ÂåπÈÖç JSON Ë¶èÁØÑ
ÈáçÈªûÔºöÊï∏ÊìöÊµÅ„ÄÅÊ†∏ÂøÉÈÇèËºØ„ÄÅÂêÑÂ±§Á¥öÊï∏Êìö‰ΩøÁî®ÁöÑÁ≤æÁ¢∫ÊÄßÂàÜÊûê
"""

import json
import ast
import re
from typing import Dict, List, Any, Set, Tuple
from pathlib import Path

class Phase1BPreciseAnalyzer:
    def __init__(self):
        self.json_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation_dependency.json"
        self.py_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation.py"
        
    def run_precise_analysis(self):
        """Âü∑Ë°åÁ≤æÁ¢∫Ê∑±Â∫¶ÂàÜÊûê"""
        print("üîç Phase1B Á≤æÁ¢∫Ê∑±Â∫¶ÂàÜÊûêÈñãÂßã")
        print("="*80)
        
        # 1. ËºâÂÖ•Ë¶èÁØÑËàá‰ª£Á¢º
        json_spec = self.load_json_spec()
        py_content = self.load_python_code()
        
        if not json_spec or not py_content:
            print("‚ùå Ê™îÊ°àËºâÂÖ•Â§±Êïó")
            return
        
        # 2. Êû∂ÊßãËßíËâ≤Á¢∫Ë™ç
        print("\nüìã 1. Êû∂ÊßãËßíËâ≤Á¢∫Ë™ç")
        role_analysis = self.analyze_architectural_role(json_spec, py_content)
        
        # 3. Êï∏ÊìöÊµÅÁ≤æÁ¢∫ÂàÜÊûê
        print("\nüîÑ 2. Êï∏ÊìöÊµÅÁ≤æÁ¢∫ÂàÜÊûê")
        dataflow_analysis = self.analyze_dataflow_precision(json_spec, py_content)
        
        # 4. Â±§Á¥öÈÇèËºØÊ∑±Â∫¶Ê™¢Êü•
        print("\nüèóÔ∏è 3. Â±§Á¥öÈÇèËºØÊ∑±Â∫¶Ê™¢Êü•")
        layer_analysis = self.analyze_layer_logic(json_spec, py_content)
        
        # 5. Êï∏Êìö‰ΩøÁî®‰∏ÄËá¥ÊÄßÈ©óË≠â
        print("\nüìä 4. Êï∏Êìö‰ΩøÁî®‰∏ÄËá¥ÊÄßÈ©óË≠â")
        data_usage_analysis = self.analyze_data_usage_consistency(json_spec, py_content)
        
        # 6. Ê†∏ÂøÉÊºîÁÆóÊ≥ïÂ∞çÊáâÊÄß
        print("\n‚ö° 5. Ê†∏ÂøÉÊºîÁÆóÊ≥ïÂ∞çÊáâÊÄß")
        algorithm_analysis = self.analyze_algorithm_correspondence(json_spec, py_content)
        
        # 7. Êï¥ÂêàÂ∫¶ËàáÂÆåÊï¥ÊÄßË©ï‰º∞
        print("\nüéØ 6. Êï¥ÂêàÂ∫¶ËàáÂÆåÊï¥ÊÄßË©ï‰º∞")
        integration_analysis = self.analyze_integration_completeness(json_spec, py_content)
        
        # 8. ÁîüÊàêÁ≤æÁ¢∫ÂåπÈÖçÂ†±Âëä
        print("\nüìà 7. Á≤æÁ¢∫ÂåπÈÖçÂ†±Âëä")
        final_report = self.generate_final_report({
            'role': role_analysis,
            'dataflow': dataflow_analysis, 
            'layers': layer_analysis,
            'data_usage': data_usage_analysis,
            'algorithms': algorithm_analysis,
            'integration': integration_analysis
        })
        
        return final_report
    
    def load_json_spec(self) -> Dict[str, Any]:
        """ËºâÂÖ• JSON Ë¶èÁØÑ"""
        try:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå JSON ËºâÂÖ•Â§±Êïó: {e}")
            return {}
    
    def load_python_code(self) -> str:
        """ËºâÂÖ• Python ‰ª£Á¢º"""
        try:
            with open(self.py_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"‚ùå Python ‰ª£Á¢ºËºâÂÖ•Â§±Êïó: {e}")
            return ""
    
    def analyze_architectural_role(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """ÂàÜÊûêÊû∂ÊßãËßíËâ≤ - Á¢∫Ë™çÊòØ‰ø°ËôüÈÅ©ÊáâÂô®ËÄåÈùûÁîüÊàêÂô®"""
        print("   Ê™¢Êü•Êû∂ÊßãËßíËâ≤ÂÆö‰Ωç...")
        
        analysis = {
            'json_role': None,
            'python_role': None,
            'role_match': False,
            'evidence': []
        }
        
        # Ê≠£Á¢∫Ë®™Âïè JSON ÁµêÊßã
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        integration_points = strategy_graph.get('integration_points', {})
        input_format = strategy_graph.get('input_format_compatibility', {})
        
        # JSON Ë¶èÁØÑËßíËâ≤ÂàÜÊûê
        receives_from = integration_points.get('receives_from', [])
        feeds_to = integration_points.get('feeds_to', [])
        
        if receives_from:
            analysis['json_role'] = 'signal_adapter'
            analysis['evidence'].append(f"JSON: receives_from = {receives_from}")
        
        processes_outputs = input_format.get('processes_standardized_outputs', False)
        if processes_outputs:
            analysis['json_role'] = 'signal_adapter'
            analysis['evidence'].append("JSON: processes_standardized_outputs = true (‰ø°ËôüÈÅ©ÊáâÂô®)")
        
        if feeds_to:
            analysis['evidence'].append(f"JSON: feeds_to = {feeds_to}")
        
        # Ê™¢Êü• JSON ‰∏≠ÁöÑÊèèËø∞
        description = strategy_graph.get('description', '')
        if 'ÂãïÊÖãÊ≥¢ÂãïÊÄßÁõ£Ê∏¨ËàáÁ≠ñÁï•ÂèÉÊï∏Ëá™ÈÅ©ÊáâË™øÊï¥Á≥ªÁµ±' in description:
            analysis['json_role'] = 'signal_adapter'
            analysis['evidence'].append("JSON: ÊèèËø∞ÁÇ∫'ÂãïÊÖãÊ≥¢ÂãïÊÄßÁõ£Ê∏¨ËàáÁ≠ñÁï•ÂèÉÊï∏Ëá™ÈÅ©ÊáâË™øÊï¥Á≥ªÁµ±' (‰ø°ËôüÈÅ©ÊáâÂô®)")
        
        # Python ‰ª£Á¢ºËßíËâ≤ÂàÜÊûê
        if 'process_signals_with_volatility_adaptation' in py_content:
            analysis['python_role'] = 'signal_adapter'
            analysis['evidence'].append("Python: ‰∏ªË¶ÅÂÖ•Âè£ÈªûÁÇ∫ process_signals_with_volatility_adaptation()")
        
        # Ê™¢Êü•ÊòØÂê¶ÊúâÈåØË™§ÁöÑ‰ø°ËôüÁîüÊàêÊñπÊ≥ï
        generation_methods = [
            '_generate_breakout_signal',
            '_generate_momentum_signal', 
            '_generate_reversal_signal'
        ]
        
        found_generation = False
        for method in generation_methods:
            if method in py_content:
                found_generation = True
                analysis['evidence'].append(f"Python: ÁôºÁèæ‰ø°ËôüÁîüÊàêÊñπÊ≥ï {method} (ËßíËâ≤ÈåØË™§)")
        
        if not found_generation:
            analysis['evidence'].append("Python: Êú™ÁôºÁèæ‰ø°ËôüÁîüÊàêÊñπÊ≥ï (ËßíËâ≤Ê≠£Á¢∫)")
        
        # ËßíËâ≤ÂåπÈÖçÂà§Êñ∑
        analysis['role_match'] = (
            analysis['json_role'] == 'signal_adapter' and 
            analysis['python_role'] == 'signal_adapter' and 
            not found_generation
        )
        
        print(f"   JSON ËßíËâ≤: {analysis['json_role']}")
        print(f"   Python ËßíËâ≤: {analysis['python_role']}")
        print(f"   ËßíËâ≤ÂåπÈÖç: {'‚úÖ' if analysis['role_match'] else '‚ùå'}")
        
        return analysis
    
    def analyze_dataflow_precision(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """Á≤æÁ¢∫ÂàÜÊûêÊï∏ÊìöÊµÅ"""
        print("   Ê™¢Êü•Êï∏ÊìöÊµÅÁ≤æÁ¢∫ÊÄß...")
        
        analysis = {
            'input_sources': {'json': [], 'python': []},
            'output_targets': {'json': [], 'python': []},
            'data_transformations': {'json': [], 'python': []},
            'flow_consistency': 0.0,
            'missing_flows': [],
            'extra_flows': []
        }
        
        # Ê≠£Á¢∫Ë®™Âïè JSON ÁµêÊßã
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        integration_points = strategy_graph.get('integration_points', {})
        computation_flow = strategy_graph.get('computation_flow', {})
        
        # JSON Êï∏ÊìöÊµÅÂàÜÊûê
        receives_from = integration_points.get('receives_from', [])
        feeds_to = integration_points.get('feeds_to', [])
        
        analysis['input_sources']['json'] = receives_from
        analysis['output_targets']['json'] = feeds_to
        
        # JSON ËΩâÊèõÊµÅÁ®ã
        layer_names = list(computation_flow.keys())
        analysis['data_transformations']['json'] = layer_names
        
        # Python Êï∏ÊìöÊµÅÂàÜÊûê
        # Ê™¢Êü•Ëº∏ÂÖ•ËôïÁêÜ
        if 'signals: List[Dict[str, Any]]' in py_content:
            analysis['input_sources']['python'].append('signals_parameter')
        
        if 'market_data: Dict[str, Any]' in py_content:
            analysis['input_sources']['python'].append('market_data_parameter')
        
        # Ê™¢Êü•Ëº∏Âá∫ÁîüÊàê
        if 'AdaptiveSignalAdjustment' in py_content:
            analysis['output_targets']['python'].append('adaptive_signal_adjustments')
        
        # Ê™¢Êü•Êï∏ÊìöËΩâÊèõ
        layer_methods = [
            'layer_1_data_collection',
            'layer_2_volatility_metrics', 
            'layer_3_signal_adjustment',
            'layer_4_final_optimization'
        ]
        
        for method in layer_methods:
            if f'_layer_' in py_content and method.split('_', 2)[-1] in py_content:
                analysis['data_transformations']['python'].append(method)
        
        # Ê™¢Êü•Áº∫Â§±ÂíåÈ°çÂ§ñÊµÅÁ®ã
        json_layers = set(analysis['data_transformations']['json'])
        python_layers = set(analysis['data_transformations']['python'])
        
        analysis['missing_flows'] = list(json_layers - python_layers)
        analysis['extra_flows'] = list(python_layers - json_layers)
        
        # Ë®àÁÆóÊµÅÁ®ã‰∏ÄËá¥ÊÄß
        if len(analysis['input_sources']['json']) > 0:
            input_match = len(set(analysis['input_sources']['json']) & 
                             set(analysis['input_sources']['python'])) / len(analysis['input_sources']['json'])
        else:
            input_match = 1.0
        
        if len(analysis['output_targets']['json']) > 0:
            output_match = len(set(analysis['output_targets']['json']) & 
                              set(analysis['output_targets']['python'])) / len(analysis['output_targets']['json'])
        else:
            output_match = 1.0
        
        if len(analysis['data_transformations']['json']) > 0:
            transform_match = len(set(analysis['data_transformations']['python'])) / len(analysis['data_transformations']['json'])
        else:
            transform_match = 1.0
        
        analysis['flow_consistency'] = (input_match + output_match + transform_match) / 3
        
        print(f"   JSON Ëº∏ÂÖ•Ê∫ê: {analysis['input_sources']['json']}")
        print(f"   Python Ëº∏ÂÖ•Ê∫ê: {analysis['input_sources']['python']}")
        print(f"   JSON Ëº∏Âá∫ÁõÆÊ®ô: {analysis['output_targets']['json']}")  
        print(f"   Python Ëº∏Âá∫ÁõÆÊ®ô: {analysis['output_targets']['python']}")
        print(f"   JSON Â±§Á¥ö: {analysis['data_transformations']['json']}")
        print(f"   Python Â±§Á¥ö: {analysis['data_transformations']['python']}")
        print(f"   Êï¥È´îÊµÅÁ®ã‰∏ÄËá¥ÊÄß: {analysis['flow_consistency']:.1%}")
        
        return analysis
    
    def analyze_layer_logic(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """ÂàÜÊûêÂ±§Á¥öÈÇèËºØÊ∑±Â∫¶Ê™¢Êü•"""
        print("   Ê™¢Êü•Â±§Á¥öÈÇèËºØÂØ¶Áèæ...")
        
        analysis = {
            'json_layers': {},
            'python_layers': {},
            'layer_alignment': {},
            'logic_completeness': 0.0,
            'missing_logic': [],
            'layer_data_flow': {}
        }
        
        # Ê≠£Á¢∫Ë®™Âïè JSON ÁµêÊßã
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        computation_flow = strategy_graph.get('computation_flow', {})
        
        # JSON Â±§Á¥öË¶èÁØÑ
        for layer_name, layer_config in computation_flow.items():
            operations = layer_config.get('operations', {})
            dependencies = layer_config.get('dependencies', [])
            description = layer_config.get('description', '')
            
            analysis['json_layers'][layer_name] = {
                'operations': list(operations.keys()),
                'dependencies': dependencies,
                'description': description,
                'operation_count': len(operations)
            }
        
        # Python Â±§Á¥öÂØ¶Áèæ
        layer_patterns = {
            'layer_1_data_collection': r'async def _layer_1_data_collection.*?(?=async def|\Z)',
            'layer_2_volatility_metrics': r'async def _layer_2_volatility_metrics.*?(?=async def|\Z)',
            'layer_3_adaptive_parameters': r'async def _layer_3_signal_adjustment.*?(?=async def|\Z)',
            'layer_4_strategy_signals': r'async def _layer_4_final_optimization.*?(?=async def|\Z)'
        }
        
        for layer_name, pattern in layer_patterns.items():
            match = re.search(pattern, py_content, re.DOTALL)
            if match:
                layer_code = match.group(0)
                analysis['python_layers'][layer_name] = {
                    'implemented': True,
                    'code_length': len(layer_code),
                    'has_error_handling': 'try:' in layer_code and 'except' in layer_code,
                    'has_timing': 'time.time()' in layer_code,
                    'data_operations': self.extract_data_operations(layer_code)
                }
            else:
                analysis['python_layers'][layer_name] = {'implemented': False}
        
        # Â±§Á¥öÂ∞çÈΩäÂàÜÊûê
        for layer_name in analysis['json_layers']:
            python_layer_name = layer_name
            # Êò†Â∞ÑÂêçÁ®±Â∑ÆÁï∞
            if layer_name == 'layer_3_adaptive_parameters':
                python_layer_name = 'layer_3_signal_adjustment'
            elif layer_name == 'layer_4_strategy_signals':
                python_layer_name = 'layer_4_final_optimization'
            
            if python_layer_name in analysis['python_layers'] and analysis['python_layers'][python_layer_name].get('implemented', False):
                json_layer = analysis['json_layers'][layer_name]
                python_layer = analysis['python_layers'][python_layer_name]
                
                # Ê™¢Êü•Êìç‰ΩúË¶ÜËìãÁéá
                json_operations = json_layer['operations']
                python_operations = python_layer['data_operations']
                
                operation_coverage = 0.0
                if json_operations:
                    matched_operations = sum(1 for op in json_operations 
                                           if any(op.lower() in py_op.lower() for py_op in python_operations))
                    operation_coverage = matched_operations / len(json_operations)
                
                analysis['layer_alignment'][layer_name] = {
                    'operation_coverage': operation_coverage,
                    'has_error_handling': python_layer['has_error_handling'],
                    'has_timing': python_layer['has_timing'],
                    'python_layer_name': python_layer_name
                }
            else:
                analysis['layer_alignment'][layer_name] = {'implemented': False}
        
        # Ë®àÁÆóÈÇèËºØÂÆåÊï¥ÊÄß
        implemented_count = sum(1 for alignment in analysis['layer_alignment'].values() 
                               if alignment.get('implemented', True))
        total_layers = len(analysis['json_layers'])
        
        if total_layers > 0:
            analysis['logic_completeness'] = implemented_count / total_layers
        
        print(f"   JSON Â±§Á¥öÊï∏: {total_layers}")
        print(f"   Python ÂØ¶ÁèæÂ±§Á¥ö: {implemented_count}")
        print(f"   ÈÇèËºØÂÆåÊï¥ÊÄß: {analysis['logic_completeness']:.1%}")
        
        for layer_name, alignment in analysis['layer_alignment'].items():
            if alignment.get('implemented', True):
                coverage = alignment.get('operation_coverage', 0)
                print(f"   - {layer_name}: Êìç‰ΩúË¶ÜËìãÁéá {coverage:.1%}")
        
        return analysis
    
    def extract_data_operations(self, code: str) -> List[str]:
        """ÊèêÂèñ‰ª£Á¢º‰∏≠ÁöÑÊï∏ÊìöÊìç‰Ωú"""
        operations = []
        
        # Â∏∏Ë¶ãÊï∏ÊìöÊìç‰ΩúÊ®°Âºè
        patterns = [
            r'(\w+_volatility)',
            r'(\w+_metrics)', 
            r'(\w+_calculation)',
            r'(\w+_adjustment)',
            r'(\w+_analysis)',
            r'(\w+_detection)',
            r'self\.(\w+)',
            r'np\.(\w+)',
            r'market_data\.get\([\'"](\w+)[\'"]'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, code)
            operations.extend(matches)
        
        return list(set(operations))  # ÂéªÈáç
    
    def analyze_data_usage_consistency(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """ÂàÜÊûêÊï∏Êìö‰ΩøÁî®‰∏ÄËá¥ÊÄß"""
        print("   Ê™¢Êü•Êï∏Êìö‰ΩøÁî®‰∏ÄËá¥ÊÄß...")
        
        analysis = {
            'json_data_types': {},
            'python_data_types': {},
            'type_consistency': 0.0,
            'missing_data_types': [],
            'dataclass_alignment': {}
        }
        
        # JSON Êï∏ÊìöÈ°ûÂûãË¶èÁØÑ
        if 'data_structures' in json_spec:
            for struct_name, struct_config in json_spec['data_structures'].items():
                analysis['json_data_types'][struct_name] = {
                    'fields': struct_config.get('fields', {}),
                    'required': struct_config.get('required', []),
                    'validation': struct_config.get('validation', {})
                }
        
        # Python Êï∏ÊìöÈ°ûÂûãÂØ¶Áèæ
        dataclass_pattern = r'@dataclass\s*\nclass\s+(\w+)'
        matches = re.finditer(dataclass_pattern, py_content, re.MULTILINE)
        
        for match in matches:
            class_name = match.group(1)
            start_pos = match.start()
            
            # ÊâæÂà∞È°ûÁöÑÁµêÊùü‰ΩçÁΩÆ
            rest_content = py_content[start_pos:]
            
            # Êü•Êâæ‰∏ã‰∏ÄÂÄãÈ°ûÊàñÂáΩÊï∏ÂÆöÁæ©
            next_def = re.search(r'\n(class\s+\w+|def\s+\w+|@dataclass)', rest_content[100:])
            if next_def:
                class_content = rest_content[:100 + next_def.start()]
            else:
                class_content = rest_content[:1000]  # ÂèñÂâç1000Â≠óÁ¨¶
            
            # ÊèêÂèñÊ¨Ñ‰Ωç
            field_pattern = r'(\w+):\s*([\w\[\],\s\.\|]+?)(?=\s*#|\s*\n|$)'
            fields = re.findall(field_pattern, class_content, re.MULTILINE)
            
            analysis['python_data_types'][class_name] = {
                'fields': {field.strip(): field_type.strip() for field, field_type in fields},
                'field_count': len(fields)
            }'
        matches = re.findall(dataclass_pattern, py_content, re.DOTALL)
        
        for match in matches:
            class_name = match
            # Êü•ÊâæÈ°ûÂÆöÁæ©
            class_pattern = f'@dataclass\\s+class\\s+{class_name}.*?(?=@dataclass|class\\s+\\w+(?!.*:)|def\\s+\\w+|\\Z)'
            class_match = re.search(class_pattern, py_content, re.DOTALL)
            
            if class_match:
                class_code = class_match.group(0)
                fields = re.findall(r'(\w+):\s*([\w\[\],\s]+)', class_code)
                analysis['python_data_types'][class_name] = {
                    'fields': {field: field_type for field, field_type in fields},
                    'field_count': len(fields)
                }
        
        # DataClass Â∞çÈΩäÂàÜÊûê
        for json_struct in analysis['json_data_types']:
            if json_struct in analysis['python_data_types']:
                json_fields = set(analysis['json_data_types'][json_struct]['fields'].keys())
                python_fields = set(analysis['python_data_types'][json_struct]['fields'].keys())
                
                common_fields = json_fields & python_fields
                missing_fields = json_fields - python_fields
                extra_fields = python_fields - json_fields
                
                analysis['dataclass_alignment'][json_struct] = {
                    'field_coverage': len(common_fields) / max(1, len(json_fields)),
                    'missing_fields': list(missing_fields),
                    'extra_fields': list(extra_fields)
                }
        
        # Ë®àÁÆóÈ°ûÂûã‰∏ÄËá¥ÊÄß
        if analysis['json_data_types'] and analysis['python_data_types']:
            aligned_count = len(analysis['dataclass_alignment'])
            total_json_types = len(analysis['json_data_types'])
            analysis['type_consistency'] = aligned_count / total_json_types
        
        print(f"   Êï∏ÊìöÁµêÊßãÂ∞çÈΩä: {len(analysis['dataclass_alignment'])}/{len(analysis['json_data_types'])}")
        print(f"   È°ûÂûã‰∏ÄËá¥ÊÄß: {analysis['type_consistency']:.1%}")
        
        return analysis
    
    def analyze_algorithm_correspondence(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """ÂàÜÊûêÊ†∏ÂøÉÊºîÁÆóÊ≥ïÂ∞çÊáâÊÄß"""
        print("   Ê™¢Êü•Ê†∏ÂøÉÊºîÁÆóÊ≥ïÂ∞çÊáâÊÄß...")
        
        analysis = {
            'json_algorithms': {},
            'python_algorithms': {},
            'algorithm_coverage': 0.0,
            'implementation_quality': {},
            'missing_algorithms': []
        }
        
        # Ê≠£Á¢∫Ë®™Âïè JSON ÁµêÊßã
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        computation_flow = strategy_graph.get('computation_flow', {})
        
        # JSON ÊºîÁÆóÊ≥ïË¶èÁØÑÔºàÂæû computation_flow ÊèêÂèñÔºâ
        for layer_name, layer_config in computation_flow.items():
            operations = layer_config.get('operations', {})
            analysis['json_algorithms'][layer_name] = list(operations.keys())
        
        # Python ÊºîÁÆóÊ≥ïÂØ¶ÁèæÊ™¢Êü•
        algorithm_patterns = {
            'volatility_calculation': [
                '_calculate_historical_volatility',
                '_calculate_realized_volatility'
            ],
            'regime_detection': [
                '_detect_volatility_regime',
                '_assess_regime_stability'
            ],
            'signal_adjustment': [
                '_calculate_volatility_adjustment',
                '_calculate_regime_adjustment',
                '_calculate_activity_adjustment'
            ],
            'optimization': [
                '_resolve_signal_conflicts',
                '_optimize_signal_portfolio',
                '_apply_risk_adjustments'
            ]
        }
        
        for algo_category, methods in algorithm_patterns.items():
            implemented_methods = []
            for method in methods:
                if method in py_content:
                    implemented_methods.append(method)
                    
                    # Ê™¢Êü•ÂØ¶ÁèæË≥™Èáè
                    method_pattern = f'def {method}.*?(?=def |\\Z)'
                    method_match = re.search(method_pattern, py_content, re.DOTALL)
                    if method_match:
                        method_code = method_match.group(0)
                        quality_score = self.assess_implementation_quality(method_code)
                        analysis['implementation_quality'][method] = quality_score
            
            analysis['python_algorithms'][algo_category] = {
                'methods': implemented_methods,
                'coverage': len(implemented_methods) / len(methods) if methods else 1.0
            }
        
        # Ë®àÁÆóÊï¥È´îÊºîÁÆóÊ≥ïË¶ÜËìãÁéá
        if algorithm_patterns:
            total_coverage = sum(cat['coverage'] for cat in analysis['python_algorithms'].values())
            analysis['algorithm_coverage'] = total_coverage / len(algorithm_patterns)
        
        # Ê™¢Êü•Áº∫Â§±ÊºîÁÆóÊ≥ï
        for category, info in analysis['python_algorithms'].items():
            if info['coverage'] < 1.0:
                total_methods = len(algorithm_patterns[category])
                implemented_count = len(info['methods'])
                missing_count = total_methods - implemented_count
                analysis['missing_algorithms'].append(f"{category}: {missing_count} ÂÄãÊñπÊ≥ïÁº∫Â§±")
        
        print(f"   JSON ÊºîÁÆóÊ≥ïÂ±§Á¥ö: {len(analysis['json_algorithms'])}")
        print(f"   Python ÊºîÁÆóÊ≥ïÈ°ûÂà•: {len(analysis['python_algorithms'])}")
        print(f"   ÊºîÁÆóÊ≥ïË¶ÜËìãÁéá: {analysis['algorithm_coverage']:.1%}")
        
        for category, info in analysis['python_algorithms'].items():
            print(f"   - {category}: {info['coverage']:.1%} ({len(info['methods'])} ÂÄãÊñπÊ≥ï)")
        
        return analysis
    
    def assess_implementation_quality(self, method_code: str) -> float:
        """Ë©ï‰º∞ÂØ¶ÁèæË≥™Èáè"""
        quality_score = 0.0
        
        # Ê™¢Êü•È†ÖÁõÆ
        checks = [
            ('error_handling', 'try:' in method_code and 'except' in method_code),
            ('input_validation', 'if len(' in method_code or 'if not ' in method_code),
            ('numpy_usage', 'np.' in method_code),
            ('logging', 'logger.' in method_code),
            ('return_validation', 'max(' in method_code and 'min(' in method_code),
            ('documentation', '"""' in method_code or "'''" in method_code)
        ]
        
        for check_name, passed in checks:
            if passed:
                quality_score += 1.0 / len(checks)
        
        return quality_score
    
    def analyze_integration_completeness(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """ÂàÜÊûêÊï¥ÂêàÂ∫¶ËàáÂÆåÊï¥ÊÄß"""
        print("   Ê™¢Êü•Êï¥ÂêàÂ∫¶ËàáÂÆåÊï¥ÊÄß...")
        
        analysis = {
            'configuration_integration': 0.0,
            'error_handling_coverage': 0.0,
            'performance_monitoring': 0.0,
            'external_dependencies': {'json': [], 'python': []},
            'integration_score': 0.0
        }
        
        # ÈÖçÁΩÆÊï¥ÂêàÂ∫¶
        config_usage = [
            'self.config',
            '_load_config',
            'config_path'
        ]
        config_found = sum(1 for usage in config_usage if usage in py_content)
        analysis['configuration_integration'] = config_found / len(config_usage)
        
        # ÈåØË™§ËôïÁêÜË¶ÜËìãÁéá
        error_patterns = [
            'try:',
            'except Exception',
            'logger.error',
            'logger.warning'
        ]
        error_found = sum(1 for pattern in error_patterns if pattern in py_content)
        analysis['error_handling_coverage'] = min(1.0, error_found / len(error_patterns))
        
        # ÊÄßËÉΩÁõ£Êéß
        performance_patterns = [
            'time.time()',
            'processing_time',
            'performance',
            'time_budget'
        ]
        perf_found = sum(1 for pattern in performance_patterns if pattern in py_content)
        analysis['performance_monitoring'] = min(1.0, perf_found / len(performance_patterns))
        
        # JSON Â§ñÈÉ®‰æùË≥¥
        if 'external_dependencies' in json_spec:
            analysis['external_dependencies']['json'] = json_spec['external_dependencies']
        
        # Python Â§ñÈÉ®‰æùË≥¥
        import_patterns = re.findall(r'import\s+(\w+)|from\s+(\w+)', py_content)
        python_imports = [imp[0] or imp[1] for imp in import_patterns]
        analysis['external_dependencies']['python'] = python_imports
        
        # Ë®àÁÆóÊï¥ÂêàË©ïÂàÜ
        analysis['integration_score'] = (
            analysis['configuration_integration'] * 0.3 +
            analysis['error_handling_coverage'] * 0.3 +
            analysis['performance_monitoring'] * 0.4
        )
        
        print(f"   ÈÖçÁΩÆÊï¥Âêà: {analysis['configuration_integration']:.1%}")
        print(f"   ÈåØË™§ËôïÁêÜ: {analysis['error_handling_coverage']:.1%}")
        print(f"   ÊÄßËÉΩÁõ£Êéß: {analysis['performance_monitoring']:.1%}")
        print(f"   Êï¥ÂêàË©ïÂàÜ: {analysis['integration_score']:.1%}")
        
        return analysis
    
    def generate_final_report(self, all_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ÁîüÊàêÊúÄÁµÇÁ≤æÁ¢∫ÂåπÈÖçÂ†±Âëä"""
        
        # Ë®àÁÆóÂêÑÈ†ÖÊ¨äÈáçË©ïÂàÜ
        weights = {
            'role': 0.20,      # Êû∂ÊßãËßíËâ≤ 20%
            'dataflow': 0.25,  # Êï∏ÊìöÊµÅ 25%
            'layers': 0.20,    # Â±§Á¥öÈÇèËºØ 20%
            'data_usage': 0.15, # Êï∏Êìö‰ΩøÁî® 15%
            'algorithms': 0.15, # ÊºîÁÆóÊ≥ï 15%
            'integration': 0.05 # Êï¥ÂêàÂ∫¶ 5%
        }
        
        scores = {
            'role': 1.0 if all_analysis['role']['role_match'] else 0.0,
            'dataflow': all_analysis['dataflow']['flow_consistency'],
            'layers': all_analysis['layers']['logic_completeness'],
            'data_usage': all_analysis['data_usage']['type_consistency'],
            'algorithms': all_analysis['algorithms']['algorithm_coverage'],
            'integration': all_analysis['integration']['integration_score']
        }
        
        # Ë®àÁÆóÁ∏ΩÈ´îÂåπÈÖçÂ∫¶
        total_score = sum(scores[key] * weights[key] for key in weights)
        
        # Ë≠òÂà•ÈóúÈçµÂïèÈ°å
        critical_issues = []
        if scores['role'] < 1.0:
            critical_issues.append("Êû∂ÊßãËßíËâ≤ÂÆö‰Ωç‰∏çÊ∫ñÁ¢∫")
        if scores['dataflow'] < 0.8:
            critical_issues.append("Êï∏ÊìöÊµÅ‰∏çÂÆåÊï¥")
        if scores['layers'] < 0.8:
            critical_issues.append("Â±§Á¥öÈÇèËºØÂØ¶Áèæ‰∏çË∂≥")
        if scores['algorithms'] < 0.7:
            critical_issues.append("Ê†∏ÂøÉÊºîÁÆóÊ≥ïË¶ÜËìã‰∏çË∂≥")
        
        # ÂÑ™ÁßÄÂØ¶ÁèæÈªû
        strengths = []
        if scores['role'] >= 1.0:
            strengths.append("Êû∂ÊßãËßíËâ≤ÂÆö‰ΩçÊ≠£Á¢∫")
        if scores['dataflow'] >= 0.9:
            strengths.append("Êï∏ÊìöÊµÅË®≠Ë®àÂÑ™ÁßÄ")
        if scores['layers'] >= 0.9:
            strengths.append("Â±§Á¥öÈÇèËºØÂØ¶ÁèæÂÆåÊï¥")
        if scores['algorithms'] >= 0.8:
            strengths.append("ÊºîÁÆóÊ≥ïÂØ¶ÁèæË¶ÜËìãËâØÂ•Ω")
        if scores['integration'] >= 0.8:
            strengths.append("Á≥ªÁµ±Êï¥ÂêàÂ∫¶È´ò")
        
        report = {
            'total_match_percentage': total_score * 100,
            'component_scores': {k: v * 100 for k, v in scores.items()},
            'critical_issues': critical_issues,
            'strengths': strengths,
            'detailed_analysis': all_analysis,
            'recommendations': self.generate_recommendations(scores, all_analysis)
        }
        
        # ÊâìÂç∞Â†±Âëä
        print(f"\nüìä Á∏ΩÈ´îÂåπÈÖçÂ∫¶: {report['total_match_percentage']:.1f}%")
        print(f"üèóÔ∏è Êû∂ÊßãËßíËâ≤: {scores['role']*100:.1f}%")
        print(f"üîÑ Êï∏ÊìöÊµÅ: {scores['dataflow']*100:.1f}%") 
        print(f"üè¢ Â±§Á¥öÈÇèËºØ: {scores['layers']*100:.1f}%")
        print(f"üìä Êï∏Êìö‰ΩøÁî®: {scores['data_usage']*100:.1f}%")
        print(f"‚ö° ÊºîÁÆóÊ≥ï: {scores['algorithms']*100:.1f}%")
        print(f"üîó Êï¥ÂêàÂ∫¶: {scores['integration']*100:.1f}%")
        
        if critical_issues:
            print(f"\n‚ùå ÈóúÈçµÂïèÈ°å: {', '.join(critical_issues)}")
        
        if strengths:
            print(f"\n‚úÖ ÂÑ™ÁßÄÂØ¶Áèæ: {', '.join(strengths)}")
        
        return report
    
    def generate_recommendations(self, scores: Dict[str, float], all_analysis: Dict[str, Any]) -> List[str]:
        """ÁîüÊàêÊîπÈÄ≤Âª∫Ë≠∞"""
        recommendations = []
        
        if scores['role'] < 1.0:
            recommendations.append("Á¢∫Ë™ç‰∏¶‰øÆÊ≠£Êû∂ÊßãËßíËâ≤ÂÆö‰ΩçÁÇ∫‰ø°ËôüÈÅ©ÊáâÂô®")
        
        if scores['dataflow'] < 0.8:
            recommendations.append("Ë£úÂÖÖÁº∫Â§±ÁöÑÊï∏ÊìöÊµÅÊé•Âè£ÂíåËΩâÊèõÈÇèËºØ")
        
        if scores['layers'] < 0.8:
            recommendations.append("ÂÆåÂñÑÂ±§Á¥öËôïÁêÜÈÇèËºØÂíåÈåØË™§ËôïÁêÜÊ©üÂà∂")
        
        if scores['data_usage'] < 0.8:
            recommendations.append("Â∞çÈΩäÊï∏ÊìöÁµêÊßãÂÆöÁæ©Âíå‰ΩøÁî®ÊñπÂºè")
        
        if scores['algorithms'] < 0.7:
            recommendations.append("ÂØ¶ÁèæÁº∫Â§±ÁöÑÊ†∏ÂøÉÊºîÁÆóÊ≥ïÊ®°ÁµÑ")
        
        if scores['integration'] < 0.7:
            recommendations.append("Âä†Âº∑ÈÖçÁΩÆÊï¥ÂêàÂíåÊÄßËÉΩÁõ£Êéß")
        
        return recommendations

if __name__ == "__main__":
    analyzer = Phase1BPreciseAnalyzer()
    report = analyzer.run_precise_analysis()
