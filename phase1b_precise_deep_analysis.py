"""
ğŸ¯ Phase1B ç²¾ç¢ºæ·±åº¦åˆ†æå·¥å…·
æª¢æŸ¥ phase1b_volatility_adaptation.py æ˜¯å¦å®Œå…¨åŒ¹é… JSON è¦ç¯„
é‡é»ï¼šæ•¸æ“šæµã€æ ¸å¿ƒé‚è¼¯ã€å„å±¤ç´šæ•¸æ“šä½¿ç”¨çš„ç²¾ç¢ºæ€§åˆ†æ
"""

import json
import ast
import re
from typing import Dict, List, Any, Set, Tuple
from pathlib import Path

class Phase1BPreciseAnalyzer:
    def __init__(self):
        self.json_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation_dependency.json"
        self.py_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation.py"
        
    def run_precise_analysis(self):
        """åŸ·è¡Œç²¾ç¢ºæ·±åº¦åˆ†æ"""
        print("ğŸ” Phase1B ç²¾ç¢ºæ·±åº¦åˆ†æé–‹å§‹")
        print("="*80)
        
        # 1. è¼‰å…¥è¦ç¯„èˆ‡ä»£ç¢¼
        json_spec = self.load_json_spec()
        py_content = self.load_python_code()
        
        if not json_spec or not py_content:
            print("âŒ æª”æ¡ˆè¼‰å…¥å¤±æ•—")
            return
        
        # 2. æ¶æ§‹è§’è‰²ç¢ºèª
        print("\nğŸ“‹ 1. æ¶æ§‹è§’è‰²ç¢ºèª")
        role_analysis = self.analyze_architectural_role(json_spec, py_content)
        
        # 3. æ•¸æ“šæµç²¾ç¢ºåˆ†æ
        print("\nğŸ”„ 2. æ•¸æ“šæµç²¾ç¢ºåˆ†æ")
        dataflow_analysis = self.analyze_dataflow_precision(json_spec, py_content)
        
        # 4. å±¤ç´šé‚è¼¯æ·±åº¦æª¢æŸ¥
        print("\nğŸ—ï¸ 3. å±¤ç´šé‚è¼¯æ·±åº¦æª¢æŸ¥")
        layer_analysis = self.analyze_layer_logic(json_spec, py_content)
        
        # 5. æ•¸æ“šä½¿ç”¨ä¸€è‡´æ€§é©—è­‰
        print("\nğŸ“Š 4. æ•¸æ“šä½¿ç”¨ä¸€è‡´æ€§é©—è­‰")
        data_usage_analysis = self.analyze_data_usage_consistency(json_spec, py_content)
        
        # 6. æ ¸å¿ƒæ¼”ç®—æ³•å°æ‡‰æ€§
        print("\nâš¡ 5. æ ¸å¿ƒæ¼”ç®—æ³•å°æ‡‰æ€§")
        algorithm_analysis = self.analyze_algorithm_correspondence(json_spec, py_content)
        
        # 7. æ•´åˆåº¦èˆ‡å®Œæ•´æ€§è©•ä¼°
        print("\nğŸ¯ 6. æ•´åˆåº¦èˆ‡å®Œæ•´æ€§è©•ä¼°")
        integration_analysis = self.analyze_integration_completeness(json_spec, py_content)
        
        # 8. ç”Ÿæˆç²¾ç¢ºåŒ¹é…å ±å‘Š
        print("\nğŸ“ˆ 7. ç²¾ç¢ºåŒ¹é…å ±å‘Š")
        final_report = self.generate_final_report({
            'role': role_analysis,
            'dataflow': dataflow_analysis, 
            'layers': layer_analysis,
            'data_usage': data_usage_analysis,
            'algorithms': algorithm_analysis,
            'integration': integration_analysis
        })
        
        return final_report
    
    def load_json_spec(self) -> Dict[str, Any]:
        """è¼‰å…¥ JSON è¦ç¯„"""
        try:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ JSON è¼‰å…¥å¤±æ•—: {e}")
            return {}
    
    def load_python_code(self) -> str:
        """è¼‰å…¥ Python ä»£ç¢¼"""
        try:
            with open(self.py_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âŒ Python ä»£ç¢¼è¼‰å…¥å¤±æ•—: {e}")
            return ""
    
    def analyze_architectural_role(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """åˆ†ææ¶æ§‹è§’è‰² - ç¢ºèªæ˜¯ä¿¡è™Ÿé©æ‡‰å™¨è€Œéç”Ÿæˆå™¨"""
        print("   æª¢æŸ¥æ¶æ§‹è§’è‰²å®šä½...")
        
        analysis = {
            'json_role': None,
            'python_role': None,
            'role_match': False,
            'evidence': []
        }
        
        # æ­£ç¢ºè¨ªå• JSON çµæ§‹
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        integration_points = strategy_graph.get('integration_points', {})
        input_format = strategy_graph.get('input_format_compatibility', {})
        
        # JSON è¦ç¯„è§’è‰²åˆ†æ
        receives_from = integration_points.get('receives_from', [])
        feeds_to = integration_points.get('feeds_to', [])
        
        if receives_from:
            analysis['json_role'] = 'signal_adapter'
            analysis['evidence'].append(f"JSON: receives_from = {receives_from}")
        
        processes_outputs = input_format.get('processes_standardized_outputs', False)
        if processes_outputs:
            analysis['json_role'] = 'signal_adapter'
            analysis['evidence'].append("JSON: processes_standardized_outputs = true (ä¿¡è™Ÿé©æ‡‰å™¨)")
        
        if feeds_to:
            analysis['evidence'].append(f"JSON: feeds_to = {feeds_to}")
        
        # æª¢æŸ¥ JSON ä¸­çš„æè¿°
        description = strategy_graph.get('description', '')
        if 'å‹•æ…‹æ³¢å‹•æ€§ç›£æ¸¬èˆ‡ç­–ç•¥åƒæ•¸è‡ªé©æ‡‰èª¿æ•´ç³»çµ±' in description:
            analysis['json_role'] = 'signal_adapter'
            analysis['evidence'].append("JSON: æè¿°ç‚º'å‹•æ…‹æ³¢å‹•æ€§ç›£æ¸¬èˆ‡ç­–ç•¥åƒæ•¸è‡ªé©æ‡‰èª¿æ•´ç³»çµ±' (ä¿¡è™Ÿé©æ‡‰å™¨)")
        
        # Python ä»£ç¢¼è§’è‰²åˆ†æ
        if 'process_signals_with_volatility_adaptation' in py_content:
            analysis['python_role'] = 'signal_adapter'
            analysis['evidence'].append("Python: ä¸»è¦å…¥å£é»ç‚º process_signals_with_volatility_adaptation()")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤çš„ä¿¡è™Ÿç”Ÿæˆæ–¹æ³•
        generation_methods = [
            '_generate_breakout_signal',
            '_generate_momentum_signal', 
            '_generate_reversal_signal'
        ]
        
        found_generation = False
        for method in generation_methods:
            if method in py_content:
                found_generation = True
                analysis['evidence'].append(f"Python: ç™¼ç¾ä¿¡è™Ÿç”Ÿæˆæ–¹æ³• {method} (è§’è‰²éŒ¯èª¤)")
        
        if not found_generation:
            analysis['evidence'].append("Python: æœªç™¼ç¾ä¿¡è™Ÿç”Ÿæˆæ–¹æ³• (è§’è‰²æ­£ç¢º)")
        
        # è§’è‰²åŒ¹é…åˆ¤æ–·
        analysis['role_match'] = (
            analysis['json_role'] == 'signal_adapter' and 
            analysis['python_role'] == 'signal_adapter' and 
            not found_generation
        )
        
        print(f"   JSON è§’è‰²: {analysis['json_role']}")
        print(f"   Python è§’è‰²: {analysis['python_role']}")
        print(f"   è§’è‰²åŒ¹é…: {'âœ…' if analysis['role_match'] else 'âŒ'}")
        
        return analysis
    
    def analyze_dataflow_precision(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """ç²¾ç¢ºåˆ†ææ•¸æ“šæµ"""
        print("   æª¢æŸ¥æ•¸æ“šæµç²¾ç¢ºæ€§...")
        
        analysis = {
            'input_sources': {'json': [], 'python': []},
            'output_targets': {'json': [], 'python': []},
            'data_transformations': {'json': [], 'python': []},
            'flow_consistency': 0.0,
            'missing_flows': [],
            'extra_flows': []
        }
        
        # æ­£ç¢ºè¨ªå• JSON çµæ§‹
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        integration_points = strategy_graph.get('integration_points', {})
        computation_flow = strategy_graph.get('computation_flow', {})
        
        # JSON æ•¸æ“šæµåˆ†æ
        receives_from = integration_points.get('receives_from', [])
        feeds_to = integration_points.get('feeds_to', [])
        
        analysis['input_sources']['json'] = receives_from
        analysis['output_targets']['json'] = feeds_to
        
        # JSON è½‰æ›æµç¨‹
        layer_names = list(computation_flow.keys())
        analysis['data_transformations']['json'] = layer_names
        
        # Python æ•¸æ“šæµåˆ†æ
        # æª¢æŸ¥è¼¸å…¥è™•ç†
        if 'signals: List[Dict[str, Any]]' in py_content:
            analysis['input_sources']['python'].append('signals_parameter')
        
        if 'market_data: Dict[str, Any]' in py_content:
            analysis['input_sources']['python'].append('market_data_parameter')
        
        # æª¢æŸ¥è¼¸å‡ºç”Ÿæˆ
        if 'AdaptiveSignalAdjustment' in py_content:
            analysis['output_targets']['python'].append('adaptive_signal_adjustments')
        
        # æª¢æŸ¥æ•¸æ“šè½‰æ›
        layer_methods = [
            'layer_1_data_collection',
            'layer_2_volatility_metrics', 
            'layer_3_signal_adjustment',
            'layer_4_final_optimization'
        ]
        
        for method in layer_methods:
            if f'_layer_' in py_content and method.split('_', 2)[-1] in py_content:
                analysis['data_transformations']['python'].append(method)
        
        # æª¢æŸ¥ç¼ºå¤±å’Œé¡å¤–æµç¨‹
        json_layers = set(analysis['data_transformations']['json'])
        python_layers = set(analysis['data_transformations']['python'])
        
        analysis['missing_flows'] = list(json_layers - python_layers)
        analysis['extra_flows'] = list(python_layers - json_layers)
        
        # è¨ˆç®—æµç¨‹ä¸€è‡´æ€§
        if len(analysis['input_sources']['json']) > 0:
            input_match = len(set(analysis['input_sources']['json']) & 
                             set(analysis['input_sources']['python'])) / len(analysis['input_sources']['json'])
        else:
            input_match = 1.0
        
        if len(analysis['output_targets']['json']) > 0:
            output_match = len(set(analysis['output_targets']['json']) & 
                              set(analysis['output_targets']['python'])) / len(analysis['output_targets']['json'])
        else:
            output_match = 1.0
        
        if len(analysis['data_transformations']['json']) > 0:
            transform_match = len(set(analysis['data_transformations']['python'])) / len(analysis['data_transformations']['json'])
        else:
            transform_match = 1.0
        
        analysis['flow_consistency'] = (input_match + output_match + transform_match) / 3
        
        print(f"   JSON è¼¸å…¥æº: {analysis['input_sources']['json']}")
        print(f"   Python è¼¸å…¥æº: {analysis['input_sources']['python']}")
        print(f"   JSON è¼¸å‡ºç›®æ¨™: {analysis['output_targets']['json']}")  
        print(f"   Python è¼¸å‡ºç›®æ¨™: {analysis['output_targets']['python']}")
        print(f"   JSON å±¤ç´š: {analysis['data_transformations']['json']}")
        print(f"   Python å±¤ç´š: {analysis['data_transformations']['python']}")
        print(f"   æ•´é«”æµç¨‹ä¸€è‡´æ€§: {analysis['flow_consistency']:.1%}")
        
        return analysis
    
    def analyze_layer_logic(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """åˆ†æå±¤ç´šé‚è¼¯æ·±åº¦æª¢æŸ¥"""
        print("   æª¢æŸ¥å±¤ç´šé‚è¼¯å¯¦ç¾...")
        
        analysis = {
            'json_layers': {},
            'python_layers': {},
            'layer_alignment': {},
            'logic_completeness': 0.0,
            'missing_logic': [],
            'layer_data_flow': {}
        }
        
        # æ­£ç¢ºè¨ªå• JSON çµæ§‹
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        computation_flow = strategy_graph.get('computation_flow', {})
        
        # JSON å±¤ç´šè¦ç¯„
        for layer_name, layer_config in computation_flow.items():
            operations = layer_config.get('operations', {})
            dependencies = layer_config.get('dependencies', [])
            description = layer_config.get('description', '')
            
            analysis['json_layers'][layer_name] = {
                'operations': list(operations.keys()),
                'dependencies': dependencies,
                'description': description,
                'operation_count': len(operations)
            }
        
        # Python å±¤ç´šå¯¦ç¾
        layer_patterns = {
            'layer_1_data_collection': r'async def _layer_1_data_collection.*?(?=async def|\Z)',
            'layer_2_volatility_metrics': r'async def _layer_2_volatility_metrics.*?(?=async def|\Z)',
            'layer_3_adaptive_parameters': r'async def _layer_3_signal_adjustment.*?(?=async def|\Z)',
            'layer_4_strategy_signals': r'async def _layer_4_final_optimization.*?(?=async def|\Z)'
        }
        
        for layer_name, pattern in layer_patterns.items():
            match = re.search(pattern, py_content, re.DOTALL)
            if match:
                layer_code = match.group(0)
                analysis['python_layers'][layer_name] = {
                    'implemented': True,
                    'code_length': len(layer_code),
                    'has_error_handling': 'try:' in layer_code and 'except' in layer_code,
                    'has_timing': 'time.time()' in layer_code,
                    'data_operations': self.extract_data_operations(layer_code)
                }
            else:
                analysis['python_layers'][layer_name] = {'implemented': False}
        
        # å±¤ç´šå°é½Šåˆ†æ
        for layer_name in analysis['json_layers']:
            python_layer_name = layer_name
            # æ˜ å°„åç¨±å·®ç•°
            if layer_name == 'layer_3_adaptive_parameters':
                python_layer_name = 'layer_3_signal_adjustment'
            elif layer_name == 'layer_4_strategy_signals':
                python_layer_name = 'layer_4_final_optimization'
            
            if python_layer_name in analysis['python_layers'] and analysis['python_layers'][python_layer_name].get('implemented', False):
                json_layer = analysis['json_layers'][layer_name]
                python_layer = analysis['python_layers'][python_layer_name]
                
                # æª¢æŸ¥æ“ä½œè¦†è“‹ç‡
                json_operations = json_layer['operations']
                python_operations = python_layer['data_operations']
                
                operation_coverage = 0.0
                if json_operations:
                    matched_operations = sum(1 for op in json_operations 
                                           if any(op.lower() in py_op.lower() for py_op in python_operations))
                    operation_coverage = matched_operations / len(json_operations)
                
                analysis['layer_alignment'][layer_name] = {
                    'operation_coverage': operation_coverage,
                    'has_error_handling': python_layer['has_error_handling'],
                    'has_timing': python_layer['has_timing'],
                    'python_layer_name': python_layer_name
                }
            else:
                analysis['layer_alignment'][layer_name] = {'implemented': False}
        
        # è¨ˆç®—é‚è¼¯å®Œæ•´æ€§
        implemented_count = sum(1 for alignment in analysis['layer_alignment'].values() 
                               if alignment.get('implemented', True))
        total_layers = len(analysis['json_layers'])
        
        if total_layers > 0:
            analysis['logic_completeness'] = implemented_count / total_layers
        
        print(f"   JSON å±¤ç´šæ•¸: {total_layers}")
        print(f"   Python å¯¦ç¾å±¤ç´š: {implemented_count}")
        print(f"   é‚è¼¯å®Œæ•´æ€§: {analysis['logic_completeness']:.1%}")
        
        for layer_name, alignment in analysis['layer_alignment'].items():
            if alignment.get('implemented', True):
                coverage = alignment.get('operation_coverage', 0)
                print(f"   - {layer_name}: æ“ä½œè¦†è“‹ç‡ {coverage:.1%}")
        
        return analysis
    
    def extract_data_operations(self, code: str) -> List[str]:
        """æå–ä»£ç¢¼ä¸­çš„æ•¸æ“šæ“ä½œ"""
        operations = []
        
        # å¸¸è¦‹æ•¸æ“šæ“ä½œæ¨¡å¼
        patterns = [
            r'(\w+_volatility)',
            r'(\w+_metrics)', 
            r'(\w+_calculation)',
            r'(\w+_adjustment)',
            r'(\w+_analysis)',
            r'(\w+_detection)',
            r'self\.(\w+)',
            r'np\.(\w+)',
            r'market_data\.get\([\'"](\w+)[\'"]'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, code)
            operations.extend(matches)
        
        return list(set(operations))  # å»é‡
    
    def analyze_data_usage_consistency(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """åˆ†ææ•¸æ“šä½¿ç”¨ä¸€è‡´æ€§"""
        print("   æª¢æŸ¥æ•¸æ“šä½¿ç”¨ä¸€è‡´æ€§...")
        
        analysis = {
            'json_data_types': {},
            'python_data_types': {},
            'type_consistency': 0.0,
            'missing_data_types': [],
            'dataclass_alignment': {}
        }
        
        # JSON æ•¸æ“šé¡å‹è¦ç¯„
        if 'data_structures' in json_spec:
            for struct_name, struct_config in json_spec['data_structures'].items():
                analysis['json_data_types'][struct_name] = {
                    'fields': struct_config.get('fields', {}),
                    'required': struct_config.get('required', []),
                    'validation': struct_config.get('validation', {})
                }
        
        # Python æ•¸æ“šé¡å‹å¯¦ç¾
        dataclass_pattern = r'@dataclass\s*\nclass\s+(\w+)'
        matches = re.finditer(dataclass_pattern, py_content, re.MULTILINE)
        
        for match in matches:
            class_name = match.group(1)
            start_pos = match.start()
            
            # æ‰¾åˆ°é¡çš„çµæŸä½ç½®
            rest_content = py_content[start_pos:]
            
            # æŸ¥æ‰¾ä¸‹ä¸€å€‹é¡æˆ–å‡½æ•¸å®šç¾©
            next_def = re.search(r'\n(class\s+\w+|def\s+\w+|@dataclass)', rest_content[100:])
            if next_def:
                class_content = rest_content[:100 + next_def.start()]
            else:
                class_content = rest_content[:1000]  # å–å‰1000å­—ç¬¦
            
            # æå–æ¬„ä½
            field_pattern = r'(\w+):\s*([\w\[\],\s\.\|]+?)(?=\s*#|\s*\n|$)'
            fields = re.findall(field_pattern, class_content, re.MULTILINE)
            
            analysis['python_data_types'][class_name] = {
                'fields': {field.strip(): field_type.strip() for field, field_type in fields},
                'field_count': len(fields)
            }'
        matches = re.findall(dataclass_pattern, py_content, re.DOTALL)
        
        for match in matches:
            class_name = match
            # æŸ¥æ‰¾é¡å®šç¾©
            class_pattern = f'@dataclass\\s+class\\s+{class_name}.*?(?=@dataclass|class\\s+\\w+(?!.*:)|def\\s+\\w+|\\Z)'
            class_match = re.search(class_pattern, py_content, re.DOTALL)
            
            if class_match:
                class_code = class_match.group(0)
                fields = re.findall(r'(\w+):\s*([\w\[\],\s]+)', class_code)
                analysis['python_data_types'][class_name] = {
                    'fields': {field: field_type for field, field_type in fields},
                    'field_count': len(fields)
                }
        
        # DataClass å°é½Šåˆ†æ
        for json_struct in analysis['json_data_types']:
            if json_struct in analysis['python_data_types']:
                json_fields = set(analysis['json_data_types'][json_struct]['fields'].keys())
                python_fields = set(analysis['python_data_types'][json_struct]['fields'].keys())
                
                common_fields = json_fields & python_fields
                missing_fields = json_fields - python_fields
                extra_fields = python_fields - json_fields
                
                analysis['dataclass_alignment'][json_struct] = {
                    'field_coverage': len(common_fields) / max(1, len(json_fields)),
                    'missing_fields': list(missing_fields),
                    'extra_fields': list(extra_fields)
                }
        
        # è¨ˆç®—é¡å‹ä¸€è‡´æ€§
        if analysis['json_data_types'] and analysis['python_data_types']:
            aligned_count = len(analysis['dataclass_alignment'])
            total_json_types = len(analysis['json_data_types'])
            analysis['type_consistency'] = aligned_count / total_json_types
        
        print(f"   æ•¸æ“šçµæ§‹å°é½Š: {len(analysis['dataclass_alignment'])}/{len(analysis['json_data_types'])}")
        print(f"   é¡å‹ä¸€è‡´æ€§: {analysis['type_consistency']:.1%}")
        
        return analysis
    
    def analyze_algorithm_correspondence(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """åˆ†ææ ¸å¿ƒæ¼”ç®—æ³•å°æ‡‰æ€§"""
        print("   æª¢æŸ¥æ ¸å¿ƒæ¼”ç®—æ³•å°æ‡‰æ€§...")
        
        analysis = {
            'json_algorithms': {},
            'python_algorithms': {},
            'algorithm_coverage': 0.0,
            'implementation_quality': {},
            'missing_algorithms': []
        }
        
        # æ­£ç¢ºè¨ªå• JSON çµæ§‹
        strategy_graph = json_spec.get('strategy_dependency_graph', {})
        computation_flow = strategy_graph.get('computation_flow', {})
        
        # JSON æ¼”ç®—æ³•è¦ç¯„ï¼ˆå¾ computation_flow æå–ï¼‰
        for layer_name, layer_config in computation_flow.items():
            operations = layer_config.get('operations', {})
            analysis['json_algorithms'][layer_name] = list(operations.keys())
        
        # Python æ¼”ç®—æ³•å¯¦ç¾æª¢æŸ¥
        algorithm_patterns = {
            'volatility_calculation': [
                '_calculate_historical_volatility',
                '_calculate_realized_volatility'
            ],
            'regime_detection': [
                '_detect_volatility_regime',
                '_assess_regime_stability'
            ],
            'signal_adjustment': [
                '_calculate_volatility_adjustment',
                '_calculate_regime_adjustment',
                '_calculate_activity_adjustment'
            ],
            'optimization': [
                '_resolve_signal_conflicts',
                '_optimize_signal_portfolio',
                '_apply_risk_adjustments'
            ]
        }
        
        for algo_category, methods in algorithm_patterns.items():
            implemented_methods = []
            for method in methods:
                if method in py_content:
                    implemented_methods.append(method)
                    
                    # æª¢æŸ¥å¯¦ç¾è³ªé‡
                    method_pattern = f'def {method}.*?(?=def |\\Z)'
                    method_match = re.search(method_pattern, py_content, re.DOTALL)
                    if method_match:
                        method_code = method_match.group(0)
                        quality_score = self.assess_implementation_quality(method_code)
                        analysis['implementation_quality'][method] = quality_score
            
            analysis['python_algorithms'][algo_category] = {
                'methods': implemented_methods,
                'coverage': len(implemented_methods) / len(methods) if methods else 1.0
            }
        
        # è¨ˆç®—æ•´é«”æ¼”ç®—æ³•è¦†è“‹ç‡
        if algorithm_patterns:
            total_coverage = sum(cat['coverage'] for cat in analysis['python_algorithms'].values())
            analysis['algorithm_coverage'] = total_coverage / len(algorithm_patterns)
        
        # æª¢æŸ¥ç¼ºå¤±æ¼”ç®—æ³•
        for category, info in analysis['python_algorithms'].items():
            if info['coverage'] < 1.0:
                total_methods = len(algorithm_patterns[category])
                implemented_count = len(info['methods'])
                missing_count = total_methods - implemented_count
                analysis['missing_algorithms'].append(f"{category}: {missing_count} å€‹æ–¹æ³•ç¼ºå¤±")
        
        print(f"   JSON æ¼”ç®—æ³•å±¤ç´š: {len(analysis['json_algorithms'])}")
        print(f"   Python æ¼”ç®—æ³•é¡åˆ¥: {len(analysis['python_algorithms'])}")
        print(f"   æ¼”ç®—æ³•è¦†è“‹ç‡: {analysis['algorithm_coverage']:.1%}")
        
        for category, info in analysis['python_algorithms'].items():
            print(f"   - {category}: {info['coverage']:.1%} ({len(info['methods'])} å€‹æ–¹æ³•)")
        
        return analysis
    
    def assess_implementation_quality(self, method_code: str) -> float:
        """è©•ä¼°å¯¦ç¾è³ªé‡"""
        quality_score = 0.0
        
        # æª¢æŸ¥é …ç›®
        checks = [
            ('error_handling', 'try:' in method_code and 'except' in method_code),
            ('input_validation', 'if len(' in method_code or 'if not ' in method_code),
            ('numpy_usage', 'np.' in method_code),
            ('logging', 'logger.' in method_code),
            ('return_validation', 'max(' in method_code and 'min(' in method_code),
            ('documentation', '"""' in method_code or "'''" in method_code)
        ]
        
        for check_name, passed in checks:
            if passed:
                quality_score += 1.0 / len(checks)
        
        return quality_score
    
    def analyze_integration_completeness(self, json_spec: Dict, py_content: str) -> Dict[str, Any]:
        """åˆ†ææ•´åˆåº¦èˆ‡å®Œæ•´æ€§"""
        print("   æª¢æŸ¥æ•´åˆåº¦èˆ‡å®Œæ•´æ€§...")
        
        analysis = {
            'configuration_integration': 0.0,
            'error_handling_coverage': 0.0,
            'performance_monitoring': 0.0,
            'external_dependencies': {'json': [], 'python': []},
            'integration_score': 0.0
        }
        
        # é…ç½®æ•´åˆåº¦
        config_usage = [
            'self.config',
            '_load_config',
            'config_path'
        ]
        config_found = sum(1 for usage in config_usage if usage in py_content)
        analysis['configuration_integration'] = config_found / len(config_usage)
        
        # éŒ¯èª¤è™•ç†è¦†è“‹ç‡
        error_patterns = [
            'try:',
            'except Exception',
            'logger.error',
            'logger.warning'
        ]
        error_found = sum(1 for pattern in error_patterns if pattern in py_content)
        analysis['error_handling_coverage'] = min(1.0, error_found / len(error_patterns))
        
        # æ€§èƒ½ç›£æ§
        performance_patterns = [
            'time.time()',
            'processing_time',
            'performance',
            'time_budget'
        ]
        perf_found = sum(1 for pattern in performance_patterns if pattern in py_content)
        analysis['performance_monitoring'] = min(1.0, perf_found / len(performance_patterns))
        
        # JSON å¤–éƒ¨ä¾è³´
        if 'external_dependencies' in json_spec:
            analysis['external_dependencies']['json'] = json_spec['external_dependencies']
        
        # Python å¤–éƒ¨ä¾è³´
        import_patterns = re.findall(r'import\s+(\w+)|from\s+(\w+)', py_content)
        python_imports = [imp[0] or imp[1] for imp in import_patterns]
        analysis['external_dependencies']['python'] = python_imports
        
        # è¨ˆç®—æ•´åˆè©•åˆ†
        analysis['integration_score'] = (
            analysis['configuration_integration'] * 0.3 +
            analysis['error_handling_coverage'] * 0.3 +
            analysis['performance_monitoring'] * 0.4
        )
        
        print(f"   é…ç½®æ•´åˆ: {analysis['configuration_integration']:.1%}")
        print(f"   éŒ¯èª¤è™•ç†: {analysis['error_handling_coverage']:.1%}")
        print(f"   æ€§èƒ½ç›£æ§: {analysis['performance_monitoring']:.1%}")
        print(f"   æ•´åˆè©•åˆ†: {analysis['integration_score']:.1%}")
        
        return analysis
    
    def generate_final_report(self, all_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚ç²¾ç¢ºåŒ¹é…å ±å‘Š"""
        
        # è¨ˆç®—å„é …æ¬Šé‡è©•åˆ†
        weights = {
            'role': 0.20,      # æ¶æ§‹è§’è‰² 20%
            'dataflow': 0.25,  # æ•¸æ“šæµ 25%
            'layers': 0.20,    # å±¤ç´šé‚è¼¯ 20%
            'data_usage': 0.15, # æ•¸æ“šä½¿ç”¨ 15%
            'algorithms': 0.15, # æ¼”ç®—æ³• 15%
            'integration': 0.05 # æ•´åˆåº¦ 5%
        }
        
        scores = {
            'role': 1.0 if all_analysis['role']['role_match'] else 0.0,
            'dataflow': all_analysis['dataflow']['flow_consistency'],
            'layers': all_analysis['layers']['logic_completeness'],
            'data_usage': all_analysis['data_usage']['type_consistency'],
            'algorithms': all_analysis['algorithms']['algorithm_coverage'],
            'integration': all_analysis['integration']['integration_score']
        }
        
        # è¨ˆç®—ç¸½é«”åŒ¹é…åº¦
        total_score = sum(scores[key] * weights[key] for key in weights)
        
        # è­˜åˆ¥é—œéµå•é¡Œ
        critical_issues = []
        if scores['role'] < 1.0:
            critical_issues.append("æ¶æ§‹è§’è‰²å®šä½ä¸æº–ç¢º")
        if scores['dataflow'] < 0.8:
            critical_issues.append("æ•¸æ“šæµä¸å®Œæ•´")
        if scores['layers'] < 0.8:
            critical_issues.append("å±¤ç´šé‚è¼¯å¯¦ç¾ä¸è¶³")
        if scores['algorithms'] < 0.7:
            critical_issues.append("æ ¸å¿ƒæ¼”ç®—æ³•è¦†è“‹ä¸è¶³")
        
        # å„ªç§€å¯¦ç¾é»
        strengths = []
        if scores['role'] >= 1.0:
            strengths.append("æ¶æ§‹è§’è‰²å®šä½æ­£ç¢º")
        if scores['dataflow'] >= 0.9:
            strengths.append("æ•¸æ“šæµè¨­è¨ˆå„ªç§€")
        if scores['layers'] >= 0.9:
            strengths.append("å±¤ç´šé‚è¼¯å¯¦ç¾å®Œæ•´")
        if scores['algorithms'] >= 0.8:
            strengths.append("æ¼”ç®—æ³•å¯¦ç¾è¦†è“‹è‰¯å¥½")
        if scores['integration'] >= 0.8:
            strengths.append("ç³»çµ±æ•´åˆåº¦é«˜")
        
        report = {
            'total_match_percentage': total_score * 100,
            'component_scores': {k: v * 100 for k, v in scores.items()},
            'critical_issues': critical_issues,
            'strengths': strengths,
            'detailed_analysis': all_analysis,
            'recommendations': self.generate_recommendations(scores, all_analysis)
        }
        
        # æ‰“å°å ±å‘Š
        print(f"\nğŸ“Š ç¸½é«”åŒ¹é…åº¦: {report['total_match_percentage']:.1f}%")
        print(f"ğŸ—ï¸ æ¶æ§‹è§’è‰²: {scores['role']*100:.1f}%")
        print(f"ğŸ”„ æ•¸æ“šæµ: {scores['dataflow']*100:.1f}%") 
        print(f"ğŸ¢ å±¤ç´šé‚è¼¯: {scores['layers']*100:.1f}%")
        print(f"ğŸ“Š æ•¸æ“šä½¿ç”¨: {scores['data_usage']*100:.1f}%")
        print(f"âš¡ æ¼”ç®—æ³•: {scores['algorithms']*100:.1f}%")
        print(f"ğŸ”— æ•´åˆåº¦: {scores['integration']*100:.1f}%")
        
        if critical_issues:
            print(f"\nâŒ é—œéµå•é¡Œ: {', '.join(critical_issues)}")
        
        if strengths:
            print(f"\nâœ… å„ªç§€å¯¦ç¾: {', '.join(strengths)}")
        
        return report
    
    def generate_recommendations(self, scores: Dict[str, float], all_analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        if scores['role'] < 1.0:
            recommendations.append("ç¢ºèªä¸¦ä¿®æ­£æ¶æ§‹è§’è‰²å®šä½ç‚ºä¿¡è™Ÿé©æ‡‰å™¨")
        
        if scores['dataflow'] < 0.8:
            recommendations.append("è£œå……ç¼ºå¤±çš„æ•¸æ“šæµæ¥å£å’Œè½‰æ›é‚è¼¯")
        
        if scores['layers'] < 0.8:
            recommendations.append("å®Œå–„å±¤ç´šè™•ç†é‚è¼¯å’ŒéŒ¯èª¤è™•ç†æ©Ÿåˆ¶")
        
        if scores['data_usage'] < 0.8:
            recommendations.append("å°é½Šæ•¸æ“šçµæ§‹å®šç¾©å’Œä½¿ç”¨æ–¹å¼")
        
        if scores['algorithms'] < 0.7:
            recommendations.append("å¯¦ç¾ç¼ºå¤±çš„æ ¸å¿ƒæ¼”ç®—æ³•æ¨¡çµ„")
        
        if scores['integration'] < 0.7:
            recommendations.append("åŠ å¼·é…ç½®æ•´åˆå’Œæ€§èƒ½ç›£æ§")
        
        return recommendations

if __name__ == "__main__":
    analyzer = Phase1BPreciseAnalyzer()
    report = analyzer.run_precise_analysis()
