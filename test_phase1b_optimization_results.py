#!/usr/bin/env python3
"""
ğŸ¯ æ¸¬è©¦ phase1b_volatility_adaptation.py å„ªåŒ–å¾Œçš„åŒ¹é…åº¦
åŸºæ–¼ JSON è¦æ ¼é€²è¡Œç²¾æº–é©—è­‰
"""

import json
import os
import importlib.util
from typing import Dict, Any, List
import ast
import inspect

class OptimizedPhase1BAnalyzer:
    """å„ªåŒ–å¾Œçš„ Phase1B åˆ†æå™¨"""
    
    def __init__(self):
        self.json_spec_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation_dependency.json"
        self.implementation_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation.py"
        
    def load_json_spec(self) -> Dict[str, Any]:
        """è¼‰å…¥ JSON è¦æ ¼"""
        try:
            with open(self.json_spec_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ JSON è¦æ ¼è¼‰å…¥å¤±æ•—: {e}")
            return {}
    
    def load_python_implementation(self):
        """è¼‰å…¥ Python å¯¦ç¾"""
        try:
            spec = importlib.util.spec_from_file_location("phase1b_module", self.implementation_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            return module
        except Exception as e:
            print(f"âŒ Python å¯¦ç¾è¼‰å…¥å¤±æ•—: {e}")
            return None
    
    def analyze_file_content(self) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶å…§å®¹"""
        try:
            with open(self.implementation_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # AST åˆ†æ
            tree = ast.parse(content)
            
            # æå–æ–¹æ³•åç¨±
            methods = []
            classes = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    methods.append(node.name)
                elif isinstance(node, ast.ClassDef):
                    classes.append(node.name)
                    # æå–é¡ä¸­çš„æ–¹æ³•
                    for item in node.body:
                        if isinstance(item, ast.FunctionDef):
                            methods.append(f"{node.name}.{item.name}")
            
            return {
                "content": content,
                "methods": methods,
                "classes": classes,
                "lines": len(content.split('\n'))
            }
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶å…§å®¹åˆ†æå¤±æ•—: {e}")
            return {}
    
    def check_enhanced_architecture_match(self, json_spec: Dict, implementation_analysis: Dict) -> Dict[str, Any]:
        """å¢å¼·æ¶æ§‹åŒ¹é…æª¢æŸ¥"""
        content = implementation_analysis.get("content", "")
        methods = implementation_analysis.get("methods", [])
        
        # 1. æª¢æŸ¥ä¸»è¦è™•ç†æ–¹æ³•
        main_processing = {
            "process_signals_with_volatility_adaptation": "process_signals_with_volatility_adaptation" in content,
            "accepts_standardized_signals": "standardized_signals" in content and "indicator_outputs" in content,
            "returns_unified_format": "unified_signal_candidate_pool" in content or "Dict[str, Any]" in content
        }
        
        # 2. æª¢æŸ¥ 4 å±¤æ¶æ§‹
        layer_architecture = {
            "layer_1_data_collection": "layer_1_data_collection" in content,
            "layer_2_volatility_metrics": "layer_2_volatility_metrics" in content, 
            "layer_3_adaptive_parameters": "layer_3_adaptive_parameters" in content,
            "layer_4_strategy_signals": "layer_4_strategy_signals" in content
        }
        
        # 3. æª¢æŸ¥ JSON è¦æ±‚çš„è¨ˆç®—æ–¹æ³•
        required_calculations = {
            "enhanced_volatility_percentile_calculation": "_calculate_enhanced_volatility_percentile" in content,
            "regime_detection_with_multi_confirmation": "_detect_volatility_regime" in content and "multi_confirmation" in content,
            "signal_threshold_adaptation": "_adapt_signal_threshold" in content,
            "position_size_scaling": "_scale_position_size" in content,
            "timeframe_optimization": "_optimize_timeframe" in content
        }
        
        # 4. æª¢æŸ¥æ³¢å‹•æ€§ä¿¡è™Ÿç”Ÿæˆæ–¹æ³•
        volatility_signal_methods = {
            "breakout_signal_generation": "_generate_breakout_signals" in content,
            "mean_reversion_signal_generation": "_generate_mean_reversion_signals" in content,
            "regime_change_signal_generation": "_generate_regime_change_signals" in content
        }
        
        # 5. æª¢æŸ¥æ•¸æ“šè™•ç†èƒ½åŠ›
        data_processing = {
            "ohlcv_data_processing": "ohlcv" in content.lower() or "_process_ohlcv_data" in content,
            "high_frequency_data_processing": "_process_high_frequency_data" in content,
            "historical_volatility_calculation": "_calculate_historical_volatility" in content,
            "intraday_volatility_tracking": "intraday_volatility" in content
        }
        
        # 6. æª¢æŸ¥é«˜éšåŠŸèƒ½
        advanced_features = {
            "signal_continuity_analysis": "_analyze_signal_continuity" in content,
            "dynamic_time_distribution": "_analyze_dynamic_time_distribution" in content,
            "regime_monitoring": "_regime_monitor" in content,
            "performance_monitoring": "_performance_monitor" in content
        }
        
        # 7. æª¢æŸ¥æ•¸æ“šçµæ§‹
        data_structures = {
            "VolatilityMetrics": "class VolatilityMetrics" in content or "@dataclass" in content and "VolatilityMetrics" in content,
            "AdaptiveSignalAdjustment": "AdaptiveSignalAdjustment" in content,
            "SignalContinuityMetrics": "SignalContinuityMetrics" in content,
            "DynamicTimeDistribution": "DynamicTimeDistribution" in content
        }
        
        return {
            "main_processing": main_processing,
            "layer_architecture": layer_architecture,
            "required_calculations": required_calculations,
            "volatility_signal_methods": volatility_signal_methods,
            "data_processing": data_processing,
            "advanced_features": advanced_features,
            "data_structures": data_structures
        }
    
    def calculate_optimization_score(self, match_results: Dict[str, Any]) -> Dict[str, float]:
        """è¨ˆç®—å„ªåŒ–è©•åˆ†"""
        scores = {}
        total_score = 0
        total_weight = 0
        
        # å®šç¾©æ¬Šé‡
        weights = {
            "main_processing": 20,
            "layer_architecture": 25,
            "required_calculations": 20,
            "volatility_signal_methods": 15,
            "data_processing": 10,
            "advanced_features": 10,
            "data_structures": 10
        }
        
        for category, items in match_results.items():
            if category in weights:
                category_score = sum(items.values()) / len(items) if items else 0
                scores[category] = category_score
                total_score += category_score * weights[category]
                total_weight += weights[category]
        
        overall_score = total_score / total_weight if total_weight > 0 else 0
        scores["overall"] = overall_score
        
        return scores
    
    def generate_optimization_report(self, scores: Dict[str, float], match_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ¯ PHASE1B å„ªåŒ–å¾ŒåŒ¹é…åº¦åˆ†æå ±å‘Š")
        report.append("=" * 80)
        
        # ç¸½é«”è©•åˆ†
        overall_score = scores.get("overall", 0)
        report.append(f"\nğŸ“Š ç¸½é«”åŒ¹é…åº¦: {overall_score:.1%}")
        
        # æ”¹é€²ç¨‹åº¦ï¼ˆèˆ‡ä¹‹å‰çš„ 72.8% æ¯”è¼ƒï¼‰
        previous_score = 0.728
        improvement = overall_score - previous_score
        report.append(f"ğŸ“ˆ å„ªåŒ–æ”¹é€²: {improvement:+.1%} (å¾ {previous_score:.1%} æå‡åˆ° {overall_score:.1%})")
        
        if improvement > 0:
            report.append("âœ… å„ªåŒ–æˆåŠŸï¼")
        else:
            report.append("âš ï¸ éœ€è¦é€²ä¸€æ­¥å„ªåŒ–")
        
        report.append("\n" + "=" * 50)
        report.append("ğŸ“‹ è©³ç´°åˆ†æçµæœ:")
        report.append("=" * 50)
        
        # å„é …ç›®è©•åˆ†
        category_names = {
            "main_processing": "ä¸»è¦è™•ç†é‚è¼¯",
            "layer_architecture": "4å±¤æ¶æ§‹å¯¦ç¾",
            "required_calculations": "å¿…éœ€è¨ˆç®—æ–¹æ³•",
            "volatility_signal_methods": "æ³¢å‹•æ€§ä¿¡è™Ÿæ–¹æ³•",
            "data_processing": "æ•¸æ“šè™•ç†èƒ½åŠ›",
            "advanced_features": "é«˜éšåŠŸèƒ½",
            "data_structures": "æ•¸æ“šçµæ§‹å®šç¾©"
        }
        
        for category, score in scores.items():
            if category != "overall" and category in category_names:
                status = "âœ…" if score >= 0.8 else "âš ï¸" if score >= 0.6 else "âŒ"
                report.append(f"{status} {category_names[category]}: {score:.1%}")
                
                # é¡¯ç¤ºå…·é«”åŒ¹é…é …ç›®
                if category in match_results:
                    items = match_results[category]
                    for item, matched in items.items():
                        item_status = "âœ“" if matched else "âœ—"
                        report.append(f"    {item_status} {item}")
        
        # å„ªåŒ–å»ºè­°
        report.append("\n" + "=" * 50)
        report.append("ğŸ’¡ å„ªåŒ–å»ºè­°:")
        report.append("=" * 50)
        
        low_score_categories = [cat for cat, score in scores.items() 
                              if cat != "overall" and score < 0.8]
        
        if not low_score_categories:
            report.append("ğŸ‰ æ‰€æœ‰é …ç›®éƒ½å·²é”åˆ°é«˜åŒ¹é…åº¦æ¨™æº–ï¼")
        else:
            for category in low_score_categories:
                missing_items = [item for item, matched in match_results.get(category, {}).items() 
                               if not matched]
                if missing_items:
                    report.append(f"\nğŸ“Œ {category_names.get(category, category)}:")
                    for item in missing_items:
                        report.append(f"   - éœ€è¦å¯¦ç¾: {item}")
        
        report.append("\n" + "=" * 80)
        
        return "\n".join(report)
    
    def run_optimization_analysis(self):
        """é‹è¡Œå„ªåŒ–åˆ†æ"""
        print("ğŸš€ é–‹å§‹ Phase1B å„ªåŒ–å¾Œåˆ†æ...")
        
        # è¼‰å…¥æ•¸æ“š
        json_spec = self.load_json_spec()
        if not json_spec:
            return
        
        implementation_analysis = self.analyze_file_content()
        if not implementation_analysis:
            return
        
        # åˆ†æåŒ¹é…åº¦
        match_results = self.check_enhanced_architecture_match(json_spec, implementation_analysis)
        
        # è¨ˆç®—è©•åˆ†
        scores = self.calculate_optimization_score(match_results)
        
        # ç”Ÿæˆå ±å‘Š
        report = self.generate_optimization_report(scores, match_results)
        print(report)
        
        # ä¿å­˜å ±å‘Š
        report_path = "/Users/henrychang/Desktop/Trading-X/phase1b_optimization_results.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“„ å ±å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return scores, match_results

if __name__ == "__main__":
    analyzer = OptimizedPhase1BAnalyzer()
    analyzer.run_optimization_analysis()
