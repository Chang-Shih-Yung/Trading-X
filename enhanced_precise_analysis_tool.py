"""
ğŸ¯ å¢å¼·ç²¾ç¢ºæ·±åº¦åˆ†æå·¥å…· v2.0 - unified_signal_candidate_pool.py vs JSON è¦ç¯„
ğŸ¯ ä¿®æ­£ AST è§£æéŒ¯èª¤ï¼Œç¢ºä¿ 100% æº–ç¢ºæª¢æ¸¬
"""

import json
import ast
import re
import logging
from typing import Dict, List, Set, Any, Tuple
from pathlib import Path
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ComponentAnalysis:
    """çµ„ä»¶åˆ†æçµæœ"""
    name: str
    required: bool
    implemented: bool
    details: Dict[str, Any]
    issues: List[str]

class EnhancedPreciseAnalyzer:
    """å¢å¼·ç²¾ç¢ºåˆ†æå™¨"""
    
    def __init__(self, json_spec_path: str, code_file_path: str):
        self.json_spec_path = json_spec_path
        self.code_file_path = code_file_path
        
        # è¼‰å…¥ JSON è¦ç¯„
        with open(json_spec_path, 'r', encoding='utf-8') as f:
            self.json_spec = json.load(f)["UNIFIED_SIGNAL_CANDIDATE_POOL_V3_DEPENDENCY"]
        
        # è®€å–ä»£ç¢¼
        with open(code_file_path, 'r', encoding='utf-8') as f:
            self.code_content = f.read()
        
        # è§£æ AST
        try:
            self.ast_tree = ast.parse(self.code_content)
        except SyntaxError as e:
            logger.error(f"ä»£ç¢¼èªæ³•éŒ¯èª¤: {e}")
            raise
        
        # åˆ†æçµæœ
        self.analysis_results: List[ComponentAnalysis] = []
        
    def analyze_complete_compliance(self) -> Dict[str, Any]:
        """å®Œæ•´åˆè¦æ€§åˆ†æ"""
        logger.info("ğŸ” é–‹å§‹å¢å¼·ç²¾ç¢ºæ·±åº¦åˆ†æ...")
        
        # 1. æ ¸å¿ƒé¡åˆ¥èˆ‡æ–¹æ³•åˆ†æ
        self._analyze_core_classes_and_methods()
        
        # 2. AI å­¸ç¿’å¼•æ“çµ„ä»¶åˆ†æ
        self._analyze_ai_learning_components()
        
        # 3. Phase1 å±¤ç´šå¯¦ç¾åˆ†æ
        self._analyze_phase1_layers()
        
        # 4. æ•¸æ“šæµèˆ‡ä¿¡è™Ÿè™•ç†åˆ†æ
        self._analyze_data_flows_and_signals()
        
        # 5. æ€§èƒ½ç›£æ§åˆ†æ
        self._analyze_performance_monitoring()
        
        # 6. ä¸ƒç¶­åº¦è©•åˆ†ç³»çµ±åˆ†æ
        self._analyze_seven_dimensional_scoring()
        
        # 7. EPL é è™•ç†å„ªåŒ–åˆ†æ
        self._analyze_epl_preprocessing()
        
        # 8. å†—é¤˜ä»£ç¢¼æª¢æ¸¬
        self._detect_truly_redundant_code()
        
        # 9. ç”Ÿæˆæœ€çµ‚å ±å‘Š
        return self._generate_enhanced_report()
    
    def _extract_all_methods(self) -> Dict[str, List[str]]:
        """æå–æ‰€æœ‰é¡åˆ¥çš„æ–¹æ³•"""
        classes_methods = {}
        
        for node in ast.walk(self.ast_tree):
            if isinstance(node, ast.ClassDef):
                methods = []
                for item in node.body:
                    if isinstance(item, ast.FunctionDef):
                        methods.append(item.name)
                classes_methods[node.name] = methods
        
        return classes_methods
    
    def _extract_all_functions(self) -> List[str]:
        """æå–æ‰€æœ‰å‡½æ•¸å"""
        functions = []
        for node in ast.walk(self.ast_tree):
            if isinstance(node, ast.FunctionDef):
                functions.append(node.name)
        return functions
    
    def _analyze_core_classes_and_methods(self):
        """åˆ†ææ ¸å¿ƒé¡åˆ¥èˆ‡æ–¹æ³•"""
        logger.info("ğŸ“Š åˆ†ææ ¸å¿ƒé¡åˆ¥èˆ‡æ–¹æ³•...")
        
        classes_methods = self._extract_all_methods()
        
        # JSON è¦ç¯„è¦æ±‚çš„æ ¸å¿ƒé¡åˆ¥
        required_classes = {
            "UnifiedSignalCandidatePoolV3": [
                "generate_signal_candidates_v3",
                "_layer_0_complete_phase1_sync", 
                "_layer_1_enhanced_multi_source_fusion",
                "_layer_2_epl_preprocessing_optimization",
                "_layer_ai_adaptive_learning"
            ],
            "StandardizedSignal": [],
            "SevenDimensionalScore": [],
            "AILearningMetrics": [],
            "MarketRegimeState": [],
            "SignalQualityValidator": [
                "validate_signal_strength_range",
                "validate_phase1a_signal",
                "validate_indicator_signal", 
                "validate_phase1b_signal",
                "validate_phase1c_signal"
            ],
            "AIAdaptiveLearningEngine": [
                "learn_from_epl_feedback",
                "predict_epl_pass_probability",
                "_calculate_signal_contribution",
                "_adjust_source_weights"
            ],
            "SevenDimensionalScorer": [
                "calculate_comprehensive_score",
                "_calculate_data_quality",
                "_calculate_market_consistency",
                "_calculate_time_effect",
                "_calculate_liquidity_factor",
                "_calculate_historical_accuracy",
                "_apply_ai_enhancement"
            ]
        }
        
        for class_name, required_methods in required_classes.items():
            implemented = class_name in classes_methods
            issues = []
            
            if implemented:
                class_methods = classes_methods[class_name]
                missing_methods = [m for m in required_methods if m not in class_methods]
                if missing_methods:
                    issues.append(f"ç¼ºå°‘æ–¹æ³•: {missing_methods}")
                    implemented = False  # éƒ¨åˆ†å¯¦ç¾è¦–ç‚ºæœªå®Œå…¨å¯¦ç¾
            else:
                issues.append(f"ç¼ºå°‘æ•´å€‹é¡åˆ¥: {class_name}")
            
            self.analysis_results.append(ComponentAnalysis(
                name=f"Class_{class_name}",
                required=True,
                implemented=implemented,
                details={"methods": classes_methods.get(class_name, [])},
                issues=issues
            ))
    
    def _analyze_ai_learning_components(self):
        """åˆ†æ AI å­¸ç¿’å¼•æ“çµ„ä»¶"""
        logger.info("ğŸ§  åˆ†æ AI å­¸ç¿’å¼•æ“çµ„ä»¶...")
        
        # JSON è¦ç¯„çš„ AI å­¸ç¿’çµ„ä»¶
        ai_spec = self.json_spec["ğŸ§  ai_adaptive_learning_engine"]["Layer_AI_Learning_Engine"]["operations"]
        
        required_components = {
            "historical_decision_learning": {
                "patterns": ["learning_data", "EPL_decision_history", "learning_metrics", "weight_adjustment"],
                "weight": 0.3
            },
            "predictive_filtering": {
                "patterns": ["ml_model", "feature_engineering", "prediction_target", "filtering_thresholds", "predict_epl_pass_probability"],
                "weight": 0.4
            },
            "real_time_adaptation": {
                "patterns": ["adaptation_trigger", "fast_learning", "emergency_adjustment", "stability_guarantee"],
                "weight": 0.3
            }
        }
        
        for component, config in required_components.items():
            implemented_patterns = []
            missing_patterns = []
            
            for pattern in config["patterns"]:
                if pattern in self.code_content:
                    implemented_patterns.append(pattern)
                else:
                    missing_patterns.append(pattern)
            
            implementation_rate = len(implemented_patterns) / len(config["patterns"])
            implemented = implementation_rate >= 0.8  # 80% å¯¦ç¾é–¾å€¼
            
            issues = []
            if missing_patterns:
                issues.append(f"ç¼ºå°‘åŠŸèƒ½: {missing_patterns}")
            
            self.analysis_results.append(ComponentAnalysis(
                name=f"AI_{component}",
                required=True,
                implemented=implemented,
                details={
                    "implementation_rate": f"{implementation_rate:.1%}",
                    "implemented": implemented_patterns,
                    "missing": missing_patterns
                },
                issues=issues
            ))
    
    def _analyze_phase1_layers(self):
        """åˆ†æ Phase1 å±¤ç´šå¯¦ç¾"""
        logger.info("ğŸ”„ åˆ†æ Phase1 å±¤ç´šå¯¦ç¾...")
        
        # JSON è¦ç¯„çš„å±¤ç´šè¦æ±‚
        required_layers = {
            "Layer_0_Complete_Phase1_Sync": {
                "method": "_layer_0_complete_phase1_sync",
                "components": ["unified_timestamp_sync", "data_flow_integrity", "extreme_market_fast_track"],
                "time_target": "3ms"
            },
            "Layer_1_Multi_Source_Fusion": {
                "method": "_layer_1_enhanced_multi_source_fusion", 
                "components": ["intelligent_signal_collection", "seven_dimensional_comprehensive_scoring"],
                "time_target": "12ms"
            },
            "Layer_2_EPL_Preprocessor": {
                "method": "_layer_2_epl_preprocessing_optimization",
                "components": ["epl_oriented_filtering", "epl_input_formatting", "emergency_signal_priority_channel"],
                "time_target": "8ms"
            },
            "Layer_AI_Adaptive_Learning": {
                "method": "_layer_ai_adaptive_learning",
                "components": ["real_time_adaptation", "weight_adjustment", "learning_feedback"],
                "time_target": "5ms"
            }
        }
        
        for layer_name, layer_config in required_layers.items():
            method_name = layer_config["method"]
            method_exists = method_name in self.code_content
            
            issues = []
            component_coverage = 0
            
            if method_exists:
                # æª¢æŸ¥æ–¹æ³•å…§å®¹æ˜¯å¦åŒ…å«å¿…è¦çµ„ä»¶
                method_pattern = rf"def {method_name}.*?(?=def|\Z)"
                method_match = re.search(method_pattern, self.code_content, re.DOTALL)
                
                if method_match:
                    method_content = method_match.group(0)
                    implemented_components = []
                    
                    for component in layer_config["components"]:
                        # æª¢æŸ¥çµ„ä»¶é—œéµå­—æ˜¯å¦åœ¨æ–¹æ³•ä¸­
                        component_keywords = component.split("_")
                        if any(keyword in method_content.lower() for keyword in component_keywords):
                            implemented_components.append(component)
                    
                    component_coverage = len(implemented_components) / len(layer_config["components"])
                    
                    if component_coverage < 1.0:
                        missing_components = [c for c in layer_config["components"] if c not in implemented_components]
                        issues.append(f"çµ„ä»¶ä¸å®Œæ•´: {missing_components}")
                else:
                    issues.append("æ–¹æ³•å…§å®¹ç„¡æ³•è§£æ")
            else:
                issues.append(f"ç¼ºå°‘æ–¹æ³•: {method_name}")
            
            # æª¢æŸ¥æ€§èƒ½ç›£æ§
            time_monitoring = f"{layer_config['time_target']}" in self.code_content
            if not time_monitoring:
                issues.append(f"ç¼ºå°‘ {layer_config['time_target']} æ€§èƒ½ç›£æ§")
            
            implemented = method_exists and component_coverage >= 0.8 and time_monitoring
            
            self.analysis_results.append(ComponentAnalysis(
                name=layer_name,
                required=True,
                implemented=implemented,
                details={
                    "method_exists": method_exists,
                    "component_coverage": f"{component_coverage:.1%}",
                    "time_monitoring": time_monitoring
                },
                issues=issues
            ))
    
    def _analyze_data_flows_and_signals(self):
        """åˆ†ææ•¸æ“šæµèˆ‡ä¿¡è™Ÿè™•ç†"""
        logger.info("ğŸ“ˆ åˆ†ææ•¸æ“šæµèˆ‡ä¿¡è™Ÿè™•ç†...")
        
        # JSON è¦ç¯„çš„è¼¸å…¥æº
        input_sources = self.json_spec["ğŸŒ complete_input_source_integration"]
        
        required_signal_types = {
            "phase1a_input": ["PRICE_BREAKOUT", "VOLUME_SURGE", "MOMENTUM_SHIFT", "EXTREME_EVENT"],
            "indicator_graph_input": ["RSI_signals", "MACD_signals", "BB_signals", "Volume_signals"],
            "phase1b_input": ["VOLATILITY_BREAKOUT", "REGIME_CHANGE", "MEAN_REVERSION"],
            "phase1c_input": ["LIQUIDITY_SHOCK", "INSTITUTIONAL_FLOW", "SENTIMENT_DIVERGENCE", "LIQUIDITY_REGIME_CHANGE"]
        }
        
        # æª¢æŸ¥ä¿¡è™Ÿæ”¶é›†æ–¹æ³•
        signal_collection_methods = {
            "phase1a": "_collect_phase1a_signals",
            "indicator_graph": "_collect_indicator_signals", 
            "phase1b": "_collect_phase1b_signals",
            "phase1c": "_collect_phase1c_signals"
        }
        
        for source_key, signal_types in required_signal_types.items():
            source_name = source_key.replace("_input", "")
            method_name = signal_collection_methods.get(source_name, f"_collect_{source_name}_signals")
            
            method_exists = method_name in self.code_content
            signal_coverage = 0
            issues = []
            
            if method_exists:
                # æª¢æŸ¥ä¿¡è™Ÿé¡å‹è¦†è“‹
                method_pattern = rf"def {method_name}.*?(?=def|\Z)"
                method_match = re.search(method_pattern, self.code_content, re.DOTALL)
                
                if method_match:
                    method_content = method_match.group(0)
                    implemented_signals = []
                    
                    for signal_type in signal_types:
                        if signal_type in method_content:
                            implemented_signals.append(signal_type)
                    
                    signal_coverage = len(implemented_signals) / len(signal_types)
                    
                    if signal_coverage < 1.0:
                        missing_signals = [s for s in signal_types if s not in implemented_signals]
                        issues.append(f"ç¼ºå°‘ä¿¡è™Ÿé¡å‹: {missing_signals}")
            else:
                issues.append(f"ç¼ºå°‘æ–¹æ³•: {method_name}")
            
            implemented = method_exists and signal_coverage >= 0.8
            
            self.analysis_results.append(ComponentAnalysis(
                name=f"DataFlow_{source_name}",
                required=True,
                implemented=implemented,
                details={
                    "method": method_name,
                    "signal_coverage": f"{signal_coverage:.1%}",
                    "expected_signals": signal_types
                },
                issues=issues
            ))
    
    def _analyze_performance_monitoring(self):
        """åˆ†ææ€§èƒ½ç›£æ§"""
        logger.info("âš¡ åˆ†ææ€§èƒ½ç›£æ§...")
        
        # JSON è¦ç¯„çš„æ€§èƒ½ç›®æ¨™
        perf_targets = self.json_spec["ğŸ¯ v3_0_performance_targets"]["layered_processing_time"]
        
        required_monitoring = {
            "layer_0_phase1_sync": "3ms",
            "layer_1_multi_fusion": "12ms",
            "layer_2_epl_preprocessor": "8ms", 
            "layer_ai_learning": "5ms",
            "total_processing_time": "28ms"
        }
        
        monitoring_patterns = [
            "elapsed.*time",
            "layer_.*_time",
            "total_time",
            "performance_status",
            "time.*target",
            "ms.*ç›®æ¨™"
        ]
        
        # æª¢æŸ¥æ€§èƒ½ç›£æ§å¯¦ç¾
        monitoring_found = []
        for pattern in monitoring_patterns:
            matches = re.findall(pattern, self.code_content, re.IGNORECASE)
            monitoring_found.extend(matches)
        
        # æª¢æŸ¥å…·é«”æ™‚é–“ç›®æ¨™
        time_targets_found = []
        for target, time_limit in required_monitoring.items():
            if time_limit in self.code_content:
                time_targets_found.append(target)
        
        coverage = len(time_targets_found) / len(required_monitoring)
        implemented = coverage >= 0.8 and len(monitoring_found) >= 3
        
        issues = []
        if coverage < 1.0:
            missing_targets = [t for t in required_monitoring.keys() if t not in time_targets_found]
            issues.append(f"ç¼ºå°‘æ™‚é–“ç›®æ¨™: {missing_targets}")
        
        if len(monitoring_found) < 3:
            issues.append("æ€§èƒ½ç›£æ§æ©Ÿåˆ¶ä¸è¶³")
        
        self.analysis_results.append(ComponentAnalysis(
            name="Performance_Monitoring",
            required=True,
            implemented=implemented,
            details={
                "target_coverage": f"{coverage:.1%}",
                "monitoring_patterns": len(monitoring_found),
                "time_targets_found": time_targets_found
            },
            issues=issues
        ))
    
    def _analyze_seven_dimensional_scoring(self):
        """åˆ†æä¸ƒç¶­åº¦è©•åˆ†ç³»çµ±"""
        logger.info("ğŸ“Š åˆ†æä¸ƒç¶­åº¦è©•åˆ†ç³»çµ±...")
        
        # JSON è¦ç¯„çš„ä¸ƒç¶­åº¦
        scoring_spec = self.json_spec["ğŸ”„ phase1_complete_flow_integration"]["Layer_1_Multi_Source_Fusion"]["operations"]["ğŸ­ seven_dimensional_comprehensive_scoring"]["scoring_dimensions"]
        
        required_dimensions = {
            "signal_strength": 0.25,
            "confidence": 0.20,
            "data_quality": 0.15,
            "market_consistency": 0.12,
            "time_effect": 0.10,
            "liquidity_factor": 0.10,
            "historical_accuracy": 0.08
        }
        
        # æª¢æŸ¥ SevenDimensionalScore é¡å’Œç›¸é—œè¨ˆç®—
        dimensions_found = []
        for dimension in required_dimensions.keys():
            if dimension in self.code_content:
                dimensions_found.append(dimension)
        
        # æª¢æŸ¥è¨ˆç®—æ–¹æ³•
        calculation_methods = [
            "_calculate_data_quality",
            "_calculate_market_consistency", 
            "_calculate_time_effect",
            "_calculate_liquidity_factor",
            "_calculate_historical_accuracy"
        ]
        
        methods_found = []
        for method in calculation_methods:
            if method in self.code_content:
                methods_found.append(method)
        
        dimension_coverage = len(dimensions_found) / len(required_dimensions)
        method_coverage = len(methods_found) / len(calculation_methods)
        overall_coverage = (dimension_coverage + method_coverage) / 2
        
        implemented = overall_coverage >= 0.8
        
        issues = []
        if dimension_coverage < 1.0:
            missing_dimensions = [d for d in required_dimensions.keys() if d not in dimensions_found]
            issues.append(f"ç¼ºå°‘ç¶­åº¦: {missing_dimensions}")
        
        if method_coverage < 1.0:
            missing_methods = [m for m in calculation_methods if m not in methods_found]
            issues.append(f"ç¼ºå°‘è¨ˆç®—æ–¹æ³•: {missing_methods}")
        
        self.analysis_results.append(ComponentAnalysis(
            name="Seven_Dimensional_Scoring",
            required=True,
            implemented=implemented,
            details={
                "dimension_coverage": f"{dimension_coverage:.1%}",
                "method_coverage": f"{method_coverage:.1%}",
                "overall_coverage": f"{overall_coverage:.1%}"
            },
            issues=issues
        ))
    
    def _analyze_epl_preprocessing(self):
        """åˆ†æ EPL é è™•ç†å„ªåŒ–"""
        logger.info("ğŸ¯ åˆ†æ EPL é è™•ç†å„ªåŒ–...")
        
        # JSON è¦ç¯„çš„ EPL é è™•ç†çµ„ä»¶
        epl_spec = self.json_spec["ğŸ”„ phase1_complete_flow_integration"]["Layer_2_EPL_Preprocessor"]["operations"]
        
        required_components = {
            "epl_success_prediction": ["predict_epl_pass_probability", "XGBoost", "prediction_accuracy"],
            "signal_optimization": ["enhanced_deduplication", "complementary_selection", "quantity_control", "quality_assurance"],
            "epl_input_formatting": ["standardized_output", "epl_optimized_fields"],
            "emergency_signal_priority": ["extreme_market_signals", "emergency_processing"]
        }
        
        component_scores = {}
        overall_issues = []
        
        for component, keywords in required_components.items():
            found_keywords = []
            for keyword in keywords:
                if keyword.lower() in self.code_content.lower():
                    found_keywords.append(keyword)
            
            coverage = len(found_keywords) / len(keywords)
            component_scores[component] = coverage
            
            if coverage < 0.8:
                missing = [k for k in keywords if k not in found_keywords]
                overall_issues.append(f"{component} ä¸å®Œæ•´: {missing}")
        
        overall_coverage = sum(component_scores.values()) / len(component_scores)
        implemented = overall_coverage >= 0.7
        
        self.analysis_results.append(ComponentAnalysis(
            name="EPL_Preprocessing",
            required=True,
            implemented=implemented,
            details={
                "component_scores": {k: f"{v:.1%}" for k, v in component_scores.items()},
                "overall_coverage": f"{overall_coverage:.1%}"
            },
            issues=overall_issues
        ))
    
    def _detect_truly_redundant_code(self):
        """æª¢æ¸¬çœŸæ­£çš„å†—é¤˜ä»£ç¢¼"""
        logger.info("ğŸ§¹ æª¢æ¸¬å†—é¤˜ä»£ç¢¼...")
        
        # æª¢æŸ¥æœªä½¿ç”¨çš„å°å…¥ï¼ˆæ›´ç²¾ç¢ºï¼‰
        import_lines = re.findall(r'^(import .*|from .* import .*)$', self.code_content, re.MULTILINE)
        truly_unused = []
        
        for import_line in import_lines:
            # æª¢æŸ¥ç‰¹å®šå°å…¥æ˜¯å¦çœŸçš„æœªä½¿ç”¨
            if "warnings" in import_line and "warnings.filterwarnings" not in self.code_content:
                truly_unused.append(import_line)
            elif "pickle" in import_line and "pickle." not in self.code_content:
                truly_unused.append(import_line)
        
        # æª¢æŸ¥æœªä½¿ç”¨çš„æ–¹æ³•ï¼ˆæ’é™¤å¯èƒ½çš„å…¬å…± APIï¼‰
        all_functions = self._extract_all_functions()
        public_api_methods = [
            "generate_signal_candidates_v3", "learn_from_epl_feedback", 
            "get_performance_report", "get_candidates_by_priority", "clear_expired_candidates"
        ]
        
        potentially_unused = []
        for func in all_functions:
            if (func not in public_api_methods and 
                not func.startswith("__") and 
                self.code_content.count(f"{func}(") <= 1):  # åªåœ¨å®šç¾©è™•å‡ºç¾
                potentially_unused.append(func)
        
        # æª¢æŸ¥æœªä½¿ç”¨çš„è®Šæ•¸ï¼ˆé¡è®Šæ•¸ï¼‰
        unused_vars = []
        class_var_pattern = r'self\.(\w+) = '
        class_vars = re.findall(class_var_pattern, self.code_content)
        
        for var in set(class_vars):  # å»é‡
            if self.code_content.count(f"self.{var}") <= 1:  # åªåœ¨è³¦å€¼è™•å‡ºç¾
                unused_vars.append(f"self.{var}")
        
        all_redundant = truly_unused + potentially_unused + unused_vars
        
        if all_redundant:
            self.analysis_results.append(ComponentAnalysis(
                name="Redundant_Code",
                required=False,
                implemented=True,  # æœ‰å†—é¤˜ä»£ç¢¼
                details={
                    "unused_imports": truly_unused,
                    "potentially_unused_methods": potentially_unused,
                    "unused_variables": unused_vars
                },
                issues=[f"ç™¼ç¾ {len(all_redundant)} é …æ½›åœ¨å†—é¤˜ä»£ç¢¼"]
            ))
    
    def _generate_enhanced_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¢å¼·åˆ†æå ±å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆå¢å¼·åˆ†æå ±å‘Š...")
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        total_required = len([r for r in self.analysis_results if r.required])
        fully_implemented = len([r for r in self.analysis_results if r.required and r.implemented])
        partially_implemented = len([r for r in self.analysis_results if r.required and not r.implemented and r.issues])
        missing_components = len([r for r in self.analysis_results if r.required and not r.implemented and not r.issues])
        
        # è¨ˆç®—åŒ¹é…åº¦
        if total_required > 0:
            match_rate = fully_implemented / total_required
        else:
            match_rate = 1.0
        
        # æ”¶é›†å•é¡Œ
        all_issues = []
        critical_issues = []
        redundant_items = []
        
        for result in self.analysis_results:
            if result.issues:
                all_issues.extend(result.issues)
                if result.required and not result.implemented:
                    critical_issues.extend(result.issues)
            
            if result.name == "Redundant_Code":
                redundant_items = result.details.get("unused_imports", []) + \
                                result.details.get("potentially_unused_methods", []) + \
                                result.details.get("unused_variables", [])
        
        # ç”Ÿæˆä¿®å¾©å»ºè­°
        recommendations = []
        for result in self.analysis_results:
            if result.required and not result.implemented:
                recommendations.append(f"âŒ ä¿®å¾© {result.name}: {', '.join(result.issues)}")
            elif result.issues and result.required:
                recommendations.append(f"âš ï¸ æ”¹é€² {result.name}: {', '.join(result.issues)}")
        
        if redundant_items:
            recommendations.append(f"ğŸ§¹ æ¸…ç†å†—é¤˜ä»£ç¢¼: {len(redundant_items)} é …")
        
        # è©•ä¼°ç­‰ç´š
        if match_rate >= 0.95:
            compliance_grade = "A+ (å„ªç§€)"
        elif match_rate >= 0.9:
            compliance_grade = "A (è‰¯å¥½)"
        elif match_rate >= 0.8:
            compliance_grade = "B (åŠæ ¼)"
        elif match_rate >= 0.6:
            compliance_grade = "C (éœ€æ”¹é€²)"
        else:
            compliance_grade = "D (ä¸åŠæ ¼)"
        
        return {
            "ç¸½é«”è©•ä¼°": {
                "JSONè¦ç¯„åŒ¹é…åº¦": f"{match_rate:.1%}",
                "åˆè¦ç­‰ç´š": compliance_grade,
                "å¿…è¦çµ„ä»¶ç¸½æ•¸": total_required,
                "å®Œå…¨å¯¦ç¾": fully_implemented,
                "éƒ¨åˆ†å¯¦ç¾": partially_implemented,
                "ç¼ºå¤±çµ„ä»¶": missing_components
            },
            "è©³ç´°åˆ†æ": {
                result.name: {
                    "å¿…è¦": result.required,
                    "å·²å¯¦ç¾": result.implemented,
                    "è©³æƒ…": result.details,
                    "å•é¡Œ": result.issues
                }
                for result in self.analysis_results
            },
            "é—œéµå•é¡Œ": critical_issues,
            "æ‰€æœ‰å•é¡Œ": all_issues,
            "å†—é¤˜ä»£ç¢¼": redundant_items,
            "ä¿®å¾©å»ºè­°": recommendations,
            "çµè«–": self._generate_conclusion(match_rate, critical_issues, redundant_items)
        }
    
    def _generate_conclusion(self, match_rate: float, critical_issues: List[str], redundant_items: List[str]) -> str:
        """ç”Ÿæˆçµè«–"""
        if match_rate >= 0.95 and len(critical_issues) == 0:
            return "âœ… ä»£ç¢¼èˆ‡ JSON è¦ç¯„é«˜åº¦åŒ¹é…ï¼Œå¯æŠ•å…¥ç”Ÿç”¢ä½¿ç”¨"
        elif match_rate >= 0.9:
            return "ğŸŸ¨ ä»£ç¢¼åŸºæœ¬ç¬¦åˆ JSON è¦ç¯„ï¼Œå»ºè­°ä¿®å¾©å°‘é‡å•é¡Œå¾ŒæŠ•å…¥ä½¿ç”¨"
        elif match_rate >= 0.8:
            return "ğŸŸ§ ä»£ç¢¼éƒ¨åˆ†ç¬¦åˆ JSON è¦ç¯„ï¼Œéœ€è¦é‡è¦æ”¹é€²"
        else:
            return "ğŸŸ¥ ä»£ç¢¼èˆ‡ JSON è¦ç¯„å·®è·è¼ƒå¤§ï¼Œéœ€è¦å¤§å¹…ä¿®æ”¹"

def main():
    """ä¸»å‡½æ•¸"""
    json_spec_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/unified_signal_pool/unified_signal_candidate_pool_v3_dependency.json"
    code_file_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/unified_signal_pool/unified_signal_candidate_pool.py"
    
    analyzer = EnhancedPreciseAnalyzer(json_spec_path, code_file_path)
    report = analyzer.analyze_complete_compliance()
    
    print("\n" + "="*90)
    print("ğŸ¯ UNIFIED SIGNAL CANDIDATE POOL - å¢å¼·ç²¾ç¢ºæ·±åº¦åˆ†æå ±å‘Š v2.0")
    print("="*90)
    
    # ç¸½é«”è©•ä¼°
    overall = report["ç¸½é«”è©•ä¼°"]
    print(f"\nğŸ“Š ç¸½é«”è©•ä¼°:")
    print(f"   JSONè¦ç¯„åŒ¹é…åº¦: {overall['JSONè¦ç¯„åŒ¹é…åº¦']}")
    print(f"   åˆè¦ç­‰ç´š: {overall['åˆè¦ç­‰ç´š']}")
    print(f"   å®Œå…¨å¯¦ç¾: {overall['å®Œå…¨å¯¦ç¾']}/{overall['å¿…è¦çµ„ä»¶ç¸½æ•¸']}")
    print(f"   éƒ¨åˆ†å¯¦ç¾: {overall['éƒ¨åˆ†å¯¦ç¾']}")
    print(f"   ç¼ºå¤±çµ„ä»¶: {overall['ç¼ºå¤±çµ„ä»¶']}")
    
    # é—œéµå•é¡Œ
    if report["é—œéµå•é¡Œ"]:
        print(f"\nğŸš¨ é—œéµå•é¡Œ ({len(report['é—œéµå•é¡Œ'])} é …):")
        for i, issue in enumerate(report["é—œéµå•é¡Œ"][:10], 1):
            print(f"   {i}. {issue}")
    else:
        print(f"\nâœ… æœªç™¼ç¾é—œéµå•é¡Œ")
    
    # å†—é¤˜ä»£ç¢¼
    if report["å†—é¤˜ä»£ç¢¼"]:
        print(f"\nğŸ§¹ å†—é¤˜ä»£ç¢¼ ({len(report['å†—é¤˜ä»£ç¢¼'])} é …):")
        for i, item in enumerate(report["å†—é¤˜ä»£ç¢¼"][:5], 1):
            print(f"   {i}. {item}")
    else:
        print(f"\nâœ… æœªç™¼ç¾å†—é¤˜ä»£ç¢¼")
    
    # ä¿®å¾©å»ºè­°
    if report["ä¿®å¾©å»ºè­°"]:
        print(f"\nğŸ› ï¸ ä¿®å¾©å»ºè­° ({len(report['ä¿®å¾©å»ºè­°'])} é …):")
        for i, rec in enumerate(report["ä¿®å¾©å»ºè­°"][:8], 1):
            print(f"   {i}. {rec}")
    else:
        print(f"\nâœ… ç„¡éœ€ä¿®å¾©")
    
    # è©³ç´°çµ„ä»¶åˆ†æï¼ˆç°¡åŒ–é¡¯ç¤ºï¼‰
    print(f"\nğŸ“‹ çµ„ä»¶å¯¦ç¾ç‹€æ…‹:")
    for name, details in report["è©³ç´°åˆ†æ"].items():
        status = "âœ…" if details["å·²å¯¦ç¾"] else ("âš ï¸" if details["å•é¡Œ"] else "âŒ")
        required_mark = "ğŸ”´" if details["å¿…è¦"] else "ğŸ”µ"
        print(f"   {status} {required_mark} {name}")
        
        if details["å•é¡Œ"] and len(details["å•é¡Œ"]) <= 2:
            for issue in details["å•é¡Œ"]:
                print(f"      â””â”€ {issue}")
    
    # çµè«–
    print(f"\nğŸ¯ çµè«–:")
    print(f"   {report['çµè«–']}")
    
    print("\n" + "="*90)
    
    return report

if __name__ == "__main__":
    main()
