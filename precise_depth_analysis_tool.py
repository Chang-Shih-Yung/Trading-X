"""
ğŸ¯ ç²¾ç¢ºæ·±åº¦åˆ†æå·¥å…· - unified_signal_candidate_pool.py vs JSON è¦ç¯„
ğŸ¯ ç¢ºä¿ 100% å®Œæ•´åŒ¹é…ï¼Œç„¡é‚è¼¯æ–·é»ï¼Œç„¡å†—é¤˜ä»£ç¢¼
"""

import json
import ast
import re
import logging
from typing import Dict, List, Set, Any, Tuple
from pathlib import Path
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""
    component_name: str
    json_spec: Dict[str, Any]
    code_implementation: Dict[str, Any]
    match_status: str  # "COMPLETE" | "PARTIAL" | "MISSING" | "REDUNDANT"
    missing_elements: List[str]
    redundant_elements: List[str]
    logic_gaps: List[str]
    data_flow_issues: List[str]

class PreciseDepthAnalyzer:
    """ç²¾ç¢ºæ·±åº¦åˆ†æå™¨"""
    
    def __init__(self, json_spec_path: str, code_file_path: str):
        self.json_spec_path = json_spec_path
        self.code_file_path = code_file_path
        
        # è¼‰å…¥ JSON è¦ç¯„
        with open(json_spec_path, 'r', encoding='utf-8') as f:
            self.json_spec = json.load(f)
        
        # è®€å–ä»£ç¢¼
        with open(code_file_path, 'r', encoding='utf-8') as f:
            self.code_content = f.read()
        
        # è§£æ AST
        self.ast_tree = ast.parse(self.code_content)
        
        # åˆ†æçµæœ
        self.analysis_results: List[AnalysisResult] = []
        
    def analyze_complete_compliance(self) -> Dict[str, Any]:
        """å®Œæ•´åˆè¦æ€§åˆ†æ"""
        logger.info("ğŸ” é–‹å§‹ç²¾ç¢ºæ·±åº¦åˆ†æ...")
        
        # 1. æ ¸å¿ƒçµ„ä»¶åˆ†æ
        self._analyze_core_components()
        
        # 2. AI å­¸ç¿’å¼•æ“åˆ†æ
        self._analyze_ai_learning_engine()
        
        # 3. Phase1 æµç¨‹æ•´åˆåˆ†æ
        self._analyze_phase1_integration()
        
        # 4. æ•¸æ“šæµåˆ†æ
        self._analyze_data_flows()
        
        # 5. æ€§èƒ½ç›®æ¨™åˆ†æ
        self._analyze_performance_targets()
        
        # 6. å†—é¤˜ä»£ç¢¼æª¢æ¸¬
        self._detect_redundant_code()
        
        # 7. é‚è¼¯æ–·é»æª¢æ¸¬
        self._detect_logic_gaps()
        
        # 8. ç”Ÿæˆæœ€çµ‚å ±å‘Š
        return self._generate_final_report()
    
    def _analyze_core_components(self):
        """åˆ†ææ ¸å¿ƒçµ„ä»¶"""
        logger.info("ğŸ“Š åˆ†ææ ¸å¿ƒçµ„ä»¶...")
        
        # æª¢æŸ¥ä¸»è¦é¡åˆ¥
        required_classes = [
            "UnifiedSignalCandidatePoolV3",
            "StandardizedSignal", 
            "SevenDimensionalScore",
            "AILearningMetrics",
            "MarketRegimeState",
            "SignalQualityValidator",
            "AIAdaptiveLearningEngine",
            "SevenDimensionalScorer"
        ]
        
        code_classes = self._extract_classes()
        
        for class_name in required_classes:
            if class_name in code_classes:
                # æª¢æŸ¥é¡åˆ¥æ–¹æ³•å®Œæ•´æ€§
                self._analyze_class_methods(class_name, code_classes[class_name])
            else:
                self.analysis_results.append(AnalysisResult(
                    component_name=f"Class_{class_name}",
                    json_spec={"required": True},
                    code_implementation={"exists": False},
                    match_status="MISSING",
                    missing_elements=[class_name],
                    redundant_elements=[],
                    logic_gaps=[f"ç¼ºå°‘æ ¸å¿ƒé¡åˆ¥: {class_name}"],
                    data_flow_issues=[]
                ))
    
    def _analyze_ai_learning_engine(self):
        """åˆ†æ AI å­¸ç¿’å¼•æ“"""
        logger.info("ğŸ§  åˆ†æ AI å­¸ç¿’å¼•æ“...")
        
        # JSON è¦ç¯„è¦æ±‚
        ai_spec = self.json_spec["UNIFIED_SIGNAL_CANDIDATE_POOL_V3_DEPENDENCY"]["ğŸ§  ai_adaptive_learning_engine"]
        
        # æª¢æŸ¥å­¸ç¿’çµ„ä»¶
        required_ai_components = {
            "historical_decision_learning": [
                "learning_data", "learning_metrics", "weight_adjustment"
            ],
            "predictive_filtering": [
                "ml_model", "feature_engineering", "prediction_target", "filtering_thresholds"
            ],
            "real_time_adaptation": [
                "adaptation_trigger", "fast_learning", "emergency_adjustment", "stability_guarantee"
            ]
        }
        
        # æª¢æŸ¥ä»£ç¢¼å¯¦ç¾
        ai_methods = self._find_ai_methods()
        
        missing_components = []
        for component, features in required_ai_components.items():
            if component not in ai_methods:
                missing_components.extend([f"{component}.{feature}" for feature in features])
        
        if missing_components:
            self.analysis_results.append(AnalysisResult(
                component_name="AI_Learning_Engine",
                json_spec=ai_spec,
                code_implementation=ai_methods,
                match_status="PARTIAL",
                missing_elements=missing_components,
                redundant_elements=[],
                logic_gaps=[f"AI å­¸ç¿’å¼•æ“åŠŸèƒ½ä¸å®Œæ•´: {missing_components}"],
                data_flow_issues=[]
            ))
    
    def _analyze_phase1_integration(self):
        """åˆ†æ Phase1 æ•´åˆ"""
        logger.info("ğŸ”„ åˆ†æ Phase1 æ•´åˆ...")
        
        # JSON è¦ç¯„ Phase1 æµç¨‹
        phase1_spec = self.json_spec["UNIFIED_SIGNAL_CANDIDATE_POOL_V3_DEPENDENCY"]["ğŸ”„ phase1_complete_flow_integration"]
        
        # æª¢æŸ¥ Layer å¯¦ç¾
        required_layers = {
            "Layer_0_Complete_Phase1_Sync": ["unified_timestamp_sync", "data_flow_integrity", "extreme_market_fast_track"],
            "Layer_1_Multi_Source_Fusion": ["intelligent_signal_collection", "seven_dimensional_comprehensive_scoring"],
            "Layer_2_EPL_Preprocessor": ["epl_oriented_filtering", "epl_input_formatting", "emergency_signal_priority_channel"]
        }
        
        # æª¢æŸ¥ä»£ç¢¼ä¸­çš„å±¤å¯¦ç¾
        layer_methods = self._find_layer_methods()
        
        missing_layers = []
        for layer, components in required_layers.items():
            layer_method = f"_layer_{layer.split('_')[1].lower()}"
            if layer_method not in layer_methods:
                missing_layers.append(layer)
            else:
                # æª¢æŸ¥çµ„ä»¶å®Œæ•´æ€§
                for component in components:
                    if not self._check_component_in_method(layer_method, component):
                        missing_layers.append(f"{layer}.{component}")
        
        if missing_layers:
            self.analysis_results.append(AnalysisResult(
                component_name="Phase1_Integration",
                json_spec=phase1_spec,
                code_implementation=layer_methods,
                match_status="PARTIAL",
                missing_elements=missing_layers,
                redundant_elements=[],
                logic_gaps=[f"Phase1 æ•´åˆä¸å®Œæ•´: {missing_layers}"],
                data_flow_issues=[]
            ))
    
    def _analyze_data_flows(self):
        """åˆ†ææ•¸æ“šæµ"""
        logger.info("ğŸ“ˆ åˆ†ææ•¸æ“šæµ...")
        
        # JSON è¦ç¯„æ•¸æ“šæµ
        input_sources = self.json_spec["UNIFIED_SIGNAL_CANDIDATE_POOL_V3_DEPENDENCY"]["ğŸŒ complete_input_source_integration"]
        
        required_inputs = {
            "phase1a_input": ["PRICE_BREAKOUT", "VOLUME_SURGE", "MOMENTUM_SHIFT", "EXTREME_EVENT"],
            "indicator_graph_input": ["RSI_signals", "MACD_signals", "BB_signals", "Volume_signals"],
            "phase1b_input": ["VOLATILITY_BREAKOUT", "REGIME_CHANGE", "MEAN_REVERSION"],
            "phase1c_input": ["LIQUIDITY_SHOCK", "INSTITUTIONAL_FLOW", "SENTIMENT_DIVERGENCE"]
        }
        
        # æª¢æŸ¥ä»£ç¢¼ä¸­çš„æ•¸æ“šæµè™•ç†
        data_flow_methods = self._find_data_flow_methods()
        
        missing_data_flows = []
        for source, signal_types in required_inputs.items():
            method_name = f"_collect_{source.replace('_input', '')}_signals"
            if method_name not in data_flow_methods:
                missing_data_flows.append(source)
            else:
                # æª¢æŸ¥ä¿¡è™Ÿé¡å‹è™•ç†
                for signal_type in signal_types:
                    if not self._check_signal_type_handling(method_name, signal_type):
                        missing_data_flows.append(f"{source}.{signal_type}")
        
        if missing_data_flows:
            self.analysis_results.append(AnalysisResult(
                component_name="Data_Flows",
                json_spec=input_sources,
                code_implementation=data_flow_methods,
                match_status="PARTIAL",
                missing_elements=missing_data_flows,
                redundant_elements=[],
                logic_gaps=[],
                data_flow_issues=[f"æ•¸æ“šæµè™•ç†ä¸å®Œæ•´: {missing_data_flows}"]
            ))
    
    def _analyze_performance_targets(self):
        """åˆ†ææ€§èƒ½ç›®æ¨™"""
        logger.info("âš¡ åˆ†ææ€§èƒ½ç›®æ¨™...")
        
        # JSON è¦ç¯„æ€§èƒ½ç›®æ¨™
        perf_spec = self.json_spec["UNIFIED_SIGNAL_CANDIDATE_POOL_V3_DEPENDENCY"]["ğŸ¯ v3_0_performance_targets"]
        
        required_targets = {
            "layer_0_phase1_sync": "3ms",
            "layer_1_multi_fusion": "12ms", 
            "layer_2_epl_preprocessor": "8ms",
            "layer_ai_learning": "5ms",
            "total_processing_time": "28ms"
        }
        
        # æª¢æŸ¥ä»£ç¢¼ä¸­çš„æ€§èƒ½ç›£æ§
        perf_monitoring = self._find_performance_monitoring()
        
        missing_monitoring = []
        for target, time_limit in required_targets.items():
            if target not in perf_monitoring:
                missing_monitoring.append(f"{target} ({time_limit})")
        
        if missing_monitoring:
            self.analysis_results.append(AnalysisResult(
                component_name="Performance_Targets",
                json_spec=perf_spec,
                code_implementation=perf_monitoring,
                match_status="PARTIAL",
                missing_elements=missing_monitoring,
                redundant_elements=[],
                logic_gaps=[f"æ€§èƒ½ç›£æ§ä¸å®Œæ•´: {missing_monitoring}"],
                data_flow_issues=[]
            ))
    
    def _detect_redundant_code(self):
        """æª¢æ¸¬å†—é¤˜ä»£ç¢¼"""
        logger.info("ğŸ§¹ æª¢æ¸¬å†—é¤˜ä»£ç¢¼...")
        
        # æª¢æŸ¥æœªä½¿ç”¨çš„å°å…¥
        unused_imports = self._find_unused_imports()
        
        # æª¢æŸ¥æœªä½¿ç”¨çš„æ–¹æ³•
        unused_methods = self._find_unused_methods()
        
        # æª¢æŸ¥æœªä½¿ç”¨çš„è®Šæ•¸
        unused_variables = self._find_unused_variables()
        
        redundant_elements = unused_imports + unused_methods + unused_variables
        
        if redundant_elements:
            self.analysis_results.append(AnalysisResult(
                component_name="Redundant_Code",
                json_spec={"required": "clean_code"},
                code_implementation={"redundant_items": redundant_elements},
                match_status="REDUNDANT",
                missing_elements=[],
                redundant_elements=redundant_elements,
                logic_gaps=[],
                data_flow_issues=[]
            ))
    
    def _detect_logic_gaps(self):
        """æª¢æ¸¬é‚è¼¯æ–·é»"""
        logger.info("ğŸ” æª¢æ¸¬é‚è¼¯æ–·é»...")
        
        # æª¢æŸ¥æ–¹æ³•èª¿ç”¨éˆ
        call_chains = self._analyze_method_call_chains()
        
        # æª¢æŸ¥æ•¸æ“šå‚³é
        data_passing = self._analyze_data_passing()
        
        # æª¢æŸ¥éŒ¯èª¤è™•ç†
        error_handling = self._analyze_error_handling()
        
        logic_gaps = []
        
        # æª¢æŸ¥èª¿ç”¨éˆæ–·é»
        for chain in call_chains:
            if chain["broken"]:
                logic_gaps.append(f"èª¿ç”¨éˆæ–·é»: {chain['chain']}")
        
        # æª¢æŸ¥æ•¸æ“šå‚³éæ–·é»
        for data in data_passing:
            if data["missing"]:
                logic_gaps.append(f"æ•¸æ“šå‚³éæ–·é»: {data['flow']}")
        
        if logic_gaps:
            self.analysis_results.append(AnalysisResult(
                component_name="Logic_Gaps",
                json_spec={"required": "complete_logic_flow"},
                code_implementation={"gaps": logic_gaps},
                match_status="PARTIAL",
                missing_elements=[],
                redundant_elements=[],
                logic_gaps=logic_gaps,
                data_flow_issues=[]
            ))
    
    def _extract_classes(self) -> Dict[str, Dict]:
        """æå–é¡åˆ¥å®šç¾©"""
        classes = {}
        for node in ast.walk(self.ast_tree):
            if isinstance(node, ast.ClassDef):
                methods = []
                for item in node.body:
                    if isinstance(item, ast.FunctionDef):
                        methods.append(item.name)
                classes[node.name] = {
                    "methods": methods,
                    "line": node.lineno
                }
        return classes
    
    def _analyze_class_methods(self, class_name: str, class_info: Dict):
        """åˆ†æé¡åˆ¥æ–¹æ³•å®Œæ•´æ€§"""
        # æ ¹æ“š JSON è¦ç¯„æª¢æŸ¥å¿…è¦æ–¹æ³•
        if class_name == "UnifiedSignalCandidatePoolV3":
            required_methods = [
                "generate_signal_candidates_v3",
                "_layer_0_complete_phase1_sync",
                "_layer_1_enhanced_multi_source_fusion", 
                "_layer_2_epl_preprocessing_optimization",
                "_layer_ai_adaptive_learning"
            ]
            
            missing_methods = [m for m in required_methods if m not in class_info["methods"]]
            if missing_methods:
                self.analysis_results.append(AnalysisResult(
                    component_name=f"Class_{class_name}_Methods",
                    json_spec={"required_methods": required_methods},
                    code_implementation={"methods": class_info["methods"]},
                    match_status="PARTIAL",
                    missing_elements=missing_methods,
                    redundant_elements=[],
                    logic_gaps=[f"ç¼ºå°‘å¿…è¦æ–¹æ³•: {missing_methods}"],
                    data_flow_issues=[]
                ))
    
    def _find_ai_methods(self) -> Dict[str, bool]:
        """æŸ¥æ‰¾ AI ç›¸é—œæ–¹æ³•"""
        ai_methods = {}
        pattern = r"(learn_from_epl_feedback|predict_epl_pass_probability|calculate_signal_contribution|adjust_source_weights)"
        matches = re.findall(pattern, self.code_content)
        for match in matches:
            ai_methods[match] = True
        return ai_methods
    
    def _find_layer_methods(self) -> Dict[str, bool]:
        """æŸ¥æ‰¾å±¤ç´šæ–¹æ³•"""
        layer_methods = {}
        pattern = r"(_layer_\d+_\w+|_layer_ai_\w+)"
        matches = re.findall(pattern, self.code_content)
        for match in matches:
            layer_methods[match] = True
        return layer_methods
    
    def _find_data_flow_methods(self) -> Dict[str, bool]:
        """æŸ¥æ‰¾æ•¸æ“šæµæ–¹æ³•"""
        data_methods = {}
        pattern = r"(_collect_\w+_signals)"
        matches = re.findall(pattern, self.code_content)
        for match in matches:
            data_methods[match] = True
        return data_methods
    
    def _find_performance_monitoring(self) -> Dict[str, bool]:
        """æŸ¥æ‰¾æ€§èƒ½ç›£æ§"""
        perf_monitoring = {}
        patterns = [
            r"layer_\d+_time",
            r"total_time",
            r"elapsed.*time",
            r"performance_status"
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, self.code_content)
            for match in matches:
                perf_monitoring[match] = True
        return perf_monitoring
    
    def _check_component_in_method(self, method_name: str, component: str) -> bool:
        """æª¢æŸ¥æ–¹æ³•ä¸­æ˜¯å¦åŒ…å«çµ„ä»¶"""
        # ç°¡åŒ–å¯¦ç¾ï¼Œå¯¦éš›éœ€è¦æ›´è¤‡é›œçš„ AST åˆ†æ
        method_pattern = rf"def {method_name}.*?(?=def|\Z)"
        method_match = re.search(method_pattern, self.code_content, re.DOTALL)
        if method_match:
            method_content = method_match.group(0)
            return component.lower() in method_content.lower()
        return False
    
    def _check_signal_type_handling(self, method_name: str, signal_type: str) -> bool:
        """æª¢æŸ¥ä¿¡è™Ÿé¡å‹è™•ç†"""
        method_pattern = rf"def {method_name}.*?(?=def|\Z)"
        method_match = re.search(method_pattern, self.code_content, re.DOTALL)
        if method_match:
            method_content = method_match.group(0)
            return signal_type in method_content
        return False
    
    def _find_unused_imports(self) -> List[str]:
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„å°å…¥"""
        # ç°¡åŒ–å¯¦ç¾
        import_pattern = r"^(from .* import .*|import .*)"
        imports = re.findall(import_pattern, self.code_content, re.MULTILINE)
        
        unused = []
        for imp in imports:
            # æª¢æŸ¥æ˜¯å¦åœ¨ä»£ç¢¼ä¸­ä½¿ç”¨
            if "warnings" in imp and "warnings.filterwarnings" not in self.code_content:
                unused.append(imp)
        return unused
    
    def _find_unused_methods(self) -> List[str]:
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„æ–¹æ³•"""
        # æå–æ‰€æœ‰æ–¹æ³•å®šç¾©
        method_pattern = r"def (\w+)\("
        defined_methods = re.findall(method_pattern, self.code_content)
        
        unused = []
        for method in defined_methods:
            # æª¢æŸ¥æ˜¯å¦è¢«èª¿ç”¨ï¼ˆæ’é™¤é­”è¡“æ–¹æ³•å’Œä¸»è¦å…¥å£æ–¹æ³•ï¼‰
            if (not method.startswith("__") and 
                method not in ["generate_signal_candidates_v3", "learn_from_epl_feedback", "get_performance_report"] and
                self.code_content.count(f"{method}(") <= 1):  # åªå‡ºç¾åœ¨å®šç¾©ä¸­
                unused.append(method)
        return unused
    
    def _find_unused_variables(self) -> List[str]:
        """æŸ¥æ‰¾æœªä½¿ç”¨çš„è®Šæ•¸"""
        # ç°¡åŒ–å¯¦ç¾ï¼Œå¯¦éš›éœ€è¦æ›´è¤‡é›œçš„åˆ†æ
        unused = []
        
        # æª¢æŸ¥é¡ç´šåˆ¥è®Šæ•¸
        class_var_pattern = r"self\.(\w+) = "
        class_vars = re.findall(class_var_pattern, self.code_content)
        
        for var in class_vars:
            if self.code_content.count(f"self.{var}") <= 1:  # åªåœ¨è³¦å€¼æ™‚å‡ºç¾
                unused.append(f"self.{var}")
        
        return unused
    
    def _analyze_method_call_chains(self) -> List[Dict[str, Any]]:
        """åˆ†ææ–¹æ³•èª¿ç”¨éˆ"""
        # ç°¡åŒ–å¯¦ç¾
        chains = []
        
        # æª¢æŸ¥ä¸»è¦æµç¨‹éˆ
        main_chain = [
            "_layer_0_complete_phase1_sync",
            "_layer_1_enhanced_multi_source_fusion",
            "_layer_2_epl_preprocessing_optimization", 
            "_layer_ai_adaptive_learning"
        ]
        
        broken = False
        for method in main_chain:
            if method not in self.code_content:
                broken = True
                break
        
        chains.append({
            "chain": " -> ".join(main_chain),
            "broken": broken
        })
        
        return chains
    
    def _analyze_data_passing(self) -> List[Dict[str, Any]]:
        """åˆ†ææ•¸æ“šå‚³é"""
        # ç°¡åŒ–å¯¦ç¾
        data_flows = []
        
        # æª¢æŸ¥æ•¸æ“šå‚³éæµ
        expected_flows = [
            "raw_signals -> epl_optimized_signals",
            "epl_optimized_signals -> final_signals",
            "signals -> StandardizedSignal"
        ]
        
        for flow in expected_flows:
            variables = flow.split(" -> ")
            missing = False
            for var in variables:
                if var not in self.code_content:
                    missing = True
                    break
                    
            data_flows.append({
                "flow": flow,
                "missing": missing
            })
        
        return data_flows
    
    def _analyze_error_handling(self) -> Dict[str, Any]:
        """åˆ†æéŒ¯èª¤è™•ç†"""
        # æª¢æŸ¥ try-except è¦†è“‹ç‡
        try_count = self.code_content.count("try:")
        except_count = self.code_content.count("except")
        
        return {
            "try_blocks": try_count,
            "except_blocks": except_count,
            "adequate_coverage": try_count >= 5  # ä¸»è¦æ–¹æ³•éƒ½æ‡‰è©²æœ‰éŒ¯èª¤è™•ç†
        }
    
    def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚å ±å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæœ€çµ‚åˆ†æå ±å‘Š...")
        
        total_components = len(self.analysis_results)
        complete_matches = len([r for r in self.analysis_results if r.match_status == "COMPLETE"])
        partial_matches = len([r for r in self.analysis_results if r.match_status == "PARTIAL"])
        missing_components = len([r for r in self.analysis_results if r.match_status == "MISSING"])
        redundant_components = len([r for r in self.analysis_results if r.match_status == "REDUNDANT"])
        
        # è¨ˆç®—ç¸½é«”åŒ¹é…åº¦
        overall_match_rate = complete_matches / total_components if total_components > 0 else 0
        
        # æ”¶é›†æ‰€æœ‰å•é¡Œ
        all_missing = []
        all_redundant = []
        all_logic_gaps = []
        all_data_issues = []
        
        for result in self.analysis_results:
            all_missing.extend(result.missing_elements)
            all_redundant.extend(result.redundant_elements)
            all_logic_gaps.extend(result.logic_gaps)
            all_data_issues.extend(result.data_flow_issues)
        
        return {
            "ç¸½é«”åŒ¹é…åº¦": f"{overall_match_rate:.1%}",
            "çµ„ä»¶åˆ†æ": {
                "ç¸½çµ„ä»¶æ•¸": total_components,
                "å®Œå…¨åŒ¹é…": complete_matches,
                "éƒ¨åˆ†åŒ¹é…": partial_matches,
                "ç¼ºå¤±çµ„ä»¶": missing_components,
                "å†—é¤˜çµ„ä»¶": redundant_components
            },
            "è©³ç´°å•é¡Œ": {
                "ç¼ºå¤±å…ƒç´ ": all_missing,
                "å†—é¤˜å…ƒç´ ": all_redundant,
                "é‚è¼¯æ–·é»": all_logic_gaps,
                "æ•¸æ“šæµå•é¡Œ": all_data_issues
            },
            "åˆ†æçµæœ": [
                {
                    "çµ„ä»¶": result.component_name,
                    "ç‹€æ…‹": result.match_status,
                    "ç¼ºå¤±": result.missing_elements,
                    "å†—é¤˜": result.redundant_elements,
                    "é‚è¼¯å•é¡Œ": result.logic_gaps,
                    "æ•¸æ“šå•é¡Œ": result.data_flow_issues
                }
                for result in self.analysis_results
            ],
            "å»ºè­°å‹•ä½œ": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¿®å¾©å»ºè­°"""
        recommendations = []
        
        for result in self.analysis_results:
            if result.match_status == "MISSING":
                recommendations.append(f"âŒ å¯¦ç¾ç¼ºå¤±çµ„ä»¶: {result.component_name}")
            elif result.match_status == "PARTIAL":
                recommendations.append(f"âš ï¸ å®Œå–„éƒ¨åˆ†å¯¦ç¾: {result.component_name} - {result.missing_elements}")
            elif result.match_status == "REDUNDANT":
                recommendations.append(f"ğŸ§¹ æ¸…ç†å†—é¤˜ä»£ç¢¼: {result.redundant_elements}")
            
            if result.logic_gaps:
                recommendations.append(f"ğŸ”§ ä¿®å¾©é‚è¼¯æ–·é»: {result.logic_gaps}")
            
            if result.data_flow_issues:
                recommendations.append(f"ğŸ“Š ä¿®å¾©æ•¸æ“šæµ: {result.data_flow_issues}")
        
        return recommendations

def main():
    """ä¸»å‡½æ•¸"""
    json_spec_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/unified_signal_pool/unified_signal_candidate_pool_v3_dependency.json"
    code_file_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/unified_signal_pool/unified_signal_candidate_pool.py"
    
    analyzer = PreciseDepthAnalyzer(json_spec_path, code_file_path)
    report = analyzer.analyze_complete_compliance()
    
    print("\n" + "="*80)
    print("ğŸ¯ UNIFIED SIGNAL CANDIDATE POOL - ç²¾ç¢ºæ·±åº¦åˆ†æå ±å‘Š")
    print("="*80)
    
    print(f"\nğŸ“Š ç¸½é«”è©•ä¼°:")
    print(f"   åŒ¹é…åº¦: {report['ç¸½é«”åŒ¹é…åº¦']}")
    print(f"   å®Œå…¨åŒ¹é…: {report['çµ„ä»¶åˆ†æ']['å®Œå…¨åŒ¹é…']}/{report['çµ„ä»¶åˆ†æ']['ç¸½çµ„ä»¶æ•¸']}")
    print(f"   éƒ¨åˆ†åŒ¹é…: {report['çµ„ä»¶åˆ†æ']['éƒ¨åˆ†åŒ¹é…']}/{report['çµ„ä»¶åˆ†æ']['ç¸½çµ„ä»¶æ•¸']}")
    print(f"   ç¼ºå¤±çµ„ä»¶: {report['çµ„ä»¶åˆ†æ']['ç¼ºå¤±çµ„ä»¶']}")
    print(f"   å†—é¤˜çµ„ä»¶: {report['çµ„ä»¶åˆ†æ']['å†—é¤˜çµ„ä»¶']}")
    
    print(f"\nğŸš¨ é—œéµå•é¡Œ:")
    if report['è©³ç´°å•é¡Œ']['ç¼ºå¤±å…ƒç´ ']:
        print(f"   ç¼ºå¤±å…ƒç´ : {len(report['è©³ç´°å•é¡Œ']['ç¼ºå¤±å…ƒç´ '])} é …")
        for item in report['è©³ç´°å•é¡Œ']['ç¼ºå¤±å…ƒç´ '][:5]:  # é¡¯ç¤ºå‰5é …
            print(f"     - {item}")
    
    if report['è©³ç´°å•é¡Œ']['å†—é¤˜å…ƒç´ ']:
        print(f"   å†—é¤˜å…ƒç´ : {len(report['è©³ç´°å•é¡Œ']['å†—é¤˜å…ƒç´ '])} é …")
        for item in report['è©³ç´°å•é¡Œ']['å†—é¤˜å…ƒç´ '][:3]:  # é¡¯ç¤ºå‰3é …
            print(f"     - {item}")
    
    if report['è©³ç´°å•é¡Œ']['é‚è¼¯æ–·é»']:
        print(f"   é‚è¼¯æ–·é»: {len(report['è©³ç´°å•é¡Œ']['é‚è¼¯æ–·é»'])} é …")
        for item in report['è©³ç´°å•é¡Œ']['é‚è¼¯æ–·é»']:
            print(f"     - {item}")
    
    print(f"\nğŸ› ï¸ ä¿®å¾©å»ºè­°:")
    for i, recommendation in enumerate(report['å»ºè­°å‹•ä½œ'][:10], 1):  # é¡¯ç¤ºå‰10é …å»ºè­°
        print(f"   {i}. {recommendation}")
    
    print(f"\nğŸ“‹ è©³ç´°åˆ†æçµæœ:")
    for result in report['åˆ†æçµæœ']:
        status_emoji = {
            "COMPLETE": "âœ…",
            "PARTIAL": "âš ï¸", 
            "MISSING": "âŒ",
            "REDUNDANT": "ğŸ§¹"
        }
        print(f"   {status_emoji.get(result['ç‹€æ…‹'], 'â“')} {result['çµ„ä»¶']}: {result['ç‹€æ…‹']}")
        
        if result['ç¼ºå¤±']:
            print(f"      ç¼ºå¤±: {result['ç¼ºå¤±']}")
        if result['å†—é¤˜']:
            print(f"      å†—é¤˜: {result['å†—é¤˜']}")
        if result['é‚è¼¯å•é¡Œ']:
            print(f"      é‚è¼¯: {result['é‚è¼¯å•é¡Œ']}")
    
    print("\n" + "="*80)
    
    return report

if __name__ == "__main__":
    main()
