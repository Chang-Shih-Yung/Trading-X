"""
ğŸ”§ EPL Decision History Tracking Python å¯¦ç¾å„ªåŒ–
==============================================

åŸºæ–¼ JSON é…ç½®ç²¾ç¢ºåŒ¹é…çš„ Python å¯¦ç¾å„ªåŒ–
"""

import sys
import ast
import re
from pathlib import Path
from typing import Dict, Any, List, Set
from datetime import datetime

class EPLPythonOptimizer:
    """EPL Python å¯¦ç¾å„ªåŒ–å™¨"""
    
    def __init__(self):
        self.python_path = Path("/Users/henrychang/Desktop/Trading-X/X/backend/phase4_output_monitoring/3_epl_decision_history_tracking/epl_decision_history_tracking.py")
        self.config_path = Path("/Users/henrychang/Desktop/Trading-X/X/backend/phase4_output_monitoring/3_epl_decision_history_tracking/epl_decision_history_tracking_config.json")
        
    def analyze_current_implementation(self) -> Dict[str, Any]:
        """åˆ†æç•¶å‰å¯¦ç¾"""
        try:
            with open(self.python_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–é¡åˆ¥å’Œæ–¹æ³•
            tree = ast.parse(content)
            
            classes = []
            methods = []
            dataclasses = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    classes.append(node.name)
                    # æª¢æŸ¥æ˜¯å¦ç‚º dataclass
                    for decorator in node.decorator_list:
                        if (isinstance(decorator, ast.Name) and decorator.id == 'dataclass'):
                            dataclasses.append(node.name)
                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    methods.append(node.name)
            
            return {
                "classes": classes,
                "methods": methods,
                "dataclasses": dataclasses,
                "total_lines": len(content.split('\n')),
                "file_size": len(content)
            }
            
        except Exception as e:
            return {"error": f"åˆ†æå¯¦ç¾å¤±æ•—: {e}"}
    
    def identify_missing_components(self) -> Dict[str, List[str]]:
        """è­˜åˆ¥ç¼ºå¤±çš„çµ„ä»¶"""
        missing_components = {
            "dataclasses": [],
            "core_methods": [],
            "analytics_methods": [],
            "integration_methods": []
        }
        
        # æª¢æŸ¥ JSON é…ç½®è¦æ±‚çš„æ•¸æ“šçµæ§‹
        required_dataclasses = [
            "EPLDecisionRecord",      # âœ… å·²å­˜åœ¨
            "DecisionOutcome",        # âœ… å·²å­˜åœ¨  
            "MarketSnapshot",         # âŒ ç¼ºå¤±
            "PortfolioState",         # âŒ ç¼ºå¤±
            "ExecutionMetrics"        # âŒ ç¼ºå¤±
        ]
        
        # æª¢æŸ¥æ ¸å¿ƒæ–¹æ³•
        required_core_methods = [
            "record_epl_decision",           # âœ… å·²å­˜åœ¨
            "update_decision_outcome",       # âœ… å·²å­˜åœ¨
            "track_execution_lifecycle",    # âŒ ç¼ºå¤±
            "capture_market_context",       # âŒ ç¼ºå¤±
            "validate_data_integrity"       # âŒ ç¼ºå¤±
        ]
        
        # æª¢æŸ¥åˆ†ææ–¹æ³•
        required_analytics_methods = [
            "analyze_replacement_patterns",      # âŒ ç¼ºå¤±
            "analyze_strengthening_patterns",    # âŒ ç¼ºå¤±
            "analyze_new_position_patterns",     # âŒ ç¼ºå¤±
            "analyze_ignore_patterns",           # âŒ ç¼ºå¤±
            "generate_learning_insights"         # âŒ ç¼ºå¤±
        ]
        
        # æª¢æŸ¥æ•´åˆæ–¹æ³•
        required_integration_methods = [
            "integrate_phase1_signals",          # âŒ ç¼ºå¤±
            "integrate_phase2_evaluation",       # âŒ ç¼ºå¤±
            "integrate_phase3_execution",        # âŒ ç¼ºå¤±
            "export_phase4_analytics"            # âŒ ç¼ºå¤±
        ]
        
        # è®€å–ç•¶å‰å¯¦ç¾ä¸¦æª¢æŸ¥
        try:
            with open(self.python_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æª¢æŸ¥ dataclasses
            for dc in required_dataclasses:
                if f"class {dc}" not in content:
                    missing_components["dataclasses"].append(dc)
            
            # æª¢æŸ¥æ–¹æ³•
            for method in required_core_methods:
                if f"def {method}" not in content and f"async def {method}" not in content:
                    missing_components["core_methods"].append(method)
            
            for method in required_analytics_methods:
                if f"def {method}" not in content and f"async def {method}" not in content:
                    missing_components["analytics_methods"].append(method)
            
            for method in required_integration_methods:
                if f"def {method}" not in content and f"async def {method}" not in content:
                    missing_components["integration_methods"].append(method)
        
        except Exception as e:
            print(f"æª¢æŸ¥ç¼ºå¤±çµ„ä»¶æ™‚å‡ºéŒ¯: {e}")
        
        return missing_components
    
    def generate_missing_dataclasses(self) -> str:
        """ç”Ÿæˆç¼ºå¤±çš„æ•¸æ“šé¡"""
        return '''
@dataclass
class MarketSnapshot:
    """å¸‚å ´å¿«ç…§æ•¸æ“š"""
    timestamp: datetime
    symbol: str
    price: float
    volume: float
    volatility: float
    market_conditions: Dict[str, Any]
    technical_indicators: Dict[str, float]
    sentiment_score: Optional[float] = None

@dataclass  
class PortfolioState:
    """æŠ•è³‡çµ„åˆç‹€æ…‹"""
    timestamp: datetime
    total_value: float
    available_cash: float
    positions: Dict[str, Dict[str, Any]]
    risk_metrics: Dict[str, float]
    correlation_matrix: Optional[Dict[str, Dict[str, float]]] = None
    exposure_limits: Optional[Dict[str, float]] = None

@dataclass
class ExecutionMetrics:
    """åŸ·è¡ŒæŒ‡æ¨™"""
    decision_id: str
    execution_timestamp: datetime
    planned_price: float
    actual_price: float
    slippage: float
    execution_latency: float
    market_impact: float
    execution_quality_score: float
    fees_and_costs: Dict[str, float]
'''
    
    def generate_missing_core_methods(self) -> str:
        """ç”Ÿæˆç¼ºå¤±çš„æ ¸å¿ƒæ–¹æ³•"""
        return '''
    async def track_execution_lifecycle(self, decision_id: str, execution_data: Dict[str, Any]) -> bool:
        """è¿½è¹¤åŸ·è¡Œç”Ÿå‘½é€±æœŸ"""
        try:
            # å‰µå»ºåŸ·è¡ŒæŒ‡æ¨™
            execution_metrics = ExecutionMetrics(
                decision_id=decision_id,
                execution_timestamp=datetime.fromisoformat(execution_data.get('timestamp', datetime.now().isoformat())),
                planned_price=execution_data.get('planned_price', 0.0),
                actual_price=execution_data.get('actual_price', 0.0),
                slippage=execution_data.get('slippage', 0.0),
                execution_latency=execution_data.get('latency', 0.0),
                market_impact=execution_data.get('market_impact', 0.0),
                execution_quality_score=execution_data.get('quality_score', 0.0),
                fees_and_costs=execution_data.get('costs', {})
            )
            
            # æ›´æ–°æ±ºç­–è¨˜éŒ„
            decision_record = self._find_decision_by_id(decision_id)
            if decision_record:
                decision_record.execution_details['execution_metrics'] = asdict(execution_metrics)
                logger.info(f"æ›´æ–°åŸ·è¡Œç”Ÿå‘½é€±æœŸ: {decision_id}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"è¿½è¹¤åŸ·è¡Œç”Ÿå‘½é€±æœŸå¤±æ•—: {e}")
            return False
    
    async def capture_market_context(self, symbol: str) -> MarketSnapshot:
        """æ“·å–å¸‚å ´ä¸Šä¸‹æ–‡"""
        try:
            # æ¨¡æ“¬å¸‚å ´æ•¸æ“šæ“·å– (å¯¦éš›æ‡‰è©²å¾å¸‚å ´æ•¸æ“šæºç²å–)
            market_snapshot = MarketSnapshot(
                timestamp=datetime.now(),
                symbol=symbol,
                price=0.0,  # æ‡‰è©²å¾å¯¦éš›æ•¸æ“šæºç²å–
                volume=0.0,
                volatility=0.0,
                market_conditions={"trend": "neutral", "session": "active"},
                technical_indicators={"rsi": 50.0, "macd": 0.0},
                sentiment_score=0.5
            )
            
            logger.info(f"æ“·å–å¸‚å ´ä¸Šä¸‹æ–‡: {symbol}")
            return market_snapshot
            
        except Exception as e:
            logger.error(f"æ“·å–å¸‚å ´ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            # è¿”å›é»˜èªå¿«ç…§
            return MarketSnapshot(
                timestamp=datetime.now(),
                symbol=symbol,
                price=0.0, volume=0.0, volatility=0.0,
                market_conditions={}, technical_indicators={}
            )
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        """é©—è­‰æ•¸æ“šå®Œæ•´æ€§"""
        try:
            integrity_report = {
                "validation_timestamp": datetime.now().isoformat(),
                "total_decisions": len(self.decision_history),
                "total_outcomes": len(self.outcome_history),
                "integrity_checks": {}
            }
            
            # æª¢æŸ¥æ±ºç­–è¨˜éŒ„å®Œæ•´æ€§
            missing_outcomes = 0
            invalid_records = 0
            
            for record in self.decision_history:
                # æª¢æŸ¥å¿…è¦æ¬„ä½
                if not all([record.decision_id, record.symbol, record.timestamp]):
                    invalid_records += 1
                
                # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„çµæœ
                if record.decision_id not in self.outcome_history:
                    missing_outcomes += 1
            
            integrity_report["integrity_checks"] = {
                "invalid_records": invalid_records,
                "missing_outcomes": missing_outcomes,
                "data_consistency": "good" if invalid_records == 0 else "issues_found",
                "outcome_coverage": (len(self.outcome_history) / max(len(self.decision_history), 1)) * 100
            }
            
            logger.info("æ•¸æ“šå®Œæ•´æ€§é©—è­‰å®Œæˆ")
            return integrity_report
            
        except Exception as e:
            logger.error(f"æ•¸æ“šå®Œæ•´æ€§é©—è­‰å¤±æ•—: {e}")
            return {"error": str(e)}
'''
    
    def generate_missing_analytics_methods(self) -> str:
        """ç”Ÿæˆç¼ºå¤±çš„åˆ†ææ–¹æ³•"""
        return '''
    async def analyze_replacement_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ›¿æ›æ±ºç­–æ¨¡å¼"""
        try:
            replacement_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.REPLACE_POSITION
            ]
            
            if not replacement_decisions:
                return {"message": "æš«ç„¡æ›¿æ›æ±ºç­–æ•¸æ“š"}
            
            # åˆ†ææ›¿æ›é »ç‡
            total_decisions = len(self.decision_history)
            replacement_rate = len(replacement_decisions) / total_decisions
            
            # åˆ†ææ›¿æ›æˆåŠŸç‡
            successful_replacements = 0
            for record in replacement_decisions:
                if record.decision_id in self.outcome_history:
                    outcome = self.outcome_history[record.decision_id]
                    if outcome.success:
                        successful_replacements += 1
            
            replacement_success_rate = successful_replacements / len(replacement_decisions) if replacement_decisions else 0
            
            # åˆ†æä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ
            confidence_scores = [r.confidence_score for r in replacement_decisions]
            avg_confidence = statistics.mean(confidence_scores) if confidence_scores else 0
            
            return {
                "replacement_frequency": {
                    "total_replacements": len(replacement_decisions),
                    "replacement_rate": replacement_rate,
                    "average_confidence": avg_confidence
                },
                "replacement_effectiveness": {
                    "success_rate": replacement_success_rate,
                    "successful_count": successful_replacements,
                    "failed_count": len(replacement_decisions) - successful_replacements
                },
                "insights": self._generate_replacement_insights(replacement_decisions)
            }
            
        except Exception as e:
            logger.error(f"åˆ†ææ›¿æ›æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def analyze_strengthening_patterns(self) -> Dict[str, Any]:
        """åˆ†æå¼·åŒ–æ±ºç­–æ¨¡å¼"""
        try:
            strengthening_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.STRENGTHEN_POSITION
            ]
            
            if not strengthening_decisions:
                return {"message": "æš«ç„¡å¼·åŒ–æ±ºç­–æ•¸æ“š"}
            
            # åŸºæœ¬çµ±è¨ˆ
            total_decisions = len(self.decision_history)
            strengthening_rate = len(strengthening_decisions) / total_decisions
            
            # æˆåŠŸç‡åˆ†æ
            successful_strengthenings = sum(
                1 for record in strengthening_decisions
                if record.decision_id in self.outcome_history and 
                self.outcome_history[record.decision_id].success
            )
            
            success_rate = successful_strengthenings / len(strengthening_decisions) if strengthening_decisions else 0
            
            return {
                "strengthening_frequency": {
                    "total_strengthenings": len(strengthening_decisions),
                    "strengthening_rate": strengthening_rate,
                    "average_confidence": statistics.mean([r.confidence_score for r in strengthening_decisions])
                },
                "strengthening_effectiveness": {
                    "success_rate": success_rate,
                    "successful_count": successful_strengthenings
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†æå¼·åŒ–æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def analyze_new_position_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ–°å€‰ä½æ±ºç­–æ¨¡å¼"""
        try:
            new_position_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.CREATE_NEW_POSITION
            ]
            
            if not new_position_decisions:
                return {"message": "æš«ç„¡æ–°å€‰ä½æ±ºç­–æ•¸æ“š"}
            
            # åŸºæœ¬çµ±è¨ˆ
            creation_rate = len(new_position_decisions) / len(self.decision_history)
            
            # æˆåŠŸç‡åˆ†æ
            successful_creations = sum(
                1 for record in new_position_decisions
                if record.decision_id in self.outcome_history and
                self.outcome_history[record.decision_id].success
            )
            
            success_rate = successful_creations / len(new_position_decisions) if new_position_decisions else 0
            
            return {
                "creation_frequency": {
                    "total_new_positions": len(new_position_decisions),
                    "creation_rate": creation_rate,
                    "average_confidence": statistics.mean([r.confidence_score for r in new_position_decisions])
                },
                "creation_effectiveness": {
                    "success_rate": success_rate,
                    "successful_count": successful_creations
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†ææ–°å€‰ä½æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def analyze_ignore_patterns(self) -> Dict[str, Any]:
        """åˆ†æå¿½ç•¥æ±ºç­–æ¨¡å¼"""
        try:
            ignore_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.IGNORE_SIGNAL
            ]
            
            if not ignore_decisions:
                return {"message": "æš«ç„¡å¿½ç•¥æ±ºç­–æ•¸æ“š"}
            
            # å¿½ç•¥ç‡åˆ†æ
            ignore_rate = len(ignore_decisions) / len(self.decision_history)
            
            # å¿½ç•¥åŸå› åˆ†æ
            ignore_reasons = defaultdict(int)
            for record in ignore_decisions:
                reason = record.position_context.get('reason', 'unknown')
                ignore_reasons[reason] += 1
            
            return {
                "ignore_frequency": {
                    "total_ignores": len(ignore_decisions),
                    "ignore_rate": ignore_rate,
                    "average_confidence": statistics.mean([r.confidence_score for r in ignore_decisions])
                },
                "ignore_reasons": dict(ignore_reasons),
                "filtering_effectiveness": self._calculate_filtering_effectiveness(ignore_decisions)
            }
            
        except Exception as e:
            logger.error(f"åˆ†æå¿½ç•¥æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def generate_learning_insights(self) -> Dict[str, Any]:
        """ç”Ÿæˆå­¸ç¿’æ´å¯Ÿ"""
        try:
            if not self.decision_history:
                return {"message": "æš«ç„¡æ±ºç­–æ•¸æ“šç”Ÿæˆå­¸ç¿’æ´å¯Ÿ"}
            
            # æ¨¡å¼è­˜åˆ¥
            successful_patterns = await self._identify_successful_patterns()
            failure_patterns = await self._identify_failure_patterns()
            
            # é©æ‡‰æ€§å­¸ç¿’å»ºè­°
            adaptive_recommendations = self._generate_adaptive_recommendations()
            
            # å›é¥‹æ•´åˆåˆ†æ
            feedback_integration = self._analyze_feedback_integration()
            
            return {
                "pattern_recognition": {
                    "successful_patterns": successful_patterns,
                    "failure_patterns": failure_patterns
                },
                "adaptive_learning": {
                    "recommendations": adaptive_recommendations,
                    "threshold_adjustments": self._suggest_threshold_adjustments()
                },
                "feedback_integration": feedback_integration,
                "learning_summary": self._summarize_learning_insights()
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå­¸ç¿’æ´å¯Ÿå¤±æ•—: {e}")
            return {"error": str(e)}
'''
    
    def generate_missing_integration_methods(self) -> str:
        """ç”Ÿæˆç¼ºå¤±çš„æ•´åˆæ–¹æ³•"""
        return '''
    async def integrate_phase1_signals(self, signal_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆ Phase1 ä¿¡è™Ÿæ•¸æ“š"""
        try:
            # æå– Phase1 ä¿¡è™Ÿå€™é¸è³‡æ–™
            signal_candidate = signal_data.get('signal_candidate', {})
            
            integrated_data = {
                "original_signal_candidate": signal_candidate,
                "signal_quality_metrics": {
                    "technical_strength": signal_candidate.get('technical_strength', 0.5),
                    "market_timing": signal_candidate.get('market_timing', 0.5),
                    "source_reliability": signal_candidate.get('source_reliability', 0.5)
                },
                "market_context": await self.capture_market_context(signal_candidate.get('symbol', 'UNKNOWN'))
            }
            
            logger.info("Phase1 ä¿¡è™Ÿæ•¸æ“šæ•´åˆå®Œæˆ")
            return integrated_data
            
        except Exception as e:
            logger.error(f"æ•´åˆ Phase1 ä¿¡è™Ÿå¤±æ•—: {e}")
            return {}
    
    async def integrate_phase2_evaluation(self, evaluation_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆ Phase2 é è©•ä¼°çµæœ"""
        try:
            # æå– Phase2 é è©•ä¼°çµæœ
            pre_evaluation = evaluation_data.get('pre_evaluation_result', {})
            
            integrated_data = {
                "pre_evaluation_result": pre_evaluation,
                "embedded_scoring": pre_evaluation.get('embedded_scoring', {}),
                "correlation_analysis": pre_evaluation.get('correlation_analysis', {}),
                "portfolio_state": self._extract_portfolio_state(evaluation_data)
            }
            
            logger.info("Phase2 é è©•ä¼°æ•¸æ“šæ•´åˆå®Œæˆ")
            return integrated_data
            
        except Exception as e:
            logger.error(f"æ•´åˆ Phase2 è©•ä¼°å¤±æ•—: {e}")
            return {}
    
    async def integrate_phase3_execution(self, execution_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆ Phase3 åŸ·è¡Œæ•¸æ“š"""
        try:
            # æå–åŸ·è¡Œç›¸é—œæ•¸æ“š
            execution_result = execution_data.get('execution_result', {})
            
            integrated_data = {
                "execution_initiation": {
                    "execution_timestamp": execution_result.get('timestamp'),
                    "execution_latency": execution_result.get('latency'),
                    "market_conditions_at_execution": execution_result.get('market_conditions'),
                    "slippage_measurement": execution_result.get('slippage')
                },
                "execution_monitoring": {
                    "position_establishment": execution_result.get('position_changes'),
                    "risk_parameter_application": execution_result.get('risk_parameters'),
                    "portfolio_impact": execution_result.get('portfolio_impact'),
                    "correlation_effects": execution_result.get('correlation_impact')
                }
            }
            
            logger.info("Phase3 åŸ·è¡Œæ•¸æ“šæ•´åˆå®Œæˆ")
            return integrated_data
            
        except Exception as e:
            logger.error(f"æ•´åˆ Phase3 åŸ·è¡Œå¤±æ•—: {e}")
            return {}
    
    async def export_phase4_analytics(self) -> Dict[str, Any]:
        """åŒ¯å‡º Phase4 åˆ†æçµæœ"""
        try:
            # åŒ¯å‡ºæ‰€æœ‰åˆ†æçµæœä¾›å…¶ä»–ç³»çµ±ä½¿ç”¨
            analytics_export = {
                "export_timestamp": datetime.now().isoformat(),
                "decision_analytics": await self.get_comprehensive_decision_analysis(),
                "performance_metrics": await self.get_performance_metrics(),
                "pattern_insights": {
                    "replacement_patterns": await self.analyze_replacement_patterns(),
                    "strengthening_patterns": await self.analyze_strengthening_patterns(),
                    "new_position_patterns": await self.analyze_new_position_patterns(),
                    "ignore_patterns": await self.analyze_ignore_patterns()
                },
                "learning_insights": await self.generate_learning_insights(),
                "system_status": await self.get_system_status()
            }
            
            logger.info("Phase4 åˆ†æçµæœåŒ¯å‡ºå®Œæˆ")
            return analytics_export
            
        except Exception as e:
            logger.error(f"åŒ¯å‡º Phase4 åˆ†æå¤±æ•—: {e}")
            return {"error": str(e)}
'''
    
    def generate_helper_methods(self) -> str:
        """ç”Ÿæˆè¼”åŠ©æ–¹æ³•"""
        return '''
    def _generate_replacement_insights(self, replacement_decisions: List) -> List[str]:
        """ç”Ÿæˆæ›¿æ›æ´å¯Ÿ"""
        insights = []
        
        if len(replacement_decisions) > 10:
            avg_confidence = statistics.mean([r.confidence_score for r in replacement_decisions])
            if avg_confidence > 0.8:
                insights.append("æ›¿æ›æ±ºç­–æ™®éå…·æœ‰é«˜ä¿¡å¿ƒåˆ†æ•¸")
            elif avg_confidence < 0.6:
                insights.append("æ›¿æ›æ±ºç­–ä¿¡å¿ƒåˆ†æ•¸åä½ï¼Œå»ºè­°æª¢è¨æ¨™æº–")
        
        return insights if insights else ["éœ€è¦æ›´å¤šæ•¸æ“šç”Ÿæˆæ´å¯Ÿ"]
    
    def _extract_portfolio_state(self, evaluation_data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æŠ•è³‡çµ„åˆç‹€æ…‹"""
        return {
            "current_positions": evaluation_data.get('portfolio', {}).get('positions', {}),
            "risk_metrics": evaluation_data.get('portfolio', {}).get('risk_metrics', {}),
            "available_cash": evaluation_data.get('portfolio', {}).get('cash', 0.0),
            "total_value": evaluation_data.get('portfolio', {}).get('total_value', 0.0)
        }
    
    def _calculate_filtering_effectiveness(self, ignore_decisions: List) -> Dict[str, Any]:
        """è¨ˆç®—éæ¿¾æ•ˆæœ"""
        if not ignore_decisions:
            return {"effectiveness": "no_data"}
        
        # è¨ˆç®—ä½å“è³ªä¿¡è™Ÿéæ¿¾ç‡
        low_quality_ignores = len([d for d in ignore_decisions if d.confidence_score < 0.5])
        filtering_rate = low_quality_ignores / len(ignore_decisions)
        
        return {
            "low_quality_filtering_rate": filtering_rate,
            "total_filtered": len(ignore_decisions),
            "effectiveness_score": min(filtering_rate * 2, 1.0)  # æ­¸ä¸€åŒ–åˆ°0-1
        }
    
    async def _identify_successful_patterns(self) -> Dict[str, Any]:
        """è­˜åˆ¥æˆåŠŸæ¨¡å¼"""
        successful_decisions = [
            record for record in self.decision_history
            if record.decision_id in self.outcome_history and
            self.outcome_history[record.decision_id].success
        ]
        
        if not successful_decisions:
            return {"patterns": "insufficient_data"}
        
        # åˆ†ææˆåŠŸæ±ºç­–çš„å…±åŒç‰¹å¾µ
        avg_confidence = statistics.mean([d.confidence_score for d in successful_decisions])
        priority_distribution = defaultdict(int)
        
        for decision in successful_decisions:
            priority_distribution[decision.signal_priority.value] += 1
        
        return {
            "average_confidence": avg_confidence,
            "priority_distribution": dict(priority_distribution),
            "pattern_count": len(successful_decisions)
        }
    
    async def _identify_failure_patterns(self) -> Dict[str, Any]:
        """è­˜åˆ¥å¤±æ•—æ¨¡å¼"""
        failed_decisions = [
            record for record in self.decision_history
            if record.decision_id in self.outcome_history and
            not self.outcome_history[record.decision_id].success
        ]
        
        if not failed_decisions:
            return {"patterns": "insufficient_data"}
        
        # åˆ†æå¤±æ•—æ±ºç­–çš„å…±åŒç‰¹å¾µ
        avg_confidence = statistics.mean([d.confidence_score for d in failed_decisions])
        priority_distribution = defaultdict(int)
        
        for decision in failed_decisions:
            priority_distribution[decision.signal_priority.value] += 1
        
        return {
            "average_confidence": avg_confidence,
            "priority_distribution": dict(priority_distribution),
            "pattern_count": len(failed_decisions)
        }
    
    def _generate_adaptive_recommendations(self) -> List[str]:
        """ç”Ÿæˆé©æ‡‰æ€§å»ºè­°"""
        recommendations = []
        
        if self.success_rates:
            overall_success_rate = sum(
                sum(successes) / len(successes) for successes in self.success_rates.values() if successes
            ) / len(self.success_rates)
            
            if overall_success_rate < 0.7:
                recommendations.append("æ•´é«”æˆåŠŸç‡åä½ï¼Œå»ºè­°èª¿æ•´æ±ºç­–é–¾å€¼")
            elif overall_success_rate > 0.9:
                recommendations.append("æˆåŠŸç‡æ¥µé«˜ï¼Œå¯è€ƒæ…®é™ä½æ±ºç­–é–€æª»ä»¥å¢åŠ æ©Ÿæœƒ")
        
        return recommendations if recommendations else ["ç³»çµ±è¡¨ç¾ç©©å®šï¼Œç¹¼çºŒç›£æ§"]
    
    def _suggest_threshold_adjustments(self) -> Dict[str, float]:
        """å»ºè­°é–¾å€¼èª¿æ•´"""
        adjustments = {}
        
        # åŸºæ–¼æˆåŠŸç‡å»ºè­°ç½®ä¿¡åº¦é–¾å€¼èª¿æ•´
        if self.success_rates:
            for decision_type, successes in self.success_rates.items():
                if successes:
                    success_rate = sum(successes) / len(successes)
                    if success_rate < 0.6:
                        adjustments[f"{decision_type}_confidence_threshold"] = 0.1  # æé«˜é–¾å€¼
                    elif success_rate > 0.9:
                        adjustments[f"{decision_type}_confidence_threshold"] = -0.05  # é™ä½é–¾å€¼
        
        return adjustments
    
    def _analyze_feedback_integration(self) -> Dict[str, Any]:
        """åˆ†æå›é¥‹æ•´åˆ"""
        return {
            "total_outcomes_tracked": len(self.outcome_history),
            "feedback_coverage": len(self.outcome_history) / max(len(self.decision_history), 1),
            "learning_effectiveness": "good" if len(self.outcome_history) > 0 else "limited",
            "improvement_areas": ["å¢åŠ çµæœè¿½è¹¤è¦†è“‹ç‡", "åŠ å¼·å³æ™‚å›é¥‹æ©Ÿåˆ¶"]
        }
    
    def _summarize_learning_insights(self) -> str:
        """ç¸½çµå­¸ç¿’æ´å¯Ÿ"""
        if not self.decision_history:
            return "æš«ç„¡è¶³å¤ æ•¸æ“šç”Ÿæˆå­¸ç¿’ç¸½çµ"
        
        total_decisions = len(self.decision_history)
        total_outcomes = len(self.outcome_history)
        
        return f"å·²è¿½è¹¤ {total_decisions} å€‹æ±ºç­–ï¼Œå…¶ä¸­ {total_outcomes} å€‹æœ‰çµæœå›é¥‹ã€‚ç³»çµ±æ­£åœ¨æŒçºŒå­¸ç¿’å’Œå„ªåŒ–æ±ºç­–æ¨¡å¼ã€‚"
'''
    
    def create_optimized_implementation(self):
        """å‰µå»ºå„ªåŒ–çš„å¯¦ç¾"""
        print("ğŸ”§ é–‹å§‹ EPL Python å¯¦ç¾å„ªåŒ–")
        print("=" * 60)
        
        # åˆ†æç•¶å‰å¯¦ç¾
        current_analysis = self.analyze_current_implementation()
        print(f"âœ… ç•¶å‰å¯¦ç¾åˆ†æå®Œæˆ:")
        print(f"  - é¡åˆ¥æ•¸: {len(current_analysis.get('classes', []))}")
        print(f"  - æ–¹æ³•æ•¸: {len(current_analysis.get('methods', []))}")
        print(f"  - ä»£ç¢¼è¡Œæ•¸: {current_analysis.get('total_lines', 0)}")
        
        # è­˜åˆ¥ç¼ºå¤±çµ„ä»¶
        missing = self.identify_missing_components()
        print(f"\nğŸ“‹ ç¼ºå¤±çµ„ä»¶è­˜åˆ¥:")
        print(f"  - ç¼ºå¤±æ•¸æ“šé¡: {len(missing['dataclasses'])}")
        print(f"  - ç¼ºå¤±æ ¸å¿ƒæ–¹æ³•: {len(missing['core_methods'])}")
        print(f"  - ç¼ºå¤±åˆ†ææ–¹æ³•: {len(missing['analytics_methods'])}")
        print(f"  - ç¼ºå¤±æ•´åˆæ–¹æ³•: {len(missing['integration_methods'])}")
        
        total_missing = sum(len(v) for v in missing.values())
        
        if total_missing == 0:
            print("âœ… å¯¦ç¾å·²å®Œæ•´ï¼Œç„¡éœ€å„ªåŒ–")
            return True
        
        print(f"\nğŸ”§ éœ€è¦æ·»åŠ  {total_missing} å€‹çµ„ä»¶")
        
        # ç”Ÿæˆå„ªåŒ–ä»£ç¢¼
        optimization_content = ""
        
        if missing['dataclasses']:
            print("ğŸ“ ç”Ÿæˆç¼ºå¤±çš„æ•¸æ“šé¡...")
            optimization_content += self.generate_missing_dataclasses()
        
        if missing['core_methods']:
            print("ğŸ“ ç”Ÿæˆç¼ºå¤±çš„æ ¸å¿ƒæ–¹æ³•...")
            optimization_content += self.generate_missing_core_methods()
        
        if missing['analytics_methods']:
            print("ğŸ“ ç”Ÿæˆç¼ºå¤±çš„åˆ†ææ–¹æ³•...")
            optimization_content += self.generate_missing_analytics_methods()
        
        if missing['integration_methods']:
            print("ğŸ“ ç”Ÿæˆç¼ºå¤±çš„æ•´åˆæ–¹æ³•...")
            optimization_content += self.generate_missing_integration_methods()
        
        # æ·»åŠ è¼”åŠ©æ–¹æ³•
        optimization_content += self.generate_helper_methods()
        
        print("âœ… å„ªåŒ–ä»£ç¢¼ç”Ÿæˆå®Œæˆ")
        return optimization_content
    
    def apply_optimizations(self):
        """æ‡‰ç”¨å„ªåŒ–"""
        try:
            # è®€å–ç•¶å‰æ–‡ä»¶
            with open(self.python_path, 'r', encoding='utf-8') as f:
                current_content = f.read()
            
            # ç”Ÿæˆå„ªåŒ–å…§å®¹
            optimization_content = self.create_optimized_implementation()
            
            if isinstance(optimization_content, str) and optimization_content:
                # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ å„ªåŒ–å…§å®¹
                optimized_content = current_content + optimization_content
                
                # å‰µå»ºå‚™ä»½
                backup_path = self.python_path.with_suffix('.py.backup')
                with open(backup_path, 'w', encoding='utf-8') as f:
                    f.write(current_content)
                
                print(f"âœ… å·²å‰µå»ºå‚™ä»½: {backup_path}")
                
                # å¯«å…¥å„ªåŒ–å¾Œçš„å…§å®¹
                with open(self.python_path, 'w', encoding='utf-8') as f:
                    f.write(optimized_content)
                
                print(f"âœ… å·²æ‡‰ç”¨å„ªåŒ–åˆ°: {self.python_path}")
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ æ‡‰ç”¨å„ªåŒ–å¤±æ•—: {e}")
            return False

def main():
    """ä¸»å‡½æ•¸"""
    optimizer = EPLPythonOptimizer()
    
    print("ğŸ”§ EPL Decision History Tracking Python å¯¦ç¾å„ªåŒ–")
    print("=" * 70)
    
    success = optimizer.apply_optimizations()
    
    if success:
        print("\nğŸ‰ EPL Python å¯¦ç¾å„ªåŒ–å®Œæˆ!")
        print("âœ… å·²æ·»åŠ ç¼ºå¤±çš„æ•¸æ“šé¡å’Œæ–¹æ³•")
        print("âœ… å·²å®Œå–„ Phase é–“æ•´åˆåŠŸèƒ½")
        print("âœ… å·²å¢å¼·æ±ºç­–åˆ†æèƒ½åŠ›")
        print("\nğŸš€ æº–å‚™é€²è¡ŒåŠŸèƒ½æ¸¬è©¦é©—è­‰...")
    else:
        print("\nâŒ å„ªåŒ–éç¨‹ä¸­å‡ºç¾å•é¡Œ")
        print("è«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯ä¸¦é‡æ–°å˜—è©¦")

if __name__ == "__main__":
    main()
