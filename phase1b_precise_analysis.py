#!/usr/bin/env python3
"""
ğŸ¯ ç²¾ç¢ºæ·±åº¦åˆ†æå·¥å…· - phase1b_volatility_adaptation.py vs JSON è¦ç¯„
ä¸å¯ç¹éä»»ä½•ç´°ç¯€ï¼Œé€²è¡Œç²¾ç¢ºåŒ¹é…æª¢æŸ¥
"""

import sys
import os
import json
import re
import ast
from typing import Dict, List, Any, Set

print("ğŸ” é–‹å§‹ç²¾ç¢ºæ·±åº¦åˆ†æ - phase1b_volatility_adaptation.py vs JSON è¦ç¯„")
print("=" * 100)

class Phase1BPreciseAnalyzer:
    def __init__(self):
        self.json_spec = self._load_json_spec()
        self.py_code = self._load_python_code()
        self.analysis_results = {
            'matched': [],
            'missing': [],
            'partially_matched': [],
            'extra_implementations': [],
            'data_flow_breaks': []
        }
    
    def _load_json_spec(self) -> Dict:
        """è¼‰å…¥ JSON è¦ç¯„"""
        try:
            with open("/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation_dependency.json", 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è¼‰å…¥ JSON è¦ç¯„: {e}")
            return {}
    
    def _load_python_code(self) -> str:
        """è¼‰å…¥ Python ä»£ç¢¼"""
        try:
            with open("/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation.py", 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"âŒ ç„¡æ³•è¼‰å…¥ Python ä»£ç¢¼: {e}")
            return ""
    
    def analyze_computation_layers(self):
        """åˆ†æè¨ˆç®—å±¤ç´š - æª¢æŸ¥ 4 å±¤æ¶æ§‹å¯¦ç¾"""
        print("\nğŸ“Š è¨ˆç®—å±¤ç´šç²¾ç¢ºåˆ†æ")
        print("-" * 60)
        
        computation_flow = self.json_spec['strategy_dependency_graph']['computation_flow']
        
        layer_mapping = {
            'layer_1_data_collection': {
                'operations': ['historical_volatility_calculation', 'realized_volatility_calculation', 'volatility_regime_detection'],
                'expected_methods': ['_calculate_historical_volatility', '_calculate_realized_volatility', '_detect_volatility_regime']
            },
            'layer_2_volatility_metrics': {
                'operations': ['volatility_percentile', 'volatility_trend', 'regime_stability', 'market_activity_factor', 'signal_smoothing'],
                'expected_methods': ['_calculate_volatility_percentile', '_calculate_volatility_trend', '_calculate_regime_stability', '_calculate_market_activity_factor', '_smooth_signals']
            },
            'layer_3_adaptive_parameters': {
                'operations': ['signal_threshold_adaptation', 'position_size_scaling', 'timeframe_optimization', 'market_sentiment_integration'],
                'expected_methods': ['_adapt_signal_threshold', '_scale_position_size', '_optimize_timeframe', '_integrate_market_sentiment']
            },
            'layer_4_strategy_signals': {
                'operations': ['volatility_breakout_signal', 'volatility_mean_reversion_signal', 'volatility_regime_change_signal'],
                'expected_methods': ['_generate_breakout_signal', '_generate_mean_reversion_signal', '_generate_regime_change_signal']
            }
        }
        
        implementation_score = 0
        total_layers = len(layer_mapping)
        
        for layer_name, layer_info in layer_mapping.items():
            print(f"\nğŸ” æª¢æŸ¥ {layer_name}:")
            
            # æª¢æŸ¥ JSON è¦ç¯„ä¸­çš„æ“ä½œ
            if layer_name in computation_flow:
                json_operations = computation_flow[layer_name]['operations']
                print(f"  ğŸ“‹ JSON è¦ç¯„æ“ä½œ: {list(json_operations.keys())}")
                
                # æª¢æŸ¥å°æ‡‰çš„ Python æ–¹æ³•å¯¦ç¾
                implemented_methods = 0
                for expected_method in layer_info['expected_methods']:
                    if expected_method in self.py_code:
                        implemented_methods += 1
                        print(f"    âœ… {expected_method}: å·²å¯¦ç¾")
                    else:
                        print(f"    âŒ {expected_method}: ç¼ºå¤±")
                        self.analysis_results['missing'].append(f"{layer_name}.{expected_method}")
                
                # æª¢æŸ¥æ•¸æ“šæµä¾è³´
                dependencies = computation_flow[layer_name].get('dependencies', [])
                print(f"  ğŸ“‹ ä¾è³´é—œä¿‚: {dependencies}")
                
                if implemented_methods == len(layer_info['expected_methods']):
                    implementation_score += 1
                    print(f"  ğŸŸ¢ {layer_name}: å®Œå…¨å¯¦ç¾ ({implemented_methods}/{len(layer_info['expected_methods'])})")
                elif implemented_methods > 0:
                    implementation_score += 0.5
                    print(f"  ğŸŸ¡ {layer_name}: éƒ¨åˆ†å¯¦ç¾ ({implemented_methods}/{len(layer_info['expected_methods'])})")
                else:
                    print(f"  ğŸ”´ {layer_name}: æœªå¯¦ç¾")
            else:
                print(f"  âŒ JSON è¦ç¯„ä¸­ç¼ºå°‘ {layer_name}")
        
        print(f"\nğŸ“Š è¨ˆç®—å±¤ç´šåŒ¹é…åº¦: {implementation_score/total_layers:.1%}")
        return implementation_score/total_layers
    
    def analyze_data_structures(self):
        """åˆ†ææ•¸æ“šçµæ§‹å®Œæ•´æ€§"""
        print("\nğŸ“Š æ•¸æ“šçµæ§‹åˆ†æ")
        print("-" * 60)
        
        # å¾ JSON ä¸­æå–æ‰€éœ€çš„æ•¸æ“šçµæ§‹
        required_data_structures = {
            'VolatilityMetrics': [
                'current_volatility', 'volatility_trend', 'volatility_percentile', 
                'regime_stability', 'enhanced_volatility_percentile', 'volatility_regime',
                'market_activity_factor', 'regime_change_probability'
            ],
            'AdaptiveSignalAdjustment': [
                'original_signal', 'adjusted_signal', 'adjustment_factor', 
                'adjustment_reason', 'confidence_boost', 'risk_mitigation'
            ],
            'VolatilityRegime': ['LOW', 'NORMAL', 'HIGH', 'EXTREME'],
            'MarketActivityLevel': ['LOW', 'NORMAL', 'HIGH']
        }
        
        structure_score = 0
        total_structures = len(required_data_structures)
        
        for structure_name, required_fields in required_data_structures.items():
            print(f"\nğŸ” æª¢æŸ¥æ•¸æ“šçµæ§‹: {structure_name}")
            
            if f"class {structure_name}" in self.py_code or f"{structure_name}(Enum)" in self.py_code:
                print(f"  âœ… {structure_name} é¡åˆ¥å­˜åœ¨")
                
                # æª¢æŸ¥å­—æ®µ
                implemented_fields = 0
                for field in required_fields:
                    if field in self.py_code:
                        implemented_fields += 1
                        print(f"    âœ… {field}: å·²å¯¦ç¾")
                    else:
                        print(f"    âŒ {field}: ç¼ºå¤±")
                
                if implemented_fields == len(required_fields):
                    structure_score += 1
                elif implemented_fields > 0:
                    structure_score += 0.5
            else:
                print(f"  âŒ {structure_name} é¡åˆ¥ç¼ºå¤±")
        
        print(f"\nğŸ“Š æ•¸æ“šçµæ§‹åŒ¹é…åº¦: {structure_score/total_structures:.1%}")
        return structure_score/total_structures
    
    def analyze_signal_generation_logic(self):
        """åˆ†æä¿¡è™Ÿç”Ÿæˆé‚è¼¯"""
        print("\nğŸ“Š ä¿¡è™Ÿç”Ÿæˆé‚è¼¯åˆ†æ")
        print("-" * 60)
        
        # å¾ JSON ä¸­æå–ä¿¡è™Ÿç”Ÿæˆé‚è¼¯
        layer_4_operations = self.json_spec['strategy_dependency_graph']['computation_flow']['layer_4_strategy_signals']['operations']
        
        signal_types = {
            'volatility_breakout_signal': {
                'condition': 'volatility_percentile > 0.9 AND volatility_trend > 0.5 AND volume_confirmation',
                'method': '_generate_breakout_signal',
                'output': 'enhanced_breakout_signal'
            },
            'volatility_mean_reversion_signal': {
                'condition': 'volatility_percentile > 0.8 AND regime_stability > 0.7 AND volume_confirmation',
                'method': '_generate_mean_reversion_signal',
                'output': 'enhanced_mean_reversion_signal'
            },
            'volatility_regime_change_signal': {
                'condition': 'regime_change_detected AND regime_stability < 0.3 AND multi_confirmation',
                'method': '_generate_regime_change_signal',
                'output': 'enhanced_regime_change_signal'
            }
        }
        
        logic_score = 0
        total_signals = len(signal_types)
        
        for signal_name, signal_info in signal_types.items():
            print(f"\nğŸ” æª¢æŸ¥ä¿¡è™Ÿ: {signal_name}")
            
            # æª¢æŸ¥ JSON è¦ç¯„
            if signal_name in layer_4_operations:
                json_condition = layer_4_operations[signal_name]['condition']
                print(f"  ğŸ“‹ JSON æ¢ä»¶: {json_condition}")
                
                # æª¢æŸ¥æ–¹æ³•å¯¦ç¾
                method_name = signal_info['method']
                if method_name in self.py_code:
                    print(f"  âœ… æ–¹æ³•: {method_name} å·²å¯¦ç¾")
                    
                    # æª¢æŸ¥é‚è¼¯æ¢ä»¶
                    condition_keywords = ['volatility_percentile', 'volatility_trend', 'regime_stability']
                    condition_implemented = any(keyword in self.py_code for keyword in condition_keywords)
                    
                    if condition_implemented:
                        logic_score += 1
                        print(f"  âœ… é‚è¼¯æ¢ä»¶: å·²å¯¦ç¾")
                    else:
                        logic_score += 0.5
                        print(f"  ğŸŸ¡ é‚è¼¯æ¢ä»¶: éƒ¨åˆ†å¯¦ç¾")
                else:
                    print(f"  âŒ æ–¹æ³•: {method_name} ç¼ºå¤±")
            else:
                print(f"  âŒ JSON è¦ç¯„ä¸­ç¼ºå°‘ {signal_name}")
        
        print(f"\nğŸ“Š ä¿¡è™Ÿç”Ÿæˆé‚è¼¯åŒ¹é…åº¦: {logic_score/total_signals:.1%}")
        return logic_score/total_signals
    
    def analyze_data_flow_integrity(self):
        """åˆ†ææ•¸æ“šæµå®Œæ•´æ€§"""
        print("\nğŸ“Š æ•¸æ“šæµå®Œæ•´æ€§åˆ†æ")
        print("-" * 60)
        
        # åˆ†ææ•¸æ“šæµéˆè·¯
        data_flows = [
            {
                'source': 'historical_volatility_calculation',
                'data': 'OHLCVæ­·å²æ•¸æ“š',
                'output': 'historical_volatility',
                'next_layer': 'volatility_percentile'
            },
            {
                'source': 'realized_volatility_calculation',
                'data': 'é«˜é »åƒ¹æ ¼æ•¸æ“š',
                'output': 'realized_volatility',
                'next_layer': 'volatility_trend'
            },
            {
                'source': 'volatility_regime_detection',
                'data': 'volatility_timeseries',
                'output': 'enhanced_volatility_regime',
                'next_layer': 'adaptive_parameters'
            },
            {
                'source': 'adaptive_parameters',
                'data': 'volatility_metrics',
                'output': 'adaptive_signal_threshold',
                'next_layer': 'strategy_signals'
            }
        ]
        
        flow_integrity_score = 0
        total_flows = len(data_flows)
        
        for i, flow in enumerate(data_flows):
            print(f"\nğŸ” æ•¸æ“šæµ {i+1}: {flow['source']} â†’ {flow['next_layer']}")
            
            # æª¢æŸ¥æ•¸æ“šè¼¸å…¥/è¼¸å‡º
            source_exists = flow['source'].replace('_', '') in self.py_code.replace('_', '')
            output_used = flow['output'].replace('_', '') in self.py_code.replace('_', '')
            
            print(f"  ğŸ“‹ æ•¸æ“šä¾†æº: {flow['data']}")
            print(f"  ğŸ“‹ è™•ç†å‡½æ•¸: {flow['source']} {'âœ…' if source_exists else 'âŒ'}")
            print(f"  ğŸ“‹ è¼¸å‡ºä½¿ç”¨: {flow['output']} {'âœ…' if output_used else 'âŒ'}")
            
            if source_exists and output_used:
                flow_integrity_score += 1
                print(f"  ğŸŸ¢ æ•¸æ“šæµå®Œæ•´")
            elif source_exists or output_used:
                flow_integrity_score += 0.5
                print(f"  ğŸŸ¡ æ•¸æ“šæµéƒ¨åˆ†å¯¦ç¾")
                self.analysis_results['data_flow_breaks'].append(f"Flow {i+1}: {flow['source']}")
            else:
                print(f"  ğŸ”´ æ•¸æ“šæµæ–·é»")
                self.analysis_results['data_flow_breaks'].append(f"Flow {i+1}: {flow['source']}")
        
        print(f"\nğŸ“Š æ•¸æ“šæµå®Œæ•´æ€§: {flow_integrity_score/total_flows:.1%}")
        return flow_integrity_score/total_flows
    
    def analyze_performance_optimization(self):
        """åˆ†ææ€§èƒ½å„ªåŒ–å¯¦ç¾"""
        print("\nğŸ“Š æ€§èƒ½å„ªåŒ–åˆ†æ")
        print("-" * 60)
        
        # å¾ JSON ä¸­æå–æ€§èƒ½å„ªåŒ–è¦æ±‚
        optimization_strategies = self.json_spec['strategy_dependency_graph']['enhanced_optimization_strategies']
        
        optimization_features = {
            'multi_confirmation_system': 'multi_confirmation',
            'vectorized_volatility_calculation': 'numpy',
            'weighted_percentile_optimization': 'weighted_percentile',
            'layered_caching_strategy': 'cache',
            'signal_smoothing_optimization': 'smoothing',
            'market_sentiment_integration': 'sentiment',
            'smart_timeframe_switching': 'timeframe'
        }
        
        optimization_score = 0
        total_optimizations = len(optimization_features)
        
        for feature_name, keyword in optimization_features.items():
            print(f"\nğŸ” æª¢æŸ¥å„ªåŒ–: {feature_name}")
            
            if keyword in self.py_code.lower():
                optimization_score += 1
                print(f"  âœ… {feature_name}: å·²å¯¦ç¾")
            else:
                print(f"  âŒ {feature_name}: ç¼ºå¤±")
                self.analysis_results['missing'].append(f"optimization.{feature_name}")
        
        print(f"\nğŸ“Š æ€§èƒ½å„ªåŒ–åŒ¹é…åº¦: {optimization_score/total_optimizations:.1%}")
        return optimization_score/total_optimizations
    
    def analyze_integration_points(self):
        """åˆ†ææ•´åˆé»"""
        print("\nğŸ“Š æ•´åˆé»åˆ†æ")
        print("-" * 60)
        
        integration_points = self.json_spec['strategy_dependency_graph']['integration_points']
        
        # æª¢æŸ¥è¼¸å…¥ä¾†æº
        receives_from = integration_points['receives_from']
        feeds_to = integration_points['feeds_to']
        cross_validation = integration_points['cross_validation_with']
        
        integration_score = 0
        total_integrations = len(receives_from) + len(feeds_to) + len(cross_validation)
        
        print(f"ğŸ” è¼¸å…¥ä¾†æºæª¢æŸ¥:")
        for source in receives_from:
            source_keyword = source.replace('_', '').lower()
            if source_keyword in self.py_code.lower().replace('_', ''):
                integration_score += 1
                print(f"  âœ… {source}: å·²æ•´åˆ")
            else:
                print(f"  âŒ {source}: ç¼ºå¤±")
        
        print(f"\nğŸ” è¼¸å‡ºç›®æ¨™æª¢æŸ¥:")
        for target in feeds_to:
            target_keyword = target.replace('_', '').lower()
            if target_keyword in self.py_code.lower().replace('_', ''):
                integration_score += 1
                print(f"  âœ… {target}: å·²æ•´åˆ")
            else:
                print(f"  âŒ {target}: ç¼ºå¤±")
        
        print(f"\nğŸ” è·¨æ¨¡çµ„é©—è­‰æª¢æŸ¥:")
        for module in cross_validation:
            module_keyword = module.replace('_', '').lower()
            if module_keyword in self.py_code.lower().replace('_', ''):
                integration_score += 1
                print(f"  âœ… {module}: å·²æ•´åˆ")
            else:
                print(f"  âŒ {module}: ç¼ºå¤±")
        
        print(f"\nğŸ“Š æ•´åˆé»åŒ¹é…åº¦: {integration_score/total_integrations:.1%}")
        return integration_score/total_integrations
    
    def analyze_signal_output_format(self):
        """åˆ†æä¿¡è™Ÿè¼¸å‡ºæ ¼å¼"""
        print("\nğŸ“Š ä¿¡è™Ÿè¼¸å‡ºæ ¼å¼åˆ†æ")
        print("-" * 60)
        
        signal_format = self.json_spec['strategy_dependency_graph']['signal_output_format']['enhanced_volatility_adapted_signal']
        
        required_fields = [
            'signal_type', 'signal_strength', 'signal_confidence', 'execution_priority',
            'adaptive_parameters', 'market_context', 'quality_indicators', 'timestamp', 'source'
        ]
        
        format_score = 0
        total_fields = len(required_fields)
        
        for field in required_fields:
            if field in self.py_code:
                format_score += 1
                print(f"  âœ… {field}: å·²å¯¦ç¾")
            else:
                print(f"  âŒ {field}: ç¼ºå¤±")
                self.analysis_results['missing'].append(f"signal_format.{field}")
        
        print(f"\nğŸ“Š ä¿¡è™Ÿè¼¸å‡ºæ ¼å¼åŒ¹é…åº¦: {format_score/total_fields:.1%}")
        return format_score/total_fields
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€çµ‚ç²¾ç¢ºåˆ†æå ±å‘Š"""
        print("\n" + "=" * 100)
        print("ğŸ¯ æœ€çµ‚ç²¾ç¢ºæ·±åº¦åˆ†æå ±å‘Š")
        print("=" * 100)
        
        # åŸ·è¡Œæ‰€æœ‰åˆ†æ
        scores = {}
        scores['computation_layers'] = self.analyze_computation_layers()
        scores['data_structures'] = self.analyze_data_structures()
        scores['signal_generation_logic'] = self.analyze_signal_generation_logic()
        scores['data_flow_integrity'] = self.analyze_data_flow_integrity()
        scores['performance_optimization'] = self.analyze_performance_optimization()
        scores['integration_points'] = self.analyze_integration_points()
        scores['signal_output_format'] = self.analyze_signal_output_format()
        
        # è¨ˆç®—ç¸½é«”åŒ¹é…åº¦
        total_score = sum(scores.values()) / len(scores)
        
        print(f"\nğŸ“Š å„çµ„ä»¶åŒ¹é…åº¦è©³ç´°çµæœ:")
        print("-" * 60)
        for component, score in scores.items():
            status = "ğŸŸ¢" if score >= 0.8 else "ğŸŸ¡" if score >= 0.6 else "ğŸ”´"
            print(f"  {status} {component:25}: {score:6.1%}")
        
        print(f"\nğŸ† ç¸½é«”ç²¾ç¢ºåŒ¹é…åº¦: {total_score:.1%}")
        
        if total_score >= 0.9:
            status = "ğŸŸ¢ å„ªç§€åŒ¹é… (Excellent)"
        elif total_score >= 0.8:
            status = "ğŸŸ¡ è‰¯å¥½åŒ¹é… (Good)"
        elif total_score >= 0.7:
            status = "ğŸŸ  éƒ¨åˆ†åŒ¹é… (Partial)"
        else:
            status = "ğŸ”´ éœ€è¦æ”¹é€² (Needs Improvement)"
        
        print(f"ğŸ“‹ åŒ¹é…ç‹€æ…‹: {status}")
        
        # æ•¸æ“šæµæ–·é»åˆ†æ
        if self.analysis_results['data_flow_breaks']:
            print(f"\nğŸ” ç™¼ç¾æ•¸æ“šæµæ–·é»:")
            for break_point in self.analysis_results['data_flow_breaks']:
                print(f"  âš ï¸  {break_point}")
        else:
            print(f"\nâœ… æœªç™¼ç¾æ•¸æ“šæµæ–·é»")
        
        # è­˜åˆ¥é—œéµç¼ºå¤±é …ç›®
        if self.analysis_results['missing']:
            print(f"\nğŸ” é—œéµç¼ºå¤±é …ç›®:")
            for missing_item in self.analysis_results['missing'][:10]:  # é¡¯ç¤ºå‰10å€‹
                print(f"  âš ï¸  {missing_item}")
            if len(self.analysis_results['missing']) > 10:
                print(f"  ... åŠå…¶ä»– {len(self.analysis_results['missing']) - 10} å€‹é …ç›®")
        
        # é‡é»æ”¹é€²å»ºè­°
        print(f"\nğŸ” é‡é»æ”¹é€²å»ºè­°:")
        for component, score in scores.items():
            if score < 0.8:
                print(f"  âš ï¸  {component}: éœ€è¦é‡é»æ”¹é€² ({score:.1%})")
        
        return total_score, scores

if __name__ == "__main__":
    analyzer = Phase1BPreciseAnalyzer()
    total_score, detailed_scores = analyzer.generate_final_report()
    
    print(f"\nâœ… ç²¾ç¢ºæ·±åº¦åˆ†æå®Œæˆ")
    print(f"ğŸ“Š æœ€çµ‚è©•åˆ†: {total_score:.1%}")
