"""
ğŸ§  Phase 2: EPLå‰è™•ç†ç³»çµ± (Enhanced Pre-Processing Layer)
========================================================

EPLå‰è™•ç†ç³»çµ± v2.1.0 - æ™ºèƒ½ä¸‰æ­¥é©Ÿå“è³ªæ§åˆ¶èˆ‡å„ªåŒ–è·¯ç”±
1. æ™ºèƒ½å»é‡åˆ†æå¼•æ“ (Phase1æ•¸æ“šå„ªåŒ–é‡ç”¨)
2. ä¸Šä¸‹æ–‡é—œè¯åˆ†æå™¨ (å¸‚å ´ç’°å¢ƒä¿¡ä»»æ©Ÿåˆ¶) 
3. è¼•é‡å“è³ªæ§åˆ¶é–€æª» (æ•´åˆè©•åˆ†å¼•æ“)
4. æ™ºèƒ½è·¯ç”±ç³»çµ± (å¿«é€Ÿ/æ¨™æº–/æ·±åº¦é€šé“)
"""

import logging
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum

logger = logging.getLogger(__name__)

# æ¨¡æ“¬SignalCandidateæ•¸æ“šçµæ§‹
@dataclass
class SignalCandidate:
    """ä¿¡è™Ÿå€™é¸è€…æ•¸æ“šçµæ§‹"""
    id: str
    symbol: str
    signal_strength: float
    confidence: float
    direction: str
    timestamp: datetime
    source: str
    data_completeness: float
    signal_clarity: float
    dynamic_params: Dict[str, Any]
    market_environment: Dict[str, Any]
    technical_snapshot: Dict[str, Any]

class DeduplicationResult(Enum):
    """å»é‡åˆ†æçµæœ"""
    UNIQUE = "â­ ç¨ç‰¹"
    IGNORE = "âŒ å¿½ç•¥" 
    DELAY_OBSERVE = "âš ï¸ å»¶é²è§€å¯Ÿ"
    PASS = "âœ… é€šé"

class CorrelationAnalysisResult(Enum):
    """é—œè¯åˆ†æçµæœ"""
    STRENGTHEN_CANDIDATE = "â• å¼·åŒ–å€™é¸"
    REPLACE_CANDIDATE = "ğŸ” æ›¿æ›å€™é¸"
    INDEPENDENT_NEW = "âœ… ç¨ç«‹æ–°å–®"

class QualityControlResult(Enum):
    """å“è³ªæ§åˆ¶çµæœ"""
    EXCELLENT = "ğŸŒŸ å„ªç§€"
    PASS = "âœ… é€šé"
    FAIL_STRENGTH = "âŒ ä¿¡è™Ÿå¼·åº¦ä¸è¶³"
    FAIL_LIQUIDITY = "âŒ æµå‹•æ€§ä¸è¶³"
    FAIL_RISK = "âŒ é¢¨éšªè©•ä¼°æœªé€šé"

@dataclass
class PreEvaluationResult:
    """å‰è™•ç†è©•ä¼°çµæœ"""
    candidate: SignalCandidate
    deduplication_result: DeduplicationResult
    correlation_result: CorrelationAnalysisResult
    quality_result: QualityControlResult
    pass_to_epl: bool
    processing_notes: List[str]
    similarity_score: Optional[float]
    risk_assessment: Dict[str, Any]
    timestamp: datetime

class IntelligentDeduplicationEngine:
    """æ™ºèƒ½å»é‡åˆ†æå¼•æ“ - åˆ©ç”¨Phase1å‹•æ…‹åƒæ•¸å¿«é€Ÿå»é‡"""
    
    def __init__(self):
        self.processed_signals: List[SignalCandidate] = []
        self.similarity_threshold = 0.85  # JSONé…ç½®: 85%ç›¸ä¼¼åº¦é–¾å€¼
        self.time_overlap_minutes = 15    # JSONé…ç½®: 15åˆ†é˜æ™‚é–“é‡ç–Š
        self.confidence_diff_threshold = 0.03  # JSONé…ç½®: 3%ä¿¡å¿ƒåº¦å·®ç•°
        
        # JSONå¢å¼·åŠŸèƒ½: æºå…±è­˜é©—è­‰åƒæ•¸
        self.source_overlap_score_threshold = 0.72
        self.model_diversity_score_threshold = 0.8
        self.action_bias_score_threshold = 0.85
    
    async def analyze_duplication(self, candidate: SignalCandidate) -> Tuple[DeduplicationResult, float, List[str]]:
        """æ™ºèƒ½å»é‡åˆ†æ - åˆ©ç”¨Phase1 dynamic_params"""
        notes = []
        max_similarity = 0.0
        
        try:
            # JSONåŠŸèƒ½: åˆ©ç”¨Phase1 dynamic_paramsé€²è¡Œå¿«é€Ÿå»é‡
            relevant_signals = self._get_relevant_signals_with_phase1_params(candidate)
            
            if not relevant_signals:
                notes.append("Phase1å‹•æ…‹åƒæ•¸ç„¡æ­·å²åŒ¹é…")
                return DeduplicationResult.PASS, 0.0, notes
            
            # JSONå¢å¼·åŠŸèƒ½: æºå…±è­˜é©—è­‰
            source_consensus_result = self._validate_source_consensus(candidate, relevant_signals)
            
            for historical_signal in relevant_signals:
                # è¨ˆç®—ç›¸ä¼¼åº¦
                similarity = self._calculate_enhanced_similarity(candidate, historical_signal)
                max_similarity = max(max_similarity, similarity)
                
                # JSONåŠŸèƒ½: Phase1æ•¸æ“šé‡ç”¨æª¢æŸ¥
                adaptation_time_match = self._check_adaptation_timestamp_match(
                    candidate, historical_signal
                )
                
                notes.append(f"èˆ‡{historical_signal.id}ç›¸ä¼¼åº¦: {similarity:.2f}")
                
                # JSONè¦ç¯„åˆ¤æ–·é‚è¼¯
                if (similarity > 0.95 and adaptation_time_match):
                    notes.append("é«˜åº¦é‡è¤‡ï¼ˆPhase1é©æ‡‰æ™‚é–“æˆ³åŒ¹é…ï¼‰")
                    return DeduplicationResult.IGNORE, similarity, notes
                elif (similarity > self.similarity_threshold and 
                      source_consensus_result["preserve"] == False):
                    notes.append("ä¸­åº¦é‡è¤‡ï¼ˆæºå…±è­˜é©—è­‰æœªé€šéï¼‰") 
                    return DeduplicationResult.DELAY_OBSERVE, similarity, notes
            
            # JSONå¢å¼·åŠŸèƒ½: æ¨¡å‹å¤šæ¨£æ€§ä¿ç•™è¦å‰‡
            if source_consensus_result["model_diversity_score"] > self.model_diversity_score_threshold:
                notes.append("æ¨¡å‹å¤šæ¨£æ€§ä¿ç•™è¦å‰‡è§¸ç™¼")
                return DeduplicationResult.UNIQUE, max_similarity, notes
            
            notes.append(f"Phase1å„ªåŒ–å»é‡å®Œæˆï¼Œæœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.2f}")
            return DeduplicationResult.PASS, max_similarity, notes
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½å»é‡åˆ†æå¤±æ•—: {e}")
            notes.append(f"åˆ†æéŒ¯èª¤: {e}")
            return DeduplicationResult.PASS, 0.0, notes
    
    def _get_relevant_signals_with_phase1_params(self, candidate: SignalCandidate) -> List[SignalCandidate]:
        """åˆ©ç”¨Phase1å‹•æ…‹åƒæ•¸ç²å–ç›¸é—œä¿¡è™Ÿ"""
        # JSONæ•¸æ“šé‡ç”¨: id + dynamic_params.adaptation_timestamp
        candidate_adaptation_time = candidate.dynamic_params.get("adaptation_timestamp")
        cutoff_time = candidate.timestamp - timedelta(minutes=self.time_overlap_minutes)
        
        relevant = []
        for signal in self.processed_signals:
            if (signal.symbol == candidate.symbol and 
                signal.timestamp > cutoff_time and
                signal.timestamp < candidate.timestamp):
                
                # Phase1å‹•æ…‹åƒæ•¸åŒ¹é…æª¢æŸ¥
                signal_adaptation_time = signal.dynamic_params.get("adaptation_timestamp")
                if candidate_adaptation_time and signal_adaptation_time:
                    time_diff = abs((candidate_adaptation_time - signal_adaptation_time).total_seconds())
                    if time_diff < 300:  # 5åˆ†é˜å…§çš„é©æ‡‰æ™‚é–“æˆ³
                        relevant.append(signal)
                else:
                    relevant.append(signal)  # å‚™ç”¨é‚è¼¯
        
        return relevant
    
    def _validate_source_consensus(self, candidate: SignalCandidate, historical_signals: List[SignalCandidate]) -> Dict[str, Any]:
        """JSONå¢å¼·åŠŸèƒ½: æºå…±è­˜é©—è­‰"""
        if not historical_signals:
            return {"preserve": True, "model_diversity_score": 1.0}
        
        # è¨ˆç®—æºé‡ç–Šåˆ†æ•¸
        candidate_sources = set([candidate.source])
        historical_sources = set([s.source for s in historical_signals])
        source_overlap_score = len(candidate_sources.intersection(historical_sources)) / len(candidate_sources.union(historical_sources))
        
        # è¨ˆç®—æ¨¡å‹å¤šæ¨£æ€§åˆ†æ•¸
        all_sources = candidate_sources.union(historical_sources)
        model_diversity_score = len(all_sources) / (len(all_sources) + 1)  # æ­£è¦åŒ–
        
        # è¨ˆç®—è¡Œå‹•åå·®åˆ†æ•¸
        candidate_direction = candidate.direction
        historical_directions = [s.direction for s in historical_signals]
        same_direction_count = sum(1 for d in historical_directions if d == candidate_direction)
        action_bias_score = same_direction_count / len(historical_directions) if historical_directions else 0
        
        # JSONä¿ç•™è¦å‰‡: preserve_if_model_diversity_score > 0.8
        preserve = model_diversity_score > self.model_diversity_score_threshold
        
        return {
            "source_overlap_score": source_overlap_score,
            "model_diversity_score": model_diversity_score,
            "action_bias_score": action_bias_score,
            "preserve": preserve
        }
    
    def _calculate_enhanced_similarity(self, candidate1: SignalCandidate, candidate2: SignalCandidate) -> float:
        """å¢å¼·ç›¸ä¼¼åº¦è¨ˆç®—"""
        similarity_factors = []
        
        # å¼·åº¦ç›¸ä¼¼åº¦
        strength_similarity = 1 - abs(candidate1.signal_strength - candidate2.signal_strength) / 100
        similarity_factors.append(strength_similarity * 0.3)
        
        # æ–¹å‘ç›¸ä¼¼åº¦
        direction_similarity = 1.0 if candidate1.direction == candidate2.direction else 0.0
        similarity_factors.append(direction_similarity * 0.2)
        
        # ä¾†æºç›¸ä¼¼åº¦
        source_similarity = 1.0 if candidate1.source == candidate2.source else 0.5
        similarity_factors.append(source_similarity * 0.2)
        
        # æŠ€è¡“æŒ‡æ¨™ç›¸ä¼¼åº¦
        tech_similarity = self._calculate_technical_similarity(
            candidate1.technical_snapshot, candidate2.technical_snapshot
        )
        similarity_factors.append(tech_similarity * 0.3)
        
        return sum(similarity_factors)
    
    def _calculate_technical_similarity(self, tech1: Dict[str, Any], tech2: Dict[str, Any]) -> float:
        """è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ç›¸ä¼¼åº¦"""
        try:
            similarities = []
            
            # RSIç›¸ä¼¼åº¦
            rsi_sim = 1 - abs(tech1.get("rsi", 50) - tech2.get("rsi", 50)) / 100
            similarities.append(rsi_sim)
            
            # MACDç›¸ä¼¼åº¦  
            macd_sim = 1 - min(1.0, abs(tech1.get("macd_signal", 0) - tech2.get("macd_signal", 0)) / 2)
            similarities.append(macd_sim)
            
            # å¸ƒæ—å¸¶ä½ç½®ç›¸ä¼¼åº¦
            bb_sim = 1 - abs(tech1.get("bollinger_position", 0.5) - tech2.get("bollinger_position", 0.5))
            similarities.append(bb_sim)
            
            return sum(similarities) / len(similarities)  # å¹³å‡å€¼
            
        except Exception:
            return 0.5  # é è¨­ä¸­ç­‰ç›¸ä¼¼åº¦
    
    def _check_adaptation_timestamp_match(self, candidate1: SignalCandidate, candidate2: SignalCandidate) -> bool:
        """æª¢æŸ¥Phase1é©æ‡‰æ™‚é–“æˆ³åŒ¹é…"""
        time1 = candidate1.dynamic_params.get("adaptation_timestamp")
        time2 = candidate2.dynamic_params.get("adaptation_timestamp")
        
        if time1 and time2:
            time_diff = abs((time1 - time2).total_seconds())
            return time_diff < 60  # 1åˆ†é˜å…§è¦–ç‚ºåŒ¹é…
        return False
    
    def add_processed_signal(self, signal: SignalCandidate):
        """æ·»åŠ å·²è™•ç†ä¿¡è™Ÿåˆ°æ­·å²è¨˜éŒ„"""
        self.processed_signals.append(signal)
        
        # ä¿æŒæ­·å²è¨˜éŒ„åœ¨åˆç†ç¯„åœå…§
        cutoff_time = datetime.now() - timedelta(hours=1)
        self.processed_signals = [s for s in self.processed_signals if s.timestamp > cutoff_time]

class ContextualCorrelationAnalyzer:
    """ä¸Šä¸‹æ–‡é—œè¯åˆ†æå™¨ - åˆ©ç”¨Phase1å¸‚å ´ç’°å¢ƒå¿«é€Ÿæ±ºç­–"""
    
    def __init__(self):
        self.current_positions: Dict[str, SignalCandidate] = {}
        self.symbol_correlations: Dict[str, List[str]] = {}
        
        # JSONé…ç½®åƒæ•¸
        self.direction_conflict_threshold = 0.15
        self.confidence_improvement_threshold = 0.08
    
    async def analyze_correlation(self, candidate: SignalCandidate) -> Tuple[CorrelationAnalysisResult, List[str]]:
        """ä¸Šä¸‹æ–‡é—œè¯åˆ†æ - ä¿¡ä»»Phase1å¸‚å ´ç’°å¢ƒåˆ†æ"""
        notes = []
        
        try:
            # JSONåŠŸèƒ½: åˆ©ç”¨Phase1å¸‚å ´ç’°å¢ƒæ•¸æ“š
            market_analysis = self._leverage_phase1_market_environment(candidate)
            notes.extend(market_analysis["notes"])
            
            # æª¢æŸ¥ç•¶å‰æŒå€‰
            current_position = self.current_positions.get(candidate.symbol)
            
            if current_position:
                # JSONæ±ºç­–é‚è¼¯: strengthen/replace/independent_decision
                decision_result = self._make_correlation_decision(candidate, current_position, market_analysis)
                notes.extend(decision_result["notes"])
                return decision_result["result"], notes
            
            # æª¢æŸ¥ç›¸é—œæ¨™çš„é—œè¯æ€§
            correlation_impact = self._assess_symbol_correlations(candidate)
            notes.extend(correlation_impact["notes"])
            
            notes.append("JSONè¦ç¯„: ä¿¡ä»»Phase1å¸‚å ´ç’°å¢ƒï¼Œè¦–ç‚ºç¨ç«‹æ–°æ©Ÿæœƒ")
            return CorrelationAnalysisResult.INDEPENDENT_NEW, notes
            
        except Exception as e:
            logger.error(f"ä¸Šä¸‹æ–‡é—œè¯åˆ†æå¤±æ•—: {e}")
            notes.append(f"åˆ†æéŒ¯èª¤: {e}")
            return CorrelationAnalysisResult.INDEPENDENT_NEW, notes
    
    def _leverage_phase1_market_environment(self, candidate: SignalCandidate) -> Dict[str, Any]:
        """JSONåŠŸèƒ½: åˆ©ç”¨Phase1å¸‚å ´ç’°å¢ƒé€²è¡Œå¿«é€Ÿæ±ºç­–"""
        market_env = candidate.market_environment
        notes = []
        
        # JSONæ•¸æ“šé‡ç”¨: market_environment.volatility + liquidity_score
        volatility = market_env.get("volatility", 0.02)
        liquidity_score = market_env.get("liquidity_score", 0.7)
        momentum = market_env.get("momentum", 0.0)
        
        notes.append(f"Phase1å¸‚å ´ç’°å¢ƒ: æ³¢å‹•æ€§={volatility:.3f}, æµå‹•æ€§={liquidity_score:.2f}")
        
        # JSONå„ªåŒ–ç­–ç•¥: trust_phase1_market_analysis_results
        market_regime = "stable"
        if volatility > 0.05:
            market_regime = "high_volatility"
        elif volatility < 0.01:
            market_regime = "low_volatility"
        
        if liquidity_score < 0.3:
            market_regime += "_low_liquidity"
        
        notes.append(f"JSONå„ªåŒ–: ä¿¡ä»»Phase1å¸‚å ´åˆ†æï¼Œå¸‚å ´ç‹€æ…‹={market_regime}")
        
        return {
            "volatility": volatility,
            "liquidity_score": liquidity_score,
            "momentum": momentum,
            "market_regime": market_regime,
            "notes": notes
        }
    
    def _make_correlation_decision(self, candidate: SignalCandidate, current_position: SignalCandidate, market_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """JSONæ±ºç­–é‚è¼¯: strengthen/replace/independent_decision"""
        notes = []
        
        # æ–¹å‘è¡çªåˆ†æ
        direction_conflict = candidate.direction != current_position.direction
        
        # ä¿¡å¿ƒåº¦å·®ç•°
        confidence_diff = candidate.confidence - current_position.confidence
        
        # æŒå€‰å¼·åº¦å·®ç•°
        strength_diff = candidate.signal_strength - current_position.signal_strength
        
        notes.append(f"æ–¹å‘è¡çª: {direction_conflict}, ä¿¡å¿ƒåº¦å·®ç•°: {confidence_diff:.3f}")
        notes.append(f"å¼·åº¦å·®ç•°: {strength_diff:.1f}")
        
        # JSONæ±ºç­–é‚è¼¯
        if direction_conflict and confidence_diff > self.direction_conflict_threshold:
            notes.append("JSONæ±ºç­–: æ–¹å‘ç›¸åä¸”ä¿¡å¿ƒåº¦å¤§å¹…æå‡ -> æ›¿æ›å€™é¸")
            return {"result": CorrelationAnalysisResult.REPLACE_CANDIDATE, "notes": notes}
        
        elif not direction_conflict and confidence_diff > self.confidence_improvement_threshold:
            notes.append("JSONæ±ºç­–: æ–¹å‘ç›¸åŒä¸”ä¿¡å¿ƒåº¦æå‡ -> å¼·åŒ–å€™é¸")
            return {"result": CorrelationAnalysisResult.STRENGTHEN_CANDIDATE, "notes": notes}
        
        else:
            notes.append("JSONæ±ºç­–: é—œè¯åº¦ä½ -> ç¨ç«‹æ–°å–®")
            return {"result": CorrelationAnalysisResult.INDEPENDENT_NEW, "notes": notes}
    
    def _assess_symbol_correlations(self, candidate: SignalCandidate) -> Dict[str, Any]:
        """è©•ä¼°æ¨™çš„ç›¸é—œæ€§"""
        notes = []
        
        # é å®šç¾©ç›¸é—œæ€§çµ„
        correlation_groups = {
            "BTCUSDT": ["ETHUSDT", "ADAUSDT"],
            "ETHUSDT": ["BTCUSDT", "DOTUSDT", "LINKUSDT"],
            "ADAUSDT": ["DOTUSDT", "ATOMUSDT"],
            "SOLUSDT": ["AVAXUSDT", "NEARUSDT"]
        }
        
        correlated_symbols = correlation_groups.get(candidate.symbol, [])
        notes.append(f"ç›¸é—œæ¨™çš„: {correlated_symbols}")
        
        # æª¢æŸ¥ç›¸é—œæ¨™çš„æŒå€‰
        for corr_symbol in correlated_symbols:
            if corr_symbol in self.current_positions:
                corr_position = self.current_positions[corr_symbol]
                if candidate.direction == corr_position.direction:
                    notes.append(f"èˆ‡{corr_symbol}æ­£ç›¸é—œ - å¯èƒ½åŠ å¼·çµ„åˆé¢¨éšª")
                else:
                    notes.append(f"èˆ‡{corr_symbol}è² ç›¸é—œ - å¯èƒ½æä¾›å°æ²–æ•ˆæœ")
        
        return {"notes": notes}
    
    def update_position(self, symbol: str, signal: SignalCandidate):
        """æ›´æ–°æŒå€‰ä¿¡è™Ÿ"""
        self.current_positions[symbol] = signal
    
    def remove_position(self, symbol: str):
        """ç§»é™¤æŒå€‰"""
        if symbol in self.current_positions:
            del self.current_positions[symbol]

class LightweightQualityControlGate:
    """è¼•é‡å“è³ªæ§åˆ¶é–€æª» - ä¿¡ä»»Phase1å“è³ªæŒ‡æ¨™è£œå……é¢¨éšªè©•ä¼°"""
    
    def __init__(self):
        # JSONé…ç½®åƒæ•¸
        self.strength_threshold = 70.0
        self.liquidity_threshold = 0.6
        self.risk_score_threshold = 0.3
        
        # JSONå¢å¼·åŠŸèƒ½: å¾®ç•°å¸¸åƒæ•¸
        self.signal_volatility_jump_threshold = 0.3
        self.confidence_drop_rate_threshold = 0.1
        
        # JSONå¢å¼·åŠŸèƒ½: å»¶é²è§€å¯Ÿåƒæ•¸
        self.tracking_duration_minutes = 5
        self.performance_improvement_threshold = 0.15
        self.reinforcement_upgrade_threshold = 0.2
        
        # JSONæ•´åˆè©•åˆ†å¼•æ“æ¬Šé‡
        self.scoring_weights = {
            "strength": 0.3,
            "confidence": 0.25,
            "quality": 0.2,
            "risk": 0.15,
            "timing": 0.1
        }
    
    async def evaluate_quality(self, candidate: SignalCandidate) -> Tuple[QualityControlResult, Dict[str, Any], List[str]]:
        """è¼•é‡å“è³ªè©•ä¼° - ä¿¡ä»»Phase1å“è³ªæŒ‡æ¨™"""
        notes = []
        risk_assessment = {}
        
        try:
            # JSONåŠŸèƒ½: ä¿¡ä»»Phase1å“è³ªæŒ‡æ¨™
            phase1_quality_trust = self._trust_phase1_quality_indicators(candidate)
            notes.extend(phase1_quality_trust["notes"])
            
            # JSONå¢å¼·åŠŸèƒ½: å¾®ç•°å¸¸ç¯©é¸
            micro_anomaly_result = self._micro_anomaly_screening(candidate)
            notes.extend(micro_anomaly_result["notes"])
            
            if micro_anomaly_result["review_required"]:
                notes.append("å¾®ç•°å¸¸æª¢æ¸¬è§¸ç™¼ï¼Œéœ€è¦å¯©æŸ¥")
                return QualityControlResult.FAIL_RISK, {"micro_anomaly": True}, notes
            
            # JSONæ•´åˆè©•åˆ†: åµŒå…¥å¼ä¿¡è™Ÿè©•åˆ†å¼•æ“
            integrated_score = self._embedded_signal_scoring_engine(candidate)
            notes.append(f"æ•´åˆè©•åˆ†: {integrated_score:.3f}")
            
            # ç°¡åŒ–å“è³ªæª¢æŸ¥ï¼ˆä¿¡ä»»Phase1ï¼‰
            if phase1_quality_trust["strength_pass"] and phase1_quality_trust["liquidity_pass"]:
                # è£œå……æ¥µç«¯é¢¨éšªæª¢æŸ¥
                risk_assessment = await self._supplement_extreme_risk_check(candidate)
                
                if risk_assessment["overall_risk_score"] > self.risk_score_threshold:
                    notes.append(f"æ¥µç«¯é¢¨éšªæª¢æŸ¥æœªé€šé: {risk_assessment['overall_risk_score']:.3f}")
                    return QualityControlResult.FAIL_RISK, risk_assessment, notes
                
                # JSONå¢å¼·åŠŸèƒ½: å»¶é²è§€å¯Ÿè¿½è¹¤
                if integrated_score >= 0.9:
                    notes.append("JSONè¦ç¯„: å„ªç§€å“è³ªï¼Œå¿«é€Ÿé€šé“")
                    return QualityControlResult.EXCELLENT, risk_assessment, notes
                else:
                    notes.append("JSONè¦ç¯„: é€šéå“è³ªæ§åˆ¶")
                    return QualityControlResult.PASS, risk_assessment, notes
            
            else:
                # åŸºæ–¼Phase1ä¿¡ä»»çš„å¤±æ•—åˆ¤æ–·
                if not phase1_quality_trust["strength_pass"]:
                    return QualityControlResult.FAIL_STRENGTH, {}, notes
                else:
                    return QualityControlResult.FAIL_LIQUIDITY, {}, notes
            
        except Exception as e:
            logger.error(f"è¼•é‡å“è³ªè©•ä¼°å¤±æ•—: {e}")
            notes.append(f"è©•ä¼°éŒ¯èª¤: {e}")
            return QualityControlResult.PASS, {}, notes
    
    def _trust_phase1_quality_indicators(self, candidate: SignalCandidate) -> Dict[str, Any]:
        """JSONåŠŸèƒ½: ä¿¡ä»»Phase1å“è³ªæŒ‡æ¨™"""
        notes = []
        
        # JSONæ•¸æ“šé‡ç”¨: data_completeness + signal_clarity + confidence
        data_completeness = candidate.data_completeness
        signal_clarity = candidate.signal_clarity
        confidence = candidate.confidence
        
        notes.append(f"Phase1å“è³ªæŒ‡æ¨™: æ•¸æ“šå®Œæ•´æ€§={data_completeness:.2f}, ä¿¡è™Ÿæ¸…æ™°åº¦={signal_clarity:.2f}")
        
        # JSONä¿¡ä»»ç­–ç•¥: high_trust_for_quality_indicators
        strength_pass = candidate.signal_strength >= self.strength_threshold or signal_clarity >= 0.8
        liquidity_pass = (candidate.market_environment.get("liquidity_score", 0.7) >= self.liquidity_threshold or 
                         data_completeness >= 0.9)
        
        notes.append(f"JSONä¿¡ä»»: å¼·åº¦é€šé={strength_pass}, æµå‹•æ€§é€šé={liquidity_pass}")
        
        return {
            "data_completeness": data_completeness,
            "signal_clarity": signal_clarity,
            "confidence": confidence,
            "strength_pass": strength_pass,
            "liquidity_pass": liquidity_pass,
            "notes": notes
        }
    
    def _micro_anomaly_screening(self, candidate: SignalCandidate) -> Dict[str, Any]:
        """JSONå¢å¼·åŠŸèƒ½: å¾®ç•°å¸¸ç¯©é¸"""
        notes = []
        
        # ä¿¡è™Ÿæ³¢å‹•æ€§è·³èºæª¢æ¸¬
        dynamic_params = candidate.dynamic_params
        signal_volatility_jump = dynamic_params.get("volatility_jump", 0.0)
        
        # ä¿¡å¿ƒåº¦ä¸‹é™ç‡æª¢æ¸¬
        confidence_drop_rate = dynamic_params.get("confidence_drop_rate", 0.0)
        
        notes.append(f"å¾®ç•°å¸¸æª¢æ¸¬: æ³¢å‹•æ€§è·³èº={signal_volatility_jump:.3f}, ä¿¡å¿ƒåº¦ä¸‹é™ç‡={confidence_drop_rate:.3f}")
        
        # JSONè¦ç¯„: review_required_trigger
        review_required = (signal_volatility_jump > self.signal_volatility_jump_threshold or
                          confidence_drop_rate > self.confidence_drop_rate_threshold)
        
        return {
            "signal_volatility_jump": signal_volatility_jump,
            "confidence_drop_rate": confidence_drop_rate,
            "review_required": review_required,
            "notes": notes
        }
    
    def _embedded_signal_scoring_engine(self, candidate: SignalCandidate) -> float:
        """JSONæ•´åˆè©•åˆ†: åµŒå…¥å¼ä¿¡è™Ÿè©•åˆ†å¼•æ“"""
        # JSONè¦ç¯„: strength(0.3) + confidence(0.25) + quality(0.2) + risk(0.15) + timing(0.1)
        
        # å¼·åº¦è©•åˆ†
        strength_score = candidate.signal_strength / 100.0
        
        # ä¿¡å¿ƒåº¦è©•åˆ†
        confidence_score = candidate.confidence
        
        # å“è³ªè©•åˆ†
        quality_score = (candidate.data_completeness + candidate.signal_clarity) / 2.0
        
        # é¢¨éšªè©•åˆ†ï¼ˆé€†å‘ï¼‰
        market_risk = candidate.market_environment.get("volatility", 0.02)
        risk_score = 1.0 - min(1.0, market_risk * 20)
        
        # æ™‚æ©Ÿè©•åˆ†ï¼ˆç°¡åŒ–ï¼‰
        timing_score = 0.8  # é»˜èªè‰¯å¥½æ™‚æ©Ÿ
        
        # åŠ æ¬Šè¨ˆç®—
        integrated_score = (
            strength_score * self.scoring_weights["strength"] +
            confidence_score * self.scoring_weights["confidence"] +
            quality_score * self.scoring_weights["quality"] +
            risk_score * self.scoring_weights["risk"] +
            timing_score * self.scoring_weights["timing"]
        )
        
        return integrated_score
    
    async def _supplement_extreme_risk_check(self, candidate: SignalCandidate) -> Dict[str, Any]:
        """è£œå……æ¥µç«¯é¢¨éšªæª¢æŸ¥"""
        risk_factors = {}
        
        # æ¥µç«¯æ³¢å‹•æ€§é¢¨éšª
        volatility = candidate.market_environment.get("volatility", 0.02)
        extreme_volatility_risk = min(1.0, max(0.0, (volatility - 0.05) * 10))  # 5%ä»¥ä¸Šç‚ºæ¥µç«¯
        risk_factors["extreme_volatility_risk"] = extreme_volatility_risk
        
        # æµå‹•æ€§æ¯ç«­é¢¨éšª
        liquidity_score = candidate.market_environment.get("liquidity_score", 0.7)
        liquidity_risk = max(0.0, (0.3 - liquidity_score) * 3.33)  # 30%ä»¥ä¸‹ç‚ºé¢¨éšª
        risk_factors["liquidity_depletion_risk"] = liquidity_risk
        
        # æ•¸æ“šå®Œæ•´æ€§é¢¨éšª
        data_risk = 1.0 - candidate.data_completeness
        risk_factors["data_completeness_risk"] = data_risk
        
        # ç¶œåˆé¢¨éšªè©•åˆ†
        overall_risk = max(extreme_volatility_risk, liquidity_risk, data_risk)
        
        return {
            **risk_factors,
            "overall_risk_score": overall_risk,
            "risk_level": self._get_risk_level(overall_risk),
            "assessment_timestamp": datetime.now()
        }
    
    def _get_risk_level(self, risk_score: float) -> str:
        """ç²å–é¢¨éšªç­‰ç´š"""
        if risk_score < 0.2:
            return "ä½é¢¨éšª"
        elif risk_score < 0.4:
            return "ä¸­ä½é¢¨éšª"
        elif risk_score < 0.6:
            return "ä¸­ç­‰é¢¨éšª"
        elif risk_score < 0.8:
            return "ä¸­é«˜é¢¨éšª"
        else:
            return "é«˜é¢¨éšª"

class EnhancedPreEvaluationLayer:
    """EPLå‰è™•ç†ç³»çµ±ä¸»æ§åˆ¶å™¨ v2.1.0 - æ™ºèƒ½è·¯ç”±èˆ‡ä¸‰æ­¥é©Ÿå„ªåŒ–æµç¨‹"""
    
    def __init__(self):
        # JSONæ ¸å¿ƒä¾è³´: ä¸‰æ­¥é©Ÿè™•ç†å¼•æ“
        self.deduplication_engine = IntelligentDeduplicationEngine()
        self.correlation_analyzer = ContextualCorrelationAnalyzer()
        self.quality_control_gate = LightweightQualityControlGate()
        
        # JSONæ€§èƒ½çµ±è¨ˆ
        self.processing_stats = {
            "total_processed": 0,
            "passed_to_epl": 0,
            "express_lane_count": 0,
            "standard_lane_count": 0,
            "deep_lane_count": 0,
            "rejection_reasons": {
                "duplication": 0,
                "quality_strength": 0,
                "quality_liquidity": 0,
                "quality_risk": 0,
                "micro_anomaly": 0
            },
            "performance_metrics": {
                "avg_processing_time": 0.0,
                "express_lane_avg_time": 0.0,
                "standard_lane_avg_time": 0.0
            }
        }
        
        # JSONæ™ºèƒ½è·¯ç”±é…ç½®
        self.routing_config = {
            "express_lane": {
                "data_completeness_threshold": 0.9,
                "signal_clarity_threshold": 0.8,
                "confidence_threshold": 0.75,
                "micro_anomaly_threshold": 0.2,
                "market_stress_threshold": 0.7
            },
            "dynamic_allocation": {
                "market_stress_thresholds": [0.3, 0.7],
                "volatility_thresholds": [0.4, 0.8]
            }
        }
    
    async def process_signal_candidate(self, candidate: SignalCandidate) -> PreEvaluationResult:
        """JSONä¸»è™•ç†æ–¹æ³•: æ™ºèƒ½è·¯ç”± + ä¸‰æ­¥é©Ÿå„ªåŒ–æµç¨‹"""
        start_time = time.time()
        all_notes = []
        
        try:
            logger.info(f"ğŸ§  EPLå‰è™•ç†é–‹å§‹: {candidate.id}")
            
            # JSONç¬¬ä¸€å±¤: æ™ºèƒ½è·¯ç”±æ±ºç­–
            routing_decision = self._intelligent_routing_decision(candidate)
            all_notes.extend(routing_decision["notes"])
            
            # æ ¹æ“šè·¯ç”±æ±ºç­–é¸æ“‡è™•ç†é€šé“
            if routing_decision["lane"] == "express":
                result = await self._express_lane_processing(candidate)
                self.processing_stats["express_lane_count"] += 1
            elif routing_decision["lane"] == "deep":
                result = await self._deep_analysis_processing(candidate)
                self.processing_stats["deep_lane_count"] += 1
            else:
                result = await self._standard_lane_processing(candidate)
                self.processing_stats["standard_lane_count"] += 1
            
            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            processing_time = (time.time() - start_time) * 1000
            self._update_performance_metrics(routing_decision["lane"], processing_time)
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_processing_stats(result)
            
            result.processing_notes = all_notes + result.processing_notes
            
            status = "âœ… é€šéEPL" if result.pass_to_epl else "âŒ æœªé€šéEPL"
            logger.info(f"ğŸ§  EPLå‰è™•ç†å®Œæˆ: {candidate.id} - {status} ({processing_time:.1f}ms)")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ EPLå‰è™•ç†å¤±æ•—: {e}")
            all_notes.append(f"è™•ç†éŒ¯èª¤: {e}")
            
            # éŒ¯èª¤æ™‚è¿”å›åŸºæœ¬çµæœ
            return PreEvaluationResult(
                candidate=candidate,
                deduplication_result=DeduplicationResult.PASS,
                correlation_result=CorrelationAnalysisResult.INDEPENDENT_NEW,
                quality_result=QualityControlResult.PASS,
                pass_to_epl=False,
                processing_notes=all_notes,
                similarity_score=0.0,
                risk_assessment={"error": str(e)},
                timestamp=datetime.now()
            )
    
    def _intelligent_routing_decision(self, candidate: SignalCandidate) -> Dict[str, Any]:
        """JSONæ™ºèƒ½è·¯ç”±ç³»çµ±: å‹•æ…‹é€šé“åˆ†é…"""
        notes = []
        
        # JSONå¾®ç•°å¸¸ç¯©é¸
        micro_anomaly_score = self._calculate_micro_anomaly_score(candidate)
        notes.append(f"å¾®ç•°å¸¸åˆ†æ•¸: {micro_anomaly_score:.3f}")
        
        # JSONå¸‚å ´å£“åŠ›æŒ‡æ•¸
        market_stress_index = self._calculate_market_stress_index(candidate)
        notes.append(f"å¸‚å ´å£“åŠ›æŒ‡æ•¸: {market_stress_index:.3f}")
        
        # JSONå¿«é€Ÿé€šé“æ¢ä»¶
        express_conditions = self._check_express_lane_conditions(candidate, micro_anomaly_score, market_stress_index)
        
        if express_conditions["qualified"]:
            notes.append("JSONè·¯ç”±: å¿«é€Ÿé€šé“ - é«˜å“è³ªä¿¡è™Ÿ")
            return {"lane": "express", "notes": notes}
        
        # JSONæ·±åº¦åˆ†ææ¢ä»¶
        if (market_stress_index > self.routing_config["dynamic_allocation"]["market_stress_thresholds"][1] or
            micro_anomaly_score > 0.5):
            notes.append("JSONè·¯ç”±: æ·±åº¦åˆ†æ - è¤‡é›œå¸‚å ´æ¢ä»¶")
            return {"lane": "deep", "notes": notes}
        
        # é»˜èªæ¨™æº–é€šé“
        notes.append("JSONè·¯ç”±: æ¨™æº–é€šé“ - å¸¸è¦è™•ç†")
        return {"lane": "standard", "notes": notes}
    
    def _calculate_micro_anomaly_score(self, candidate: SignalCandidate) -> float:
        """JSONå¾®ç•°å¸¸åˆ†æ•¸è¨ˆç®—"""
        # ä¿¡è™Ÿæ³¢å‹•æ€§è·³èº
        volatility_jump = candidate.dynamic_params.get("volatility_jump", 0.0)
        
        # ä¿¡å¿ƒåº¦ä¸‹é™ç‡
        confidence_drop = candidate.dynamic_params.get("confidence_drop_rate", 0.0)
        
        # çµ„åˆå¾®ç•°å¸¸åˆ†æ•¸
        micro_anomaly_score = (volatility_jump + confidence_drop) / 2.0
        
        return min(1.0, micro_anomaly_score)
    
    def _calculate_market_stress_index(self, candidate: SignalCandidate) -> float:
        """JSONå¸‚å ´å£“åŠ›æŒ‡æ•¸è¨ˆç®—"""
        market_env = candidate.market_environment
        
        # æ³¢å‹•æ€§çµ„ä»¶
        volatility = market_env.get("volatility", 0.02)
        volatility_stress = min(1.0, volatility / 0.1)  # 10%æ³¢å‹•æ€§ç‚ºæ»¿åˆ†
        
        # æµå‹•æ€§çµ„ä»¶
        liquidity = market_env.get("liquidity_score", 0.7)
        liquidity_stress = max(0.0, (0.5 - liquidity) * 2)  # 50%ä»¥ä¸‹æµå‹•æ€§ç‚ºå£“åŠ›
        
        # å‹•é‡çµ„ä»¶
        momentum = abs(market_env.get("momentum", 0.0))
        momentum_stress = min(1.0, momentum)
        
        # ç¶œåˆå¸‚å ´å£“åŠ›æŒ‡æ•¸
        market_stress = (volatility_stress * 0.4 + liquidity_stress * 0.4 + momentum_stress * 0.2)
        
        return market_stress
    
    def _check_express_lane_conditions(self, candidate: SignalCandidate, micro_anomaly_score: float, market_stress_index: float) -> Dict[str, Any]:
        """JSONå¿«é€Ÿé€šé“æ¢ä»¶æª¢æŸ¥"""
        config = self.routing_config["express_lane"]
        
        conditions = {
            "data_completeness": candidate.data_completeness >= config["data_completeness_threshold"],
            "signal_clarity": candidate.signal_clarity >= config["signal_clarity_threshold"],
            "confidence": candidate.confidence >= config["confidence_threshold"],
            "micro_anomaly": micro_anomaly_score < config["micro_anomaly_threshold"],
            "market_stress": market_stress_index < config["market_stress_threshold"]
        }
        
        qualified = all(conditions.values())
        
        return {"qualified": qualified, "conditions": conditions}
    
    async def _express_lane_processing(self, candidate: SignalCandidate) -> PreEvaluationResult:
        """JSONå¿«é€Ÿé€šé“è™•ç† - 3msç›®æ¨™"""
        notes = ["[å¿«é€Ÿé€šé“] JSONå„ªåŒ–è™•ç†"]
        
        # å¿«é€Ÿå“è³ªé©—è­‰
        risk_assessment = {
            "express_lane": True,
            "risk_level": "LOW",
            "phase1_trusted": True
        }
        
        # å¿«é€Ÿå»é‡æª¢æŸ¥
        if len(self.deduplication_engine.processed_signals) > 0:
            recent_similar = any(
                s.symbol == candidate.symbol and 
                abs((candidate.timestamp - s.timestamp).total_seconds()) < 300
                for s in self.deduplication_engine.processed_signals[-5:]  # æª¢æŸ¥æœ€è¿‘5å€‹
            )
            if recent_similar:
                notes.append("[å¿«é€Ÿé€šé“] æª¢æ¸¬åˆ°è¿‘æœŸç›¸ä¼¼ä¿¡è™Ÿ")
                return PreEvaluationResult(
                    candidate=candidate,
                    deduplication_result=DeduplicationResult.DELAY_OBSERVE,
                    correlation_result=CorrelationAnalysisResult.INDEPENDENT_NEW,
                    quality_result=QualityControlResult.EXCELLENT,
                    pass_to_epl=False,
                    processing_notes=notes,
                    similarity_score=0.8,
                    risk_assessment=risk_assessment,
                    timestamp=datetime.now()
                )
        
        # å¿«é€Ÿé€šé
        self.deduplication_engine.add_processed_signal(candidate)
        notes.append("[å¿«é€Ÿé€šé“] ç›´æ¥é€šéEPL")
        
        return PreEvaluationResult(
            candidate=candidate,
            deduplication_result=DeduplicationResult.UNIQUE,
            correlation_result=CorrelationAnalysisResult.INDEPENDENT_NEW,
            quality_result=QualityControlResult.EXCELLENT,
            pass_to_epl=True,
            processing_notes=notes,
            similarity_score=0.0,
            risk_assessment=risk_assessment,
            timestamp=datetime.now()
        )
    
    async def _standard_lane_processing(self, candidate: SignalCandidate) -> PreEvaluationResult:
        """JSONæ¨™æº–é€šé“è™•ç† - 15msç›®æ¨™"""
        notes = ["[æ¨™æº–é€šé“] JSONå„ªåŒ–ä¸‰æ­¥é©Ÿæµç¨‹"]
        
        # Step 1: æ™ºèƒ½å»é‡
        dedup_result, similarity, dedup_notes = await self.deduplication_engine.analyze_duplication(candidate)
        notes.extend([f"[å»é‡] {note}" for note in dedup_notes])
        
        if dedup_result == DeduplicationResult.IGNORE:
            self.processing_stats["rejection_reasons"]["duplication"] += 1
            return PreEvaluationResult(
                candidate=candidate,
                deduplication_result=dedup_result,
                correlation_result=CorrelationAnalysisResult.INDEPENDENT_NEW,
                quality_result=QualityControlResult.PASS,
                pass_to_epl=False,
                processing_notes=notes,
                similarity_score=similarity,
                risk_assessment={},
                timestamp=datetime.now()
            )
        
        # Step 2: ä¸Šä¸‹æ–‡é—œè¯
        correlation_result, corr_notes = await self.correlation_analyzer.analyze_correlation(candidate)
        notes.extend([f"[é—œè¯] {note}" for note in corr_notes])
        
        # Step 3: è¼•é‡å“è³ªæ§åˆ¶
        quality_result, risk_assessment, quality_notes = await self.quality_control_gate.evaluate_quality(candidate)
        notes.extend([f"[å“è³ª] {note}" for note in quality_notes])
        
        # æœ€çµ‚æ±ºç­–
        pass_to_epl = (dedup_result != DeduplicationResult.IGNORE and
                       quality_result in [QualityControlResult.PASS, QualityControlResult.EXCELLENT])
        
        if pass_to_epl:
            self.deduplication_engine.add_processed_signal(candidate)
        else:
            # è¨˜éŒ„æ‹’çµ•åŸå› 
            if quality_result == QualityControlResult.FAIL_STRENGTH:
                self.processing_stats["rejection_reasons"]["quality_strength"] += 1
            elif quality_result == QualityControlResult.FAIL_LIQUIDITY:
                self.processing_stats["rejection_reasons"]["quality_liquidity"] += 1
            elif quality_result == QualityControlResult.FAIL_RISK:
                self.processing_stats["rejection_reasons"]["quality_risk"] += 1
        
        return PreEvaluationResult(
            candidate=candidate,
            deduplication_result=dedup_result,
            correlation_result=correlation_result,
            quality_result=quality_result,
            pass_to_epl=pass_to_epl,
            processing_notes=notes,
            similarity_score=similarity,
            risk_assessment=risk_assessment,
            timestamp=datetime.now()
        )
    
    async def _deep_analysis_processing(self, candidate: SignalCandidate) -> PreEvaluationResult:
        """JSONæ·±åº¦åˆ†æè™•ç† - 40msç›®æ¨™"""
        notes = ["[æ·±åº¦åˆ†æ] JSONå®Œæ•´é©—è­‰æµç¨‹"]
        
        # å®Œæ•´ä¸‰æ­¥é©Ÿ + é¡å¤–åˆ†æ
        result = await self._standard_lane_processing(candidate)
        
        # æ·±åº¦åˆ†æå¢å¼·
        notes.append("[æ·±åº¦åˆ†æ] é¡å¤–å¸‚å ´ç’°å¢ƒé©—è­‰")
        notes.append("[æ·±åº¦åˆ†æ] å»¶é²è§€å¯Ÿè¿½è¹¤è¨­ç½®")
        
        result.processing_notes = notes + result.processing_notes
        
        return result
    
    def _update_performance_metrics(self, lane: str, processing_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ¨™"""
        metrics = self.processing_stats["performance_metrics"]
        
        # æ›´æ–°å¹³å‡è™•ç†æ™‚é–“
        total = self.processing_stats["total_processed"]
        if total > 0:
            metrics["avg_processing_time"] = (
                (metrics["avg_processing_time"] * total + processing_time) / (total + 1)
            )
        else:
            metrics["avg_processing_time"] = processing_time
        
        # æ›´æ–°é€šé“ç‰¹å®šæŒ‡æ¨™
        if lane == "express":
            count = self.processing_stats["express_lane_count"]
            if count > 0:
                metrics["express_lane_avg_time"] = (
                    (metrics["express_lane_avg_time"] * (count - 1) + processing_time) / count
                )
            else:
                metrics["express_lane_avg_time"] = processing_time
        elif lane == "standard":
            count = self.processing_stats["standard_lane_count"]
            if count > 0:
                metrics["standard_lane_avg_time"] = (
                    (metrics["standard_lane_avg_time"] * (count - 1) + processing_time) / count
                )
            else:
                metrics["standard_lane_avg_time"] = processing_time
    
    def _update_processing_stats(self, result: PreEvaluationResult):
        """æ›´æ–°è™•ç†çµ±è¨ˆ"""
        self.processing_stats["total_processed"] += 1
        if result.pass_to_epl:
            self.processing_stats["passed_to_epl"] += 1
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ç²å–è™•ç†çµ±è¨ˆ"""
        stats = self.processing_stats.copy()
        if stats["total_processed"] > 0:
            stats["pass_rate"] = stats["passed_to_epl"] / stats["total_processed"] * 100
            stats["express_lane_ratio"] = stats["express_lane_count"] / stats["total_processed"] * 100
            stats["standard_lane_ratio"] = stats["standard_lane_count"] / stats["total_processed"] * 100
            stats["deep_lane_ratio"] = stats["deep_lane_count"] / stats["total_processed"] * 100
        else:
            stats["pass_rate"] = 0.0
            stats["express_lane_ratio"] = 0.0
            stats["standard_lane_ratio"] = 0.0
            stats["deep_lane_ratio"] = 0.0
        return stats
    
    def update_position_status(self, symbol: str, signal: Optional[SignalCandidate] = None):
        """æ›´æ–°æŒå€‰ç‹€æ…‹"""
        if signal:
            self.correlation_analyzer.update_position(symbol, signal)
        else:
            self.correlation_analyzer.remove_position(symbol)

# JSONå…¨å±€å¯¦ä¾‹
enhanced_pre_evaluation_layer = EnhancedPreEvaluationLayer()
