"""
ğŸ§  Phase 2: ä¿¡è™Ÿå‰è™•ç†å±¤ (Pre-Evaluation Layer)
=============================================

EPL å‰è™•ç†ç³»çµ± - ä¸‰æ­¥é©Ÿä¿¡è™Ÿå“è³ªæ§åˆ¶
1. å»é‡åˆ†æå¼•æ“
2. ä¿¡è™Ÿé—œè¯åˆ†æå™¨  
3. å“è³ªæ§åˆ¶é–€æª»
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
import numpy as np
import sys
from pathlib import Path

# æ·»åŠ è·¯å¾‘
current_dir = Path(__file__).parent
sys.path.extend([
    str(current_dir.parent / "shared_core"),
    str(current_dir.parent / "phase1_signal_generation"),
])

from unified_signal_candidate_pool import SignalCandidate, SignalSource

logger = logging.getLogger(__name__)

class DeduplicationResult(Enum):
    """å»é‡åˆ†æçµæœ"""
    UNIQUE = "â­ ç¨ç‰¹"             # å¿«é€Ÿé€šé“ - ç¨ç‰¹ä¿¡è™Ÿ
    IGNORE = "âŒ å¿½ç•¥"           # å®Œå…¨é‡è¤‡ï¼Œå¿½ç•¥
    DELAY_OBSERVE = "âš ï¸ å»¶é²è§€å¯Ÿ"  # å¯èƒ½é‡è¤‡ï¼Œå»¶é²è§€å¯Ÿ
    PASS = "âœ… é€šé"            # ç„¡é‡è¤‡ï¼Œé€šé

class CorrelationAnalysisResult(Enum):
    """é—œè¯åˆ†æçµæœ"""
    STRENGTHEN_CANDIDATE = "â• å¼·åŒ–å€™é¸"  # å¼·åŒ–ç¾æœ‰æŒå€‰
    REPLACE_CANDIDATE = "ğŸ” æ›¿æ›å€™é¸"    # æ›¿æ›ç¾æœ‰æŒå€‰
    INDEPENDENT_NEW = "âœ… ç¨ç«‹æ–°å–®"      # ç¨ç«‹æ–°äº¤æ˜“

class QualityControlResult(Enum):
    """å“è³ªæ§åˆ¶çµæœ"""
    EXCELLENT = "ğŸŒŸ å„ªç§€"              # å¿«é€Ÿé€šé“ - å„ªç§€å“è³ª
    PASS = "âœ… é€šé"
    FAIL_STRENGTH = "âŒ ä¿¡è™Ÿå¼·åº¦ä¸è¶³"
    FAIL_LIQUIDITY = "âŒ æµå‹•æ€§ä¸è¶³"
    FAIL_RISK = "âŒ é¢¨éšªè©•ä¼°æœªé€šé"

@dataclass
class PreEvaluationResult:
    """å‰è™•ç†è©•ä¼°çµæœ"""
    candidate: SignalCandidate
    deduplication_result: DeduplicationResult
    correlation_result: CorrelationAnalysisResult
    quality_result: QualityControlResult
    pass_to_epl: bool                    # æ˜¯å¦é€šéé€²å…¥EPL
    processing_notes: List[str]          # è™•ç†å‚™è¨»
    similarity_score: Optional[float]    # ç›¸ä¼¼åº¦åˆ†æ•¸
    risk_assessment: Dict[str, Any]      # é¢¨éšªè©•ä¼°è©³æƒ…
    timestamp: datetime

class DeduplicationEngine:
    """Step 1: å»é‡åˆ†æå¼•æ“"""
    
    def __init__(self):
        self.processed_signals: List[SignalCandidate] = []
        self.similarity_threshold = 0.85  # 85%ç›¸ä¼¼åº¦é–¾å€¼
        self.time_overlap_minutes = 15    # 15åˆ†é˜æ™‚é–“é‡ç–Š
        self.confidence_diff_threshold = 0.03  # 3%ä¿¡å¿ƒåº¦å·®ç•°
    
    async def analyze_duplication(self, candidate: SignalCandidate) -> Tuple[DeduplicationResult, float, List[str]]:
        """åˆ†æä¿¡è™Ÿé‡è¤‡æ€§"""
        notes = []
        max_similarity = 0.0
        
        try:
            # ç¯©é¸ç›¸é—œçš„æ­·å²ä¿¡è™Ÿ (åŒæ¨™çš„ + æ™‚é–“çª—å£å…§)
            relevant_signals = self._get_relevant_historical_signals(candidate)
            
            if not relevant_signals:
                notes.append("ç„¡æ­·å²ä¿¡è™Ÿæ¯”å°")
                return DeduplicationResult.PASS, 0.0, notes
            
            # é€ä¸€æ¯”è¼ƒç›¸ä¼¼åº¦
            for historical_signal in relevant_signals:
                similarity = self._calculate_signal_similarity(candidate, historical_signal)
                max_similarity = max(max_similarity, similarity)
                
                # æ™‚é–“é‡ç–Šæª¢æ¸¬
                time_overlap = self._check_time_overlap(candidate, historical_signal)
                
                # æŒ‡æ¨™ä¾†æºæ¯”å°
                indicator_similarity = self._compare_indicator_sources(candidate, historical_signal)
                
                # ä¿¡å¿ƒåº¦å·®ç•°åˆ†æ
                confidence_diff = abs(candidate.confidence - historical_signal.confidence)
                
                notes.append(f"èˆ‡{historical_signal.id}ç›¸ä¼¼åº¦: {similarity:.2f}")
                
                # åˆ¤æ–·é‡è¤‡æ€§
                if (time_overlap and 
                    indicator_similarity > self.similarity_threshold and 
                    confidence_diff < self.confidence_diff_threshold):
                    
                    if similarity > 0.95:  # é«˜åº¦ç›¸ä¼¼
                        notes.append("é«˜åº¦é‡è¤‡ä¿¡è™Ÿï¼Œå»ºè­°å¿½ç•¥")
                        return DeduplicationResult.IGNORE, similarity, notes
                    elif similarity > 0.80:  # ä¸­åº¦ç›¸ä¼¼
                        notes.append("ä¸­åº¦é‡è¤‡ä¿¡è™Ÿï¼Œå»¶é²è§€å¯Ÿ")
                        return DeduplicationResult.DELAY_OBSERVE, similarity, notes
            
            notes.append(f"æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.2f}")
            return DeduplicationResult.PASS, max_similarity, notes
            
        except Exception as e:
            logger.error(f"å»é‡åˆ†æå¤±æ•—: {e}")
            notes.append(f"åˆ†æéŒ¯èª¤: {e}")
            return DeduplicationResult.PASS, 0.0, notes  # éŒ¯èª¤æ™‚é è¨­é€šé
    
    def _get_relevant_historical_signals(self, candidate: SignalCandidate) -> List[SignalCandidate]:
        """ç²å–ç›¸é—œæ­·å²ä¿¡è™Ÿ"""
        cutoff_time = candidate.timestamp - timedelta(minutes=self.time_overlap_minutes)
        
        return [
            signal for signal in self.processed_signals
            if (signal.symbol == candidate.symbol and 
                signal.timestamp > cutoff_time and
                signal.timestamp < candidate.timestamp)
        ]
    
    def _calculate_signal_similarity(self, candidate1: SignalCandidate, candidate2: SignalCandidate) -> float:
        """è¨ˆç®—ä¿¡è™Ÿç›¸ä¼¼åº¦"""
        similarity_factors = []
        
        # 1. å¼·åº¦ç›¸ä¼¼åº¦
        strength_similarity = 1 - abs(candidate1.signal_strength - candidate2.signal_strength) / 100
        similarity_factors.append(strength_similarity * 0.3)
        
        # 2. æ–¹å‘ç›¸ä¼¼åº¦
        direction_similarity = 1.0 if candidate1.direction == candidate2.direction else 0.0
        similarity_factors.append(direction_similarity * 0.2)
        
        # 3. ä¾†æºç›¸ä¼¼åº¦
        source_similarity = 1.0 if candidate1.source == candidate2.source else 0.5
        similarity_factors.append(source_similarity * 0.2)
        
        # 4. æŠ€è¡“æŒ‡æ¨™ç›¸ä¼¼åº¦
        tech_similarity = self._calculate_technical_similarity(
            candidate1.technical_snapshot, candidate2.technical_snapshot
        )
        similarity_factors.append(tech_similarity * 0.3)
        
        return sum(similarity_factors)
    
    def _calculate_technical_similarity(self, tech1, tech2) -> float:
        """è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ç›¸ä¼¼åº¦"""
        try:
            similarities = []
            
            # RSIç›¸ä¼¼åº¦
            rsi_sim = 1 - abs(tech1.rsi - tech2.rsi) / 100
            similarities.append(rsi_sim)
            
            # MACDç›¸ä¼¼åº¦
            macd_sim = 1 - min(1.0, abs(tech1.macd_signal - tech2.macd_signal) / 2)
            similarities.append(macd_sim)
            
            # å¸ƒæ—å¸¶ä½ç½®ç›¸ä¼¼åº¦
            bb_sim = 1 - abs(tech1.bollinger_position - tech2.bollinger_position)
            similarities.append(bb_sim)
            
            return np.mean(similarities)
            
        except Exception:
            return 0.5  # é è¨­ä¸­ç­‰ç›¸ä¼¼åº¦
    
    def _check_time_overlap(self, candidate1: SignalCandidate, candidate2: SignalCandidate) -> bool:
        """æª¢æŸ¥æ™‚é–“é‡ç–Š"""
        time_diff = abs((candidate1.timestamp - candidate2.timestamp).total_seconds() / 60)
        return time_diff < self.time_overlap_minutes
    
    def _compare_indicator_sources(self, candidate1: SignalCandidate, candidate2: SignalCandidate) -> float:
        """æ¯”è¼ƒæŒ‡æ¨™ä¾†æºç›¸ä¼¼åº¦"""
        # ç°¡åŒ–å¯¦ç¾ - æ¯”è¼ƒä¾†æºé¡å‹
        if candidate1.source == candidate2.source:
            return 1.0
        elif candidate1.source in [SignalSource.PHASE1ABC_DYNAMIC] and candidate2.source in [SignalSource.PHASE1ABC_DYNAMIC]:
            return 0.8
        else:
            return 0.3
    
    def add_processed_signal(self, signal: SignalCandidate):
        """æ·»åŠ å·²è™•ç†ä¿¡è™Ÿåˆ°æ­·å²è¨˜éŒ„"""
        self.processed_signals.append(signal)
        
        # ä¿æŒæ­·å²è¨˜éŒ„åœ¨åˆç†ç¯„åœå…§ (æœ€è¿‘1å°æ™‚)
        cutoff_time = datetime.now() - timedelta(hours=1)
        self.processed_signals = [s for s in self.processed_signals if s.timestamp > cutoff_time]

class CorrelationAnalyzer:
    """Step 2: ä¿¡è™Ÿé—œè¯åˆ†æå™¨"""
    
    def __init__(self):
        self.current_positions: Dict[str, SignalCandidate] = {}  # ç•¶å‰æŒå€‰ä¿¡è™Ÿ
        self.symbol_correlations: Dict[str, List[str]] = {}      # æ¨™çš„ç›¸é—œæ€§æ˜ å°„
    
    async def analyze_correlation(self, candidate: SignalCandidate) -> Tuple[CorrelationAnalysisResult, List[str]]:
        """åˆ†æä¿¡è™Ÿé—œè¯æ€§"""
        notes = []
        
        try:
            # æ¨™çš„ç›¸é—œæ€§æª¢æ¸¬
            correlated_symbols = self._get_correlated_symbols(candidate.symbol)
            notes.append(f"ç›¸é—œæ¨™çš„: {correlated_symbols}")
            
            # æª¢æŸ¥ç•¶å‰æŒå€‰
            current_position = self.current_positions.get(candidate.symbol)
            
            if current_position:
                # æ–¹å‘è¡çªåˆ†æ
                direction_conflict = self._analyze_direction_conflict(candidate, current_position)
                
                # æŒå€‰é—œè¯è©•ä¼°
                position_strength_diff = candidate.signal_strength - current_position.signal_strength
                confidence_diff = candidate.confidence - current_position.confidence
                
                notes.append(f"ç•¶å‰æŒå€‰å¼·åº¦å·®: {position_strength_diff:.1f}")
                notes.append(f"ä¿¡å¿ƒåº¦å·®ç•°: {confidence_diff:.3f}")
                
                # æ±ºç­–é‚è¼¯
                if direction_conflict and confidence_diff > 0.15:  # æ–¹å‘ç›¸åä¸”ä¿¡å¿ƒåº¦å¤§å¹…æå‡
                    notes.append("å»ºè­°æ›¿æ›ï¼šæ–¹å‘ç›¸åä¸”ä¿¡å¿ƒåº¦å¤§å¹…æå‡")
                    return CorrelationAnalysisResult.REPLACE_CANDIDATE, notes
                
                elif not direction_conflict and confidence_diff > 0.08:  # æ–¹å‘ç›¸åŒä¸”ä¿¡å¿ƒåº¦æå‡
                    notes.append("å»ºè­°å¼·åŒ–ï¼šæ–¹å‘ç›¸åŒä¸”ä¿¡å¿ƒåº¦æå‡")
                    return CorrelationAnalysisResult.STRENGTHEN_CANDIDATE, notes
                
                else:
                    notes.append("æŒå€‰é—œè¯åº¦ä½ï¼Œè¦–ç‚ºç¨ç«‹æ©Ÿæœƒ")
                    return CorrelationAnalysisResult.INDEPENDENT_NEW, notes
            
            # æª¢æŸ¥ç›¸é—œæ¨™çš„æŒå€‰
            for corr_symbol in correlated_symbols:
                if corr_symbol in self.current_positions:
                    corr_position = self.current_positions[corr_symbol]
                    correlation_impact = self._assess_correlation_impact(candidate, corr_position)
                    notes.append(f"èˆ‡{corr_symbol}æŒå€‰ç›¸é—œæ€§: {correlation_impact}")
            
            notes.append("ç„¡ç›´æ¥æŒå€‰è¡çªï¼Œè¦–ç‚ºç¨ç«‹æ–°æ©Ÿæœƒ")
            return CorrelationAnalysisResult.INDEPENDENT_NEW, notes
            
        except Exception as e:
            logger.error(f"é—œè¯åˆ†æå¤±æ•—: {e}")
            notes.append(f"åˆ†æéŒ¯èª¤: {e}")
            return CorrelationAnalysisResult.INDEPENDENT_NEW, notes
    
    def _get_correlated_symbols(self, symbol: str) -> List[str]:
        """ç²å–ç›¸é—œæ¨™çš„"""
        # é å®šç¾©çš„åŠ å¯†è²¨å¹£ç›¸é—œæ€§
        correlation_groups = {
            "BTCUSDT": ["ETHUSDT", "ADAUSDT"],
            "ETHUSDT": ["BTCUSDT", "DOTUSDT", "LINKUSDT"],
            "ADAUSDT": ["DOTUSDT", "ATOMUSDT"],
            "SOLUSDT": ["AVAXUSDT", "NEARUSDT"]
        }
        
        return correlation_groups.get(symbol, [])
    
    def _analyze_direction_conflict(self, candidate: SignalCandidate, current_position: SignalCandidate) -> bool:
        """åˆ†ææ–¹å‘è¡çª"""
        return candidate.direction != current_position.direction
    
    def _assess_correlation_impact(self, candidate: SignalCandidate, correlated_position: SignalCandidate) -> str:
        """è©•ä¼°ç›¸é—œæ€§å½±éŸ¿"""
        if candidate.direction == correlated_position.direction:
            return "æ­£ç›¸é—œ - å¯èƒ½åŠ å¼·çµ„åˆé¢¨éšª"
        else:
            return "è² ç›¸é—œ - å¯èƒ½æä¾›å°æ²–æ•ˆæœ"
    
    def update_position(self, symbol: str, signal: SignalCandidate):
        """æ›´æ–°æŒå€‰ä¿¡è™Ÿ"""
        self.current_positions[symbol] = signal
    
    def remove_position(self, symbol: str):
        """ç§»é™¤æŒå€‰"""
        if symbol in self.current_positions:
            del self.current_positions[symbol]

class QualityControlGate:
    """Step 3: å“è³ªæ§åˆ¶é–€æª»"""
    
    def __init__(self):
        self.strength_threshold = 70.0      # ä¿¡è™Ÿå¼·åº¦é–¾å€¼
        self.liquidity_threshold = 0.6      # æµå‹•æ€§é–¾å€¼
        self.risk_score_threshold = 0.3     # é¢¨éšªè©•åˆ†é–¾å€¼
    
    async def evaluate_quality(self, candidate: SignalCandidate) -> Tuple[QualityControlResult, Dict[str, Any], List[str]]:
        """è©•ä¼°ä¿¡è™Ÿå“è³ª"""
        notes = []
        risk_assessment = {}
        
        try:
            # 1. ä¿¡è™Ÿå¼·åº¦ç¯©é¸
            if candidate.signal_strength < self.strength_threshold:
                notes.append(f"ä¿¡è™Ÿå¼·åº¦ {candidate.signal_strength:.1f} < é–¾å€¼ {self.strength_threshold}")
                return QualityControlResult.FAIL_STRENGTH, {}, notes
            
            # 2. å¸‚å ´æµå‹•æ€§é©—è­‰
            liquidity_score = candidate.market_environment.liquidity_score
            if liquidity_score < self.liquidity_threshold:
                notes.append(f"æµå‹•æ€§ {liquidity_score:.2f} < é–¾å€¼ {self.liquidity_threshold}")
                return QualityControlResult.FAIL_LIQUIDITY, {}, notes
            
            # 3. é¢¨éšªè©•ä¼°
            risk_assessment = await self._comprehensive_risk_assessment(candidate)
            
            if risk_assessment["overall_risk_score"] > self.risk_score_threshold:
                notes.append(f"é¢¨éšªè©•åˆ†éé«˜: {risk_assessment['overall_risk_score']:.3f}")
                return QualityControlResult.FAIL_RISK, risk_assessment, notes
            
            # é€šéæ‰€æœ‰æª¢æŸ¥
            notes.append("âœ… é€šéæ‰€æœ‰å“è³ªæ§åˆ¶æª¢æŸ¥")
            notes.append(f"å¼·åº¦: {candidate.signal_strength:.1f}, æµå‹•æ€§: {liquidity_score:.2f}, é¢¨éšª: {risk_assessment['overall_risk_score']:.3f}")
            
            return QualityControlResult.PASS, risk_assessment, notes
            
        except Exception as e:
            logger.error(f"å“è³ªè©•ä¼°å¤±æ•—: {e}")
            notes.append(f"è©•ä¼°éŒ¯èª¤: {e}")
            return QualityControlResult.PASS, {}, notes  # éŒ¯èª¤æ™‚é è¨­é€šé
    
    async def _comprehensive_risk_assessment(self, candidate: SignalCandidate) -> Dict[str, Any]:
        """ç¶œåˆé¢¨éšªè©•ä¼°"""
        risk_factors = {}
        
        # 1. æ³¢å‹•æ€§é¢¨éšª
        volatility = candidate.market_environment.volatility
        volatility_risk = min(1.0, volatility * 20)  # æ­£è¦åŒ–åˆ°0-1
        risk_factors["volatility_risk"] = volatility_risk
        
        # 2. æµå‹•æ€§é¢¨éšª
        liquidity_risk = 1.0 - candidate.market_environment.liquidity_score
        risk_factors["liquidity_risk"] = liquidity_risk
        
        # 3. æŠ€è¡“æŒ‡æ¨™é¢¨éšª (æ¥µç«¯å€¼é¢¨éšª)
        tech_risk = 0.0
        if candidate.technical_snapshot.rsi > 80 or candidate.technical_snapshot.rsi < 20:
            tech_risk += 0.3
        if abs(candidate.technical_snapshot.williams_r) > 80:
            tech_risk += 0.2
        risk_factors["technical_risk"] = min(1.0, tech_risk)
        
        # 4. å¸‚å ´ç’°å¢ƒé¢¨éšª
        momentum = abs(candidate.market_environment.momentum)
        momentum_risk = min(1.0, momentum * 2)
        risk_factors["momentum_risk"] = momentum_risk
        
        # 5. æ•¸æ“šå®Œæ•´æ€§é¢¨éšª
        data_risk = 1.0 - candidate.data_completeness
        risk_factors["data_completeness_risk"] = data_risk
        
        # ç¶œåˆé¢¨éšªè©•åˆ† (åŠ æ¬Šå¹³å‡)
        weights = {
            "volatility_risk": 0.3,
            "liquidity_risk": 0.25,
            "technical_risk": 0.2,
            "momentum_risk": 0.15,
            "data_completeness_risk": 0.1
        }
        
        overall_risk = sum(risk_factors[factor] * weights[factor] for factor in risk_factors)
        
        return {
            **risk_factors,
            "overall_risk_score": overall_risk,
            "risk_level": self._get_risk_level(overall_risk),
            "assessment_timestamp": datetime.now()
        }
    
    def _get_risk_level(self, risk_score: float) -> str:
        """ç²å–é¢¨éšªç­‰ç´š"""
        if risk_score < 0.2:
            return "ä½é¢¨éšª"
        elif risk_score < 0.4:
            return "ä¸­ä½é¢¨éšª"
        elif risk_score < 0.6:
            return "ä¸­ç­‰é¢¨éšª"
        elif risk_score < 0.8:
            return "ä¸­é«˜é¢¨éšª"
        else:
            return "é«˜é¢¨éšª"

class PreEvaluationLayer:
    """EPL å‰è™•ç†ç³»çµ±ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.deduplication_engine = DeduplicationEngine()
        self.correlation_analyzer = CorrelationAnalyzer()
        self.quality_control_gate = QualityControlGate()
        
        self.processing_stats = {
            "total_processed": 0,
            "passed_to_epl": 0,
            "rejection_reasons": {
                "duplication": 0,
                "quality_strength": 0,
                "quality_liquidity": 0,
                "quality_risk": 0
            }
        }
    
    async def process_signal_candidate(self, candidate: SignalCandidate) -> PreEvaluationResult:
        """è™•ç†ä¿¡è™Ÿå€™é¸è€… - ä¸‰æ­¥é©Ÿæµç¨‹ (Phase1æ•¸æ“šå„ªåŒ–ç‰ˆ)"""
        all_notes = []
        
        try:
            logger.info(f"ğŸ§  EPLå‰è™•ç†é–‹å§‹: {candidate.id}")
            
            # ğŸš€ æ™ºèƒ½å¿«é€Ÿé€šé“åˆ¤æ–·
            if self._is_high_quality_from_phase1(candidate):
                logger.info(f"âš¡ é«˜å“è³ªä¿¡è™Ÿå¿«é€Ÿé€šé“: {candidate.id}")
                return self._express_lane_processing(candidate)
            
            # Step 1: ç²¾ç°¡å»é‡åˆ†æ (åˆ©ç”¨Phase1 dynamic_params)
            dedup_result, similarity, dedup_notes = await self.deduplication_engine.analyze_duplication(candidate)
            all_notes.extend([f"[å»é‡] {note}" for note in dedup_notes])
            
            # å¦‚æœé‡è¤‡æ€§å¤ªé«˜ï¼Œç›´æ¥æ‹’çµ•
            if dedup_result == DeduplicationResult.IGNORE:
                self.processing_stats["rejection_reasons"]["duplication"] += 1
                result = PreEvaluationResult(
                    candidate=candidate,
                    deduplication_result=dedup_result,
                    correlation_result=CorrelationAnalysisResult.INDEPENDENT_NEW,  # é è¨­å€¼
                    quality_result=QualityControlResult.PASS,  # é è¨­å€¼
                    pass_to_epl=False,
                    processing_notes=all_notes,
                    similarity_score=similarity,
                    risk_assessment={},
                    timestamp=datetime.now()
                )
                self._update_processing_stats(result)
                return result
            
            # Step 2: ä¿¡è™Ÿé—œè¯åˆ†æ (åˆ©ç”¨Phase1 market_environment)
            correlation_result, corr_notes = await self.correlation_analyzer.analyze_correlation(candidate)
            all_notes.extend([f"[é—œè¯] {note}" for note in corr_notes])
            
            # Step 3: ç°¡åŒ–å“è³ªæ§åˆ¶ (ä¿¡ä»»Phase1å“è³ªæŒ‡æ¨™)
            quality_result, risk_assessment, quality_notes = await self.quality_control_gate.evaluate_quality(candidate)
            all_notes.extend([f"[å“è³ª] {note}" for note in quality_notes])
            
            # åˆ¤æ–·æ˜¯å¦é€šéé€²å…¥EPL
            pass_to_epl = (
                dedup_result != DeduplicationResult.IGNORE and
                quality_result == QualityControlResult.PASS
            )
            
            # è¨˜éŒ„æ‹’çµ•åŸå› 
            if not pass_to_epl:
                if quality_result == QualityControlResult.FAIL_STRENGTH:
                    self.processing_stats["rejection_reasons"]["quality_strength"] += 1
                elif quality_result == QualityControlResult.FAIL_LIQUIDITY:
                    self.processing_stats["rejection_reasons"]["quality_liquidity"] += 1
                elif quality_result == QualityControlResult.FAIL_RISK:
                    self.processing_stats["rejection_reasons"]["quality_risk"] += 1
            
            # å¦‚æœé€šéï¼Œæ·»åŠ åˆ°å·²è™•ç†ä¿¡è™Ÿè¨˜éŒ„
            if pass_to_epl:
                self.deduplication_engine.add_processed_signal(candidate)
                self.processing_stats["passed_to_epl"] += 1
            
            result = PreEvaluationResult(
                candidate=candidate,
                deduplication_result=dedup_result,
                correlation_result=correlation_result,
                quality_result=quality_result,
                pass_to_epl=pass_to_epl,
                processing_notes=all_notes,
                similarity_score=similarity,
                risk_assessment=risk_assessment,
                timestamp=datetime.now()
            )
            
            self._update_processing_stats(result)
            
            status = "âœ… é€šéEPL" if pass_to_epl else "âŒ æœªé€šéEPL"
            logger.info(f"ğŸ§  EPLå‰è™•ç†å®Œæˆ: {candidate.id} - {status}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ EPLå‰è™•ç†å¤±æ•—: {e}")
            all_notes.append(f"è™•ç†éŒ¯èª¤: {e}")
            
            # éŒ¯èª¤æ™‚è¿”å›åŸºæœ¬çµæœ
            return PreEvaluationResult(
                candidate=candidate,
                deduplication_result=DeduplicationResult.PASS,
                correlation_result=CorrelationAnalysisResult.INDEPENDENT_NEW,
                quality_result=QualityControlResult.PASS,
                pass_to_epl=False,
                processing_notes=all_notes,
                similarity_score=0.0,
                risk_assessment={},
                timestamp=datetime.now()
            )
    
    def _update_processing_stats(self, result: PreEvaluationResult):
        """æ›´æ–°è™•ç†çµ±è¨ˆ"""
        self.processing_stats["total_processed"] += 1
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ç²å–è™•ç†çµ±è¨ˆ"""
        stats = self.processing_stats.copy()
        if stats["total_processed"] > 0:
            stats["pass_rate"] = stats["passed_to_epl"] / stats["total_processed"] * 100
        else:
            stats["pass_rate"] = 0.0
        return stats
    
    def update_position_status(self, symbol: str, signal: Optional[SignalCandidate] = None):
        """æ›´æ–°æŒå€‰ç‹€æ…‹"""
        if signal:
            self.correlation_analyzer.update_position(symbol, signal)
        else:
            self.correlation_analyzer.remove_position(symbol)
    
    def _is_high_quality_from_phase1(self, candidate: SignalCandidate) -> bool:
        """åŸºæ–¼Phase1æ•¸æ“šåˆ¤æ–·æ˜¯å¦ç‚ºé«˜å“è³ªä¿¡è™Ÿ (å¿«é€Ÿé€šé“æ¢ä»¶)"""
        try:
            # Phase1å“è³ªæŒ‡æ¨™æª¢æŸ¥
            data_quality = candidate.data_completeness >= 0.9
            signal_clarity = candidate.signal_clarity >= 0.8
            confidence_high = candidate.confidence >= 0.75
            
            # æŠ€è¡“æŒ‡æ¨™å®Œæ•´æ€§æª¢æŸ¥
            tech_snapshot = candidate.technical_snapshot
            has_strong_indicators = (
                tech_snapshot and
                hasattr(tech_snapshot, 'rsi') and
                hasattr(tech_snapshot, 'macd_signal') and
                hasattr(tech_snapshot, 'bb_position')
            )
            
            # å¸‚å ´ç’°å¢ƒç©©å®šæ€§
            market_env = candidate.market_environment
            stable_market = (
                market_env and
                hasattr(market_env, 'volatility') and
                market_env.volatility < 0.08  # ä½æ³¢å‹•
            )
            
            # ä¿¡è™Ÿå¼·åº¦é–¾å€¼
            strong_signal = candidate.signal_strength >= 75
            
            # ç¶œåˆåˆ¤æ–·
            return (data_quality and signal_clarity and confidence_high and 
                   has_strong_indicators and stable_market and strong_signal)
                   
        except Exception as e:
            logger.warning(f"âš ï¸ å¿«é€Ÿé€šé“æª¢æŸ¥å¤±æ•—: {e}")
            return False
    
    def _express_lane_processing(self, candidate: SignalCandidate) -> PreEvaluationResult:
        """å¿«é€Ÿé€šé“è™•ç† - ç›´æ¥é€šéé«˜å“è³ªä¿¡è™Ÿ"""
        logger.info(f"ğŸš€ å¿«é€Ÿé€šé“è™•ç†: {candidate.id}")
        
        # å¿«é€Ÿé¢¨éšªè©•ä¼°
        risk_assessment = {
            "overall_risk": "LOW",
            "risk_score": 0.15,
            "phase1_quality_verified": True,
            "express_lane": True,
            "confidence_boost": 0.05  # çµ¦äºˆé¡å¤–ä¿¡å¿ƒåŠ æˆ
        }
        
        result = PreEvaluationResult(
            candidate=candidate,
            deduplication_result=DeduplicationResult.UNIQUE,  # ä¿¡ä»»é«˜å“è³ªä¿¡è™Ÿ
            correlation_result=CorrelationAnalysisResult.INDEPENDENT_NEW,
            quality_result=QualityControlResult.EXCELLENT,
            pass_to_epl=True,
            processing_notes=[
                "[å¿«é€Ÿé€šé“] Phase1å“è³ªé©—è­‰é€šé",
                "[å¿«é€Ÿé€šé“] æ•¸æ“šå®Œæ•´æ€§å„ªç§€",
                "[å¿«é€Ÿé€šé“] ä¿¡è™Ÿæ¸…æ™°åº¦é«˜",
                "[å¿«é€Ÿé€šé“] ç›´æ¥æ¨é€²EPLæ±ºç­–å±¤"
            ],
            similarity_score=0.0,  # å‡è¨­ç¨ç‰¹
            risk_assessment=risk_assessment,
            timestamp=datetime.now()
        )
        
        # æ›´æ–°çµ±è¨ˆ
        self.processing_stats["total_processed"] += 1
        self.processing_stats["passed_to_epl"] += 1
        self.processing_stats["express_lane_count"] = self.processing_stats.get("express_lane_count", 0) + 1
        
        # æ·»åŠ åˆ°å·²è™•ç†ä¿¡è™Ÿè¨˜éŒ„
        self.deduplication_engine.add_processed_signal(candidate)
        
        return result

# å…¨å±€å‰è™•ç†å±¤å¯¦ä¾‹
pre_evaluation_layer = PreEvaluationLayer()
