#!/usr/bin/env python3
"""
ğŸ›¡ï¸ Trading X - JSONé…ç½®å®‰å…¨ç®¡ç†å™¨ (ç”Ÿç”¢ç‰ˆæœ¬)
ç¢ºä¿åŸå§‹é…ç½®æ°¸ä¸è¢«ç ´å£çš„å®‰å…¨æ©Ÿåˆ¶

å¯¦æ–½éšæ®µ1ï¼šç«‹å³éƒ¨ç½²çš„å®‰å…¨æ©Ÿåˆ¶
"""

import json
import shutil
import logging
import asyncio
from pathlib import Path
import datetime
from typing import Dict, Any, Optional
import hashlib

logger = logging.getLogger(__name__)

class Phase1AConfigSafetyManager:
    """
    Phase1Aé…ç½®å®‰å…¨ç®¡ç†å™¨ - ç”Ÿç”¢ç‰ˆæœ¬
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. åŸå§‹é…ç½®æ°¸ä¹…ä¿è­·
    2. è‡ªå‹•å‚™ä»½èˆ‡æ¢å¾©
    3. å®Œæ•´æ€§é©—è­‰
    4. å®‰å…¨çš„åƒæ•¸æ›´æ–°
    5. å‚™ä»½æ¸…ç†æ©Ÿåˆ¶
    """
    
    def __init__(self, config_path: str):
        """åˆå§‹åŒ–å®‰å…¨ç®¡ç†å™¨"""
        self.config_path = Path(config_path)
        self.original_backup_dir = Path("safety_backups/original")
        self.working_backup_dir = Path("safety_backups/working")
        self.original_backup_path = self.original_backup_dir / "phase1a_basic_signal_generation_ORIGINAL.json"
        
        # å‚™ä»½æ¸…ç†é…ç½®
        self.backup_retention = {
            "max_backups": 10,      # æœ€å¤šä¿ç•™10å€‹å·¥ä½œå‚™ä»½
            "max_age_days": 7,      # è¶…é7å¤©çš„å‚™ä»½æœƒè¢«æ¸…ç†
            "cleanup_on_deploy": True  # éƒ¨ç½²æ™‚è‡ªå‹•æ¸…ç†
        }
        
        # å®‰å…¨ç‹€æ…‹è¿½è¸ª
        self.safety_state = {
            "is_deployed": False,
            "original_hash": None,
            "modification_count": 0,
            "last_verification": None,
            "integrity_confirmed": False
        }
    
    async def deploy_safety_system(self) -> Dict[str, Any]:
        """éƒ¨ç½²å®‰å…¨ç³»çµ±"""
        try:
            logger.info("ğŸš€ éƒ¨ç½²Phase1Aå®‰å…¨ç³»çµ±...")
            
            deployment_actions = []
            
            # Step 1: å‰µå»ºå®‰å…¨å‚™ä»½ç›®éŒ„
            self.original_backup_dir.mkdir(parents=True, exist_ok=True)
            self.working_backup_dir.mkdir(parents=True, exist_ok=True)
            deployment_actions.append("created_directories")
            
            # Step 2: å‰µå»ºåŸå§‹é…ç½®çš„æ°¸ä¹…å‚™ä»½
            if not self.original_backup_path.exists():
                shutil.copy2(self.config_path, self.original_backup_path)
                deployment_actions.append("created_original_backup")
                logger.info(f"âœ… åŸå§‹é…ç½®å·²å®‰å…¨å‚™ä»½: {self.original_backup_path}")
            
            # Step 3: è¨ˆç®—åŸå§‹é…ç½®æŒ‡ç´‹
            self.safety_state["original_hash"] = await self._calculate_file_hash(self.config_path)
            deployment_actions.append("calculated_hash")
            
            # Step 4: é©—è­‰é…ç½®å®Œæ•´æ€§
            integrity_check = await self._verify_json_integrity()
            if not integrity_check["valid"]:
                raise Exception(f"é…ç½®å®Œæ•´æ€§æª¢æŸ¥å¤±æ•—: {integrity_check['error']}")
            deployment_actions.append("verified_integrity")
            
            # Step 5: åˆå§‹å·¥ä½œå‚™ä»½
            await self._create_safety_backup("deployment_initial")
            deployment_actions.append("created_initial_backup")
            
            # Step 6: æ¸…ç†èˆŠå‚™ä»½ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.backup_retention["cleanup_on_deploy"]:
                cleanup_result = await self._cleanup_old_backups()
                if cleanup_result["cleaned_count"] > 0:
                    deployment_actions.append(f"cleaned_{cleanup_result['cleaned_count']}_old_backups")
            
            # æ¨™è¨˜ç‚ºå·²éƒ¨ç½²
            self.safety_state["is_deployed"] = True
            self.safety_state["last_verification"] = datetime.datetime.now().isoformat()
            self.safety_state["integrity_confirmed"] = True
            
            # è¿”å›éƒ¨ç½²çµæœ
            config_stats = await self._get_config_stats()
            
            return {
                "status": "success",
                "message": "Phase1Aå®‰å…¨ç³»çµ±éƒ¨ç½²å®Œæˆ",
                "actions_performed": deployment_actions,
                "original_config_fingerprint": self.safety_state["original_hash"][:12],
                "config_size": f"{config_stats['size']:.2f}KB",
                "deployment_timestamp": self.safety_state["last_verification"]
            }
            
        except Exception as e:
            logger.error(f"âŒ å®‰å…¨ç³»çµ±éƒ¨ç½²å¤±æ•—: {e}")
            return {
                "status": "error",
                "message": f"éƒ¨ç½²å¤±æ•—: {str(e)}",
                "actions_performed": deployment_actions
            }
    
    async def safe_parameter_update(self, parameter_updates: Dict[str, Any], 
                                   backup_label: str = None) -> Dict[str, Any]:
        """å®‰å…¨çš„åƒæ•¸æ›´æ–°"""
        try:
            if not self.safety_state["is_deployed"]:
                raise Exception("å®‰å…¨ç³»çµ±å°šæœªéƒ¨ç½²ï¼Œè«‹å…ˆåŸ·è¡Œ deploy_safety_system()")
            
            logger.info(f"ğŸ”§ é–‹å§‹å®‰å…¨åƒæ•¸æ›´æ–°: {len(parameter_updates)} å€‹åƒæ•¸")
            
            # å¢åŠ ä¿®æ”¹è¨ˆæ•¸å™¨
            self.safety_state["modification_count"] += 1
            
            update_result = {
                "status": "success",
                "updated_parameters": [],
                "locations_updated": {},
                "modification_count": self.safety_state["modification_count"]
            }
            
            # Step 3: å‰µå»ºä¿®æ”¹å‰å‚™ä»½
            backup_result = await self._create_safety_backup(f"before_update_{self.safety_state['modification_count']}")
            update_result["backup_created"] = backup_result["success"]
            
            # Step 3.1: é©æ™‚æ¸…ç†èˆŠå‚™ä»½
            if self.safety_state["modification_count"] % 5 == 0:  # æ¯5æ¬¡ä¿®æ”¹æ¸…ç†ä¸€æ¬¡
                cleanup_result = await self._cleanup_old_backups()
                if cleanup_result["cleaned_count"] > 0:
                    update_result["backups_cleaned"] = cleanup_result["cleaned_count"]
            
            # Step 4: è¼‰å…¥é…ç½®
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # Step 5: å®‰å…¨æ›´æ–°åƒæ•¸
            for param_name, new_value in parameter_updates.items():
                logger.info(f"  æ›´æ–°åƒæ•¸: {param_name} = {new_value}")
                locations = await self._update_parameter_recursive(config, param_name, new_value)
                update_result["locations_updated"][param_name] = locations
                update_result["updated_parameters"].append({
                    "name": param_name,
                    "value": new_value,
                    "locations_count": len(locations)
                })
            
            # Step 6: ä¿å­˜é…ç½®
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            # Step 7: é©—è­‰æ›´æ–°
            verification = await self._verify_json_integrity()
            if not verification["valid"]:
                # è‡ªå‹•æ¢å¾©
                await self.emergency_restore()
                raise Exception(f"æ›´æ–°å¾Œé©—è­‰å¤±æ•—ï¼Œå·²è‡ªå‹•æ¢å¾©: {verification['error']}")
            
            update_result["verification_passed"] = True
            logger.info(f"âœ… åƒæ•¸æ›´æ–°å®Œæˆ: {len(parameter_updates)} å€‹åƒæ•¸")
            
            return update_result
            
        except Exception as e:
            logger.error(f"âŒ åƒæ•¸æ›´æ–°å¤±æ•—: {e}")
            # å˜—è©¦ç·Šæ€¥æ¢å¾©
            restore_result = await self.emergency_restore()
            return {
                "status": "error",
                "message": f"æ›´æ–°å¤±æ•—: {str(e)}",
                "emergency_restore": restore_result
            }
    
    async def emergency_restore(self) -> Dict[str, Any]:
        """ç·Šæ€¥æ¢å¾©åˆ°åŸå§‹é…ç½®"""
        try:
            logger.warning("ğŸš¨ åŸ·è¡Œç·Šæ€¥æ¢å¾©...")
            
            if not self.original_backup_path.exists():
                raise Exception("åŸå§‹å‚™ä»½ä¸å­˜åœ¨ï¼Œç„¡æ³•æ¢å¾©")
            
            # æ¢å¾©åŸå§‹é…ç½®
            shutil.copy2(self.original_backup_path, self.config_path)
            
            # é‡ç½®å®‰å…¨ç‹€æ…‹
            self.safety_state["modification_count"] = 0
            self.safety_state["last_verification"] = datetime.datetime.now().isoformat()
            
            # é©—è­‰æ¢å¾©
            verification = await self._verify_json_integrity()
            
            return {
                "status": "success",
                "message": "ç·Šæ€¥æ¢å¾©å®Œæˆ",
                "restored_from": str(self.original_backup_path),
                "verification_passed": verification["valid"],
                "timestamp": datetime.datetime.now().isoformat(),
            }
            
        except Exception as e:
            logger.error(f"âŒ ç·Šæ€¥æ¢å¾©å¤±æ•—: {e}")
            return {
                "status": "critical_error",
                "message": f"ç·Šæ€¥æ¢å¾©å¤±æ•—: {str(e)}"
            }
    
    async def _cleanup_old_backups(self) -> Dict[str, Any]:
        """æ¸…ç†èˆŠçš„å‚™ä»½æª”æ¡ˆ"""
        try:
            logger.info("ğŸ§¹ æ¸…ç†èˆŠå‚™ä»½æª”æ¡ˆ...")
            
            if not self.working_backup_dir.exists():
                return {"cleaned_count": 0, "status": "no_backups"}
            
            backup_files = list(self.working_backup_dir.glob("*.json"))
            if not backup_files:
                return {"cleaned_count": 0, "status": "no_backups"}
            
            # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            cleaned_count = 0
            current_time = datetime.datetime.now()
            
            # æ¸…ç†ç­–ç•¥1: ä¿ç•™æœ€æ–°çš„Nå€‹å‚™ä»½
            if len(backup_files) > self.backup_retention["max_backups"]:
                excess_files = backup_files[self.backup_retention["max_backups"]:]
                for file_path in excess_files:
                    try:
                        file_path.unlink()
                        cleaned_count += 1
                        logger.info(f"  å·²æ¸…ç†å¤šé¤˜å‚™ä»½: {file_path.name}")
                    except Exception as e:
                        logger.warning(f"  æ¸…ç†æª”æ¡ˆå¤±æ•— {file_path.name}: {e}")
            
            # æ¸…ç†ç­–ç•¥2: æ¸…ç†è¶…éæ™‚é–“é™åˆ¶çš„å‚™ä»½
            remaining_files = backup_files[:self.backup_retention["max_backups"]]
            for file_path in remaining_files:
                try:
                    file_time = datetime.datetime.fromtimestamp(file_path.stat().st_mtime)
                    age_days = (current_time - file_time).days
                    
                    if age_days > self.backup_retention["max_age_days"]:
                        file_path.unlink()
                        cleaned_count += 1
                        logger.info(f"  å·²æ¸…ç†éæœŸå‚™ä»½: {file_path.name} (å­˜åœ¨{age_days}å¤©)")
                except Exception as e:
                    logger.warning(f"  æ¸…ç†æª”æ¡ˆå¤±æ•— {file_path.name}: {e}")
            
            logger.info(f"âœ… å‚™ä»½æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_count} å€‹æª”æ¡ˆ")
            
            return {
                "cleaned_count": cleaned_count,
                "status": "success",
                "retention_policy": self.backup_retention
            }
            
        except Exception as e:
            logger.error(f"âŒ å‚™ä»½æ¸…ç†å¤±æ•—: {e}")
            return {
                "cleaned_count": 0,
                "status": "error",
                "error": str(e)
            }
    
    async def _create_safety_backup(self, label: str) -> Dict[str, Any]:
        """å‰µå»ºå®‰å…¨å‚™ä»½"""
        try:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"phase1a_backup_{label}_{timestamp}.json"
            backup_path = self.working_backup_dir / backup_filename
            
            shutil.copy2(self.config_path, backup_path)
            
            return {
                "success": True,
                "backup_path": str(backup_path),
                "timestamp": timestamp
            }
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»ºå‚™ä»½å¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _verify_json_integrity(self) -> Dict[str, Any]:
        """é©—è­‰JSONé…ç½®å®Œæ•´æ€§"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                json.load(f)
            
            self.safety_state["last_verification"] = datetime.datetime.now().isoformat()
            
            return {
                "valid": True,
                "verification_time": self.safety_state["last_verification"]
            }
            
        except json.JSONDecodeError as e:
            return {
                "valid": False,
                "error": f"JSONèªæ³•éŒ¯èª¤: {str(e)}"
            }
        except Exception as e:
            return {
                "valid": False,
                "error": f"æª”æ¡ˆè®€å–éŒ¯èª¤: {str(e)}"
            }
    
    async def _calculate_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æª”æ¡ˆå“ˆå¸Œå€¼"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    async def _get_config_stats(self) -> Dict[str, Any]:
        """ç²å–é…ç½®æª”æ¡ˆçµ±è¨ˆä¿¡æ¯"""
        stats = self.config_path.stat()
        return {
            "size": stats.st_size / 1024,  # KB
            "modified": datetime.datetime.fromtimestamp(stats.st_mtime).isoformat()
        }
    
    async def _update_parameter_recursive(self, obj: Any, param_name: str, new_value: Any, 
                                         current_path: str = "", locations: list = None) -> list:
        """éæ­¸æ›´æ–°åƒæ•¸ï¼Œæ”¯æ´æ–°å¢åƒæ•¸"""
        if locations is None:
            locations = []
        
        if isinstance(obj, dict):
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ–°å¢åƒæ•¸åˆ°æ ¹å±¤ç´š
            if current_path == "" and param_name not in obj:
                obj[param_name] = new_value
                locations.append(param_name)
                logger.info(f"    âœ… æ–°å¢åƒæ•¸: {param_name} = {new_value}")
                return locations
            
            for key, value in obj.items():
                new_path = f"{current_path}.{key}" if current_path else key
                
                if key == param_name:
                    obj[key] = new_value
                    locations.append(new_path)
                    logger.info(f"    âœ… æ›´æ–°ä½ç½®: {new_path} = {new_value}")
                else:
                    await self._update_parameter_recursive(value, param_name, new_value, new_path, locations)
        
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                new_path = f"{current_path}[{i}]"
                await self._update_parameter_recursive(item, param_name, new_value, new_path, locations)
        
        return locations
    
    async def get_safety_status(self) -> Dict[str, Any]:
        """ç²å–å®‰å…¨ç‹€æ…‹å ±å‘Š"""
        try:
            # çµ±è¨ˆå‚™ä»½æª”æ¡ˆ
            backup_count = len(list(self.working_backup_dir.glob("*.json"))) if self.working_backup_dir.exists() else 0
            
            # é©—è­‰åŸå§‹å‚™ä»½
            original_exists = self.original_backup_path.exists()
            
            # ç•¶å‰é…ç½®å“ˆå¸Œ
            current_hash = await self._calculate_file_hash(self.config_path)
            
            # å®Œæ•´æ€§æª¢æŸ¥
            integrity_check = await self._verify_json_integrity()
            
            return {
                "safety_deployed": self.safety_state["is_deployed"],
                "original_backup_exists": original_exists,
                "current_config_hash": current_hash[:12],
                "original_config_hash": self.safety_state["original_hash"][:12] if self.safety_state["original_hash"] else None,
                "config_modified": current_hash != self.safety_state["original_hash"],
                "modification_count": self.safety_state["modification_count"],
                "working_backups_count": backup_count,
                "last_verification": self.safety_state["last_verification"],
                "integrity_confirmed": integrity_check["valid"],
                "backup_retention_policy": self.backup_retention
            }
            
        except Exception as e:
            return {
                "error": f"ç‹€æ…‹æª¢æŸ¥å¤±æ•—: {str(e)}"
            }


async def main():
    """ä¸»ç¨‹åº - éƒ¨ç½²å®‰å…¨ç³»çµ±"""
    # é…ç½®è·¯å¾‘
    config_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1a_basic_signal_generation/phase1a_basic_signal_generation.json"
    
    # å‰µå»ºå®‰å…¨ç®¡ç†å™¨
    safety_manager = Phase1AConfigSafetyManager(config_path)
    
    print("ğŸ›¡ï¸ Trading X - Phase1A å®‰å…¨ç®¡ç†å™¨")
    print("=" * 50)
    
    # éƒ¨ç½²å®‰å…¨ç³»çµ±
    print("\nğŸ“‹ éƒ¨ç½²å®‰å…¨ç³»çµ±...")
    result = await safety_manager.deploy_safety_system()
    
    if result["status"] == "success":
        print(f"âœ… {result['message']}")
        print(f"   åŸ·è¡Œå‹•ä½œ: {len(result['actions_performed'])} å€‹")
        print(f"   åŸå§‹é…ç½®æŒ‡ç´‹: {result['original_config_fingerprint']}")
        print(f"   é…ç½®æª”æ¡ˆå¤§å°: {result['config_size']}")
        
        # é¡¯ç¤ºå®‰å…¨ç‹€æ…‹
        print("\nğŸ“Š å®‰å…¨ç‹€æ…‹å ±å‘Š:")
        status = await safety_manager.get_safety_status()
        for key, value in status.items():
            print(f"   {key}: {value}")
        
    else:
        print(f"âŒ {result['message']}")


if __name__ == "__main__":
    asyncio.run(main())
