"""
ğŸ¯ Phase4 EPL Decision History Tracking
======================================

EPLæ±ºç­–æ­·å²è¿½è¹¤å¯¦ç¾ - åŸºæ–¼é…ç½®é©…å‹•çš„æ±ºç­–éç¨‹è¨˜éŒ„èˆ‡åˆ†æ
èˆ‡ epl_decision_history_tracking_config.json é…ç½®æ–‡ä»¶å°æ‡‰
"""

import asyncio
import logging
import json
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import defaultdict, deque
from enum import Enum
import statistics

logger = logging.getLogger(__name__)

class EPLDecisionType(Enum):
    """EPLæ±ºç­–é¡å‹"""
    REPLACE_POSITION = "REPLACE_POSITION"
    STRENGTHEN_POSITION = "STRENGTHEN_POSITION"
    CREATE_NEW_POSITION = "CREATE_NEW_POSITION"
    IGNORE_SIGNAL = "IGNORE_SIGNAL"

class SignalPriority(Enum):
    """ä¿¡è™Ÿå„ªå…ˆç´š"""
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"

@dataclass
class EPLDecisionRecord:
    """EPLæ±ºç­–è¨˜éŒ„"""
    decision_id: str
    timestamp: datetime
    symbol: str
    signal_priority: SignalPriority
    decision_type: EPLDecisionType
    confidence_score: float
    risk_assessment: Dict[str, float]
    position_context: Dict[str, Any]
    execution_details: Dict[str, Any]
    outcome_tracking: Optional[Dict[str, Any]] = None

@dataclass
class DecisionOutcome:
    """æ±ºç­–çµæœè¿½è¹¤"""
    decision_id: str
    timestamp: datetime
    success: bool  # åŸ·è¡ŒæˆåŠŸ
    pnl: Optional[float] = None  # æç›Š
    risk_realized: Optional[float] = None  # å¯¦éš›é¢¨éšª
    execution_quality: Optional[float] = None  # åŸ·è¡Œå“è³ª
    performance_metrics: Optional[Dict[str, float]] = None  # æ€§èƒ½æŒ‡æ¨™
    actual_vs_expected: Optional[Dict[str, float]] = None  # å¯¦éš›vsé æœŸ
    lessons_learned: Optional[List[str]] = None  # ç¶“é©—æ•™è¨“

class EPLDecisionHistoryTracker:
    """EPLæ±ºç­–æ­·å²è¿½è¹¤ç³»çµ±"""
    
    def __init__(self):
        # è¼‰å…¥é…ç½®æ–‡ä»¶
        self.config = self._load_config()
        
        # æ±ºç­–æ­·å²å­˜å„²
        self.decision_history: deque = deque(maxlen=50000)  # ä¿ç•™æœ€è¿‘50000å€‹æ±ºç­–
        self.outcome_history: Dict[str, DecisionOutcome] = {}
        
        # çµ±è¨ˆæ•¸æ“š
        self.decision_type_stats: Dict[str, int] = defaultdict(int)
        self.priority_stats: Dict[str, int] = defaultdict(int)
        self.success_rates: Dict[str, List[bool]] = defaultdict(list)
        
        # æ€§èƒ½è¿½è¹¤
        self.tracking_enabled = True
        self.last_update = datetime.now()
        
        # åˆå§‹åŒ–è¿½è¹¤ç³»çµ±
        self._initialize_tracking()
        
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            config_path = Path(__file__).parent / "epl_decision_history_tracking_config.json"
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥EPLè¿½è¹¤é…ç½®å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ç²å–é»˜èªé…ç½®"""
        return {
            "PHASE4_EPL_DECISION_HISTORY_TRACKING": {
                "system_metadata": {
                    "version": "2.1.0",
                    "description": "EPL Decision History Tracking - Default Configuration"
                },
                "decision_tracking": {
                    "track_all_decisions": True,
                    "retention_period_days": 30,
                    "outcome_follow_up_hours": 24
                },
                "decision_tracking_architecture": {
                    "decision_lifecycle_monitoring": {
                        "decision_creation": {"enabled": True},
                        "decision_execution_tracking": {"enabled": True},
                        "outcome_measurement": {"enabled": True}
                    },
                    "decision_type_analytics": {
                        "replacement_decision_tracking": {"enabled": True},
                        "strengthening_decision_tracking": {"enabled": True},
                        "new_position_decision_tracking": {"enabled": True},
                        "ignore_decision_tracking": {"enabled": True}
                    }
                },
                "learning_and_optimization": {
                    "pattern_recognition": {"enabled": True},
                    "adaptive_learning": {"enabled": True},
                    "feedback_integration": {"enabled": True}
                },
                "reporting_and_analytics": {
                    "real_time_dashboards": {"enabled": True},
                    "historical_analysis": {"enabled": True}
                },
                "data_storage_and_retrieval": {
                    "decision_data_storage": {
                        "raw_decision_data": {"retention_period": "5_years"},
                        "aggregated_analytics": {"retention_period": "10_years"}
                    }
                }
            }
        }
    
    def _initialize_tracking(self):
        """åˆå§‹åŒ–è¿½è¹¤ç³»çµ±"""
        logger.info("åˆå§‹åŒ–EPLæ±ºç­–æ­·å²è¿½è¹¤ç³»çµ±")
        
        # è¨­ç½®è¿½è¹¤åƒæ•¸
        config = self.config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        tracking_config = config.get("decision_tracking", {})
        
        self.track_all_decisions = tracking_config.get("track_all_decisions", True)
        self.retention_days = tracking_config.get("retention_period_days", 30)
        self.outcome_follow_up_hours = tracking_config.get("outcome_follow_up_hours", 24)
        
        # æ¸…ç†éèˆŠçš„è¨˜éŒ„
        self._cleanup_old_records()
        
    def _cleanup_old_records(self):
        """æ¸…ç†éèˆŠçš„è¨˜éŒ„"""
        cutoff_time = datetime.now() - timedelta(days=self.retention_days)
        
        # æ¸…ç†æ±ºç­–æ­·å²
        self.decision_history = deque(
            [record for record in self.decision_history 
             if record.timestamp > cutoff_time],
            maxlen=50000
        )
        
        # æ¸…ç†çµæœæ­·å²
        old_decision_ids = [
            decision_id for decision_id, outcome in self.outcome_history.items()
            if outcome.timestamp < cutoff_time
        ]
        for decision_id in old_decision_ids:
            del self.outcome_history[decision_id]
    
    async def record_epl_decision(self, decision_data: Dict[str, Any]) -> str:
        """è¨˜éŒ„EPLæ±ºç­–"""
        try:
            # ç”Ÿæˆæ±ºç­–ID
            decision_id = f"epl_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{decision_data.get('symbol', 'UNK')}"
            
            # å‰µå»ºæ±ºç­–è¨˜éŒ„
            decision_record = EPLDecisionRecord(
                decision_id=decision_id,
                timestamp=datetime.fromisoformat(decision_data.get('timestamp', datetime.now().isoformat())),
                symbol=decision_data.get('symbol', 'UNKNOWN'),
                signal_priority=SignalPriority(decision_data.get('priority', 'MEDIUM')),
                decision_type=EPLDecisionType(decision_data.get('decision_type', 'IGNORE_SIGNAL')),
                confidence_score=float(decision_data.get('confidence_score', 0.5)),
                risk_assessment=decision_data.get('risk_assessment', {}),
                position_context=decision_data.get('position_context', {}),
                execution_details=decision_data.get('execution_details', {})
            )
            
            # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
            self.decision_history.append(decision_record)
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_decision_statistics(decision_record)
            
            self.last_update = datetime.now()
            
            logger.info(f"è¨˜éŒ„EPLæ±ºç­–: {decision_id}, é¡å‹: {decision_record.decision_type.value}")
            return decision_id
            
        except Exception as e:
            logger.error(f"è¨˜éŒ„EPLæ±ºç­–å¤±æ•—: {e}")
            return ""
    
    def _update_decision_statistics(self, record: EPLDecisionRecord):
        """æ›´æ–°æ±ºç­–çµ±è¨ˆ"""
        self.decision_type_stats[record.decision_type.value] += 1
        self.priority_stats[record.signal_priority.value] += 1
    
    async def record_decision_outcome(self, decision_id: str, outcome_data: Dict[str, Any]) -> bool:
        """è¨˜éŒ„æ±ºç­–çµæœ"""
        try:
            outcome = DecisionOutcome(
                decision_id=decision_id,
                timestamp=datetime.now(),
                success=outcome_data.get('success', outcome_data.get('execution_success', False)),
                pnl=outcome_data.get('pnl'),
                risk_realized=outcome_data.get('risk_realized'),
                execution_quality=outcome_data.get('execution_quality'),
                performance_metrics=outcome_data.get('performance_metrics', {}),
                actual_vs_expected=outcome_data.get('actual_vs_expected', {}),
                lessons_learned=outcome_data.get('lessons_learned', [])
            )
            
            self.outcome_history[decision_id] = outcome
            
            # æ›´æ–°æˆåŠŸç‡çµ±è¨ˆ
            decision_record = self._find_decision_by_id(decision_id)
            if decision_record:
                decision_type = decision_record.decision_type.value
                self.success_rates[decision_type].append(outcome.success)
                
                # é™åˆ¶æˆåŠŸç‡æ­·å²è¨˜éŒ„é•·åº¦
                if len(self.success_rates[decision_type]) > 1000:
                    self.success_rates[decision_type].pop(0)
            
            logger.info(f"è¨˜éŒ„æ±ºç­–çµæœ: {decision_id}, æˆåŠŸ: {outcome.success}")
            return True
            
        except Exception as e:
            logger.error(f"è¨˜éŒ„æ±ºç­–çµæœå¤±æ•—: {e}")
            return False
    
    def _find_decision_by_id(self, decision_id: str) -> Optional[EPLDecisionRecord]:
        """æ ¹æ“šIDæŸ¥æ‰¾æ±ºç­–è¨˜éŒ„"""
        for record in self.decision_history:
            if record.decision_id == decision_id:
                return record
        return None
    
    async def get_comprehensive_decision_analysis(self) -> Dict[str, Any]:
        """ç²å–ç¶œåˆæ±ºç­–åˆ†æ"""
        try:
            current_time = datetime.now()
            
            if not self.decision_history:
                return self._get_empty_analysis()
            
            # åŸºæœ¬çµ±è¨ˆ
            total_decisions = len(self.decision_history)
            decisions_with_outcomes = len(self.outcome_history)
            
            # æ±ºç­–é¡å‹åˆ†æ
            decision_type_analysis = self._analyze_decision_types()
            
            # å„ªå…ˆç´šåˆ†æ
            priority_analysis = self._analyze_priority_patterns()
            
            # æˆåŠŸç‡åˆ†æ
            success_rate_analysis = self._analyze_success_rates()
            
            # ç½®ä¿¡åº¦ç›¸é—œæ€§åˆ†æ
            confidence_analysis = self._analyze_confidence_correlation()
            
            # æ™‚é–“æ¨¡å¼åˆ†æ
            temporal_analysis = self._analyze_temporal_patterns()
            
            # æ€§èƒ½æŒ‡æ¨™
            performance_metrics = self._calculate_performance_metrics()
            
            # é¢¨éšªè©•ä¼°åˆ†æ
            risk_analysis = self._analyze_risk_patterns()
            
            return {
                "analysis_metadata": {
                    "generated_at": current_time.isoformat(),
                    "total_decisions_tracked": total_decisions,
                    "decisions_with_outcomes": decisions_with_outcomes,
                    "tracking_period": {
                        "start": min(r.timestamp for r in self.decision_history).isoformat(),
                        "end": max(r.timestamp for r in self.decision_history).isoformat()
                    },
                    "last_update": self.last_update.isoformat()
                },
                "decision_type_distribution": decision_type_analysis,
                "priority_level_insights": priority_analysis,
                "success_rate_analysis": success_rate_analysis,
                "confidence_correlation_analysis": confidence_analysis,
                "temporal_decision_patterns": temporal_analysis,
                "performance_benchmarks": performance_metrics,
                "risk_assessment_insights": risk_analysis,
                "decision_quality_trends": self._analyze_decision_quality_trends(),
                "recommendations": self._generate_recommendations()
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¶œåˆæ±ºç­–åˆ†æå¤±æ•—: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}
    
    def _get_empty_analysis(self) -> Dict[str, Any]:
        """ç²å–ç©ºåˆ†ææ•¸æ“š"""
        return {
            "analysis_metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_decisions_tracked": 0,
                "status": "no_data"
            },
            "message": "æ²’æœ‰è¶³å¤ çš„æ±ºç­–æ•¸æ“šé€²è¡Œåˆ†æ"
        }
    
    def _analyze_decision_types(self) -> Dict[str, Any]:
        """åˆ†ææ±ºç­–é¡å‹"""
        type_distribution = dict(self.decision_type_stats)
        total = sum(type_distribution.values())
        
        if total == 0:
            return {"distribution": {}, "trends": []}
        
        # è¨ˆç®—ç™¾åˆ†æ¯”
        type_percentages = {
            decision_type: (count / total) * 100
            for decision_type, count in type_distribution.items()
        }
        
        # åˆ†æè¶¨å‹¢ï¼ˆæœ€è¿‘24å°æ™‚ vs ä¹‹å‰ï¼‰
        cutoff_time = datetime.now() - timedelta(hours=24)
        recent_decisions = [r for r in self.decision_history if r.timestamp > cutoff_time]
        older_decisions = [r for r in self.decision_history if r.timestamp <= cutoff_time]
        
        recent_type_stats = defaultdict(int)
        older_type_stats = defaultdict(int)
        
        for record in recent_decisions:
            recent_type_stats[record.decision_type.value] += 1
        
        for record in older_decisions:
            older_type_stats[record.decision_type.value] += 1
        
        # è¨ˆç®—è¶¨å‹¢è®ŠåŒ–
        trend_changes = {}
        for decision_type in type_distribution.keys():
            recent_ratio = recent_type_stats[decision_type] / max(len(recent_decisions), 1)
            older_ratio = older_type_stats[decision_type] / max(len(older_decisions), 1)
            
            trend_changes[decision_type] = {
                "recent_ratio": recent_ratio,
                "previous_ratio": older_ratio,
                "change_percentage": ((recent_ratio - older_ratio) / max(older_ratio, 0.01)) * 100
            }
        
        return {
            "distribution": type_distribution,
            "percentages": type_percentages,
            "trend_analysis": trend_changes,
            "dominant_decision_type": max(type_distribution.items(), key=lambda x: x[1])[0] if type_distribution else None
        }
    
    def _analyze_priority_patterns(self) -> Dict[str, Any]:
        """åˆ†æå„ªå…ˆç´šæ¨¡å¼"""
        priority_distribution = dict(self.priority_stats)
        
        # æŒ‰å„ªå…ˆç´šåˆ†ææ±ºç­–é¡å‹
        priority_decision_matrix = defaultdict(lambda: defaultdict(int))
        
        for record in self.decision_history:
            priority_decision_matrix[record.signal_priority.value][record.decision_type.value] += 1
        
        # è¨ˆç®—å„å„ªå…ˆç´šçš„è™•ç†æ•ˆç‡
        priority_efficiency = {}
        for priority, decisions in priority_decision_matrix.items():
            total = sum(decisions.values())
            efficiency_score = 0
            
            if total > 0:
                # é«˜æ•ˆæ±ºç­–æ¬Šé‡: CREATE_NEW_POSITION=1.0, STRENGTHEN=0.8, REPLACE=0.6, IGNORE=0.2
                weights = {
                    "CREATE_NEW_POSITION": 1.0,
                    "STRENGTHEN_POSITION": 0.8,
                    "REPLACE_POSITION": 0.6,
                    "IGNORE_SIGNAL": 0.2
                }
                
                weighted_sum = sum(decisions[dt] * weights.get(dt, 0.5) for dt in decisions)
                efficiency_score = weighted_sum / total
            
            priority_efficiency[priority] = efficiency_score
        
        return {
            "distribution": priority_distribution,
            "decision_matrix": dict(priority_decision_matrix),
            "processing_efficiency": priority_efficiency,
            "priority_ranking": sorted(priority_efficiency.items(), key=lambda x: x[1], reverse=True)
        }
    
    def _analyze_success_rates(self) -> Dict[str, Any]:
        """åˆ†ææˆåŠŸç‡"""
        success_rate_summary = {}
        
        for decision_type, successes in self.success_rates.items():
            if successes:
                success_rate = sum(successes) / len(successes)
                recent_successes = successes[-10:] if len(successes) >= 10 else successes
                recent_success_rate = sum(recent_successes) / len(recent_successes)
                
                success_rate_summary[decision_type] = {
                    "overall_success_rate": success_rate,
                    "recent_success_rate": recent_success_rate,
                    "total_attempts": len(successes),
                    "trend": "improving" if recent_success_rate > success_rate else "declining" if recent_success_rate < success_rate else "stable"
                }
        
        # è¨ˆç®—æ•´é«”æˆåŠŸç‡
        all_successes = []
        for successes in self.success_rates.values():
            all_successes.extend(successes)
        
        overall_success_rate = sum(all_successes) / len(all_successes) if all_successes else 0
        
        return {
            "by_decision_type": success_rate_summary,
            "overall_success_rate": overall_success_rate,
            "total_tracked_outcomes": len(all_successes),
            "best_performing_decision": max(success_rate_summary.items(), key=lambda x: x[1]["overall_success_rate"])[0] if success_rate_summary else None
        }
    
    def _analyze_confidence_correlation(self) -> Dict[str, Any]:
        """åˆ†æç½®ä¿¡åº¦ç›¸é—œæ€§"""
        decisions_with_outcomes = []
        
        for record in self.decision_history:
            if record.decision_id in self.outcome_history:
                outcome = self.outcome_history[record.decision_id]
                decisions_with_outcomes.append({
                    "confidence": record.confidence_score,
                    "success": outcome.success,
                    "decision_type": record.decision_type.value
                })
        
        if not decisions_with_outcomes:
            return {"correlation_analysis": "insufficient_data"}
        
        # æŒ‰ç½®ä¿¡åº¦å€é–“åˆ†ææˆåŠŸç‡
        confidence_bins = {
            "low_confidence": [d for d in decisions_with_outcomes if d["confidence"] < 0.5],
            "medium_confidence": [d for d in decisions_with_outcomes if 0.5 <= d["confidence"] < 0.8],
            "high_confidence": [d for d in decisions_with_outcomes if d["confidence"] >= 0.8]
        }
        
        bin_analysis = {}
        for bin_name, decisions in confidence_bins.items():
            if decisions:
                success_rate = sum(d["success"] for d in decisions) / len(decisions)
                avg_confidence = statistics.mean(d["confidence"] for d in decisions)
                
                bin_analysis[bin_name] = {
                    "count": len(decisions),
                    "success_rate": success_rate,
                    "average_confidence": avg_confidence
                }
        
        # è¨ˆç®—ç›¸é—œæ€§ä¿‚æ•¸ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        confidences = [d["confidence"] for d in decisions_with_outcomes]
        successes = [1 if d["success"] else 0 for d in decisions_with_outcomes]
        
        correlation_coefficient = 0
        if len(confidences) > 1:
            try:
                mean_conf = statistics.mean(confidences)
                mean_succ = statistics.mean(successes)
                
                numerator = sum((c - mean_conf) * (s - mean_succ) for c, s in zip(confidences, successes))
                denominator = (sum((c - mean_conf) ** 2 for c in confidences) * sum((s - mean_succ) ** 2 for s in successes)) ** 0.5
                
                if denominator > 0:
                    correlation_coefficient = numerator / denominator
            except:
                pass
        
        return {
            "confidence_bin_analysis": bin_analysis,
            "correlation_coefficient": correlation_coefficient,
            "correlation_strength": self._interpret_correlation(correlation_coefficient),
            "total_analyzed": len(decisions_with_outcomes)
        }
    
    def _interpret_correlation(self, coefficient: float) -> str:
        """è§£é‡‹ç›¸é—œæ€§ä¿‚æ•¸"""
        abs_coeff = abs(coefficient)
        if abs_coeff >= 0.8:
            return "very_strong"
        elif abs_coeff >= 0.6:
            return "strong"
        elif abs_coeff >= 0.4:
            return "moderate"
        elif abs_coeff >= 0.2:
            return "weak"
        else:
            return "very_weak"
    
    def _analyze_temporal_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ™‚é–“æ¨¡å¼"""
        # æŒ‰å°æ™‚åˆ†ææ±ºç­–æ¨¡å¼
        hourly_patterns = defaultdict(lambda: defaultdict(int))
        
        for record in self.decision_history:
            hour = record.timestamp.hour
            hourly_patterns[hour][record.decision_type.value] += 1
        
        # æ‰¾å‡ºæœ€æ´»èºçš„æ™‚æ®µ
        hourly_activity = {hour: sum(decisions.values()) for hour, decisions in hourly_patterns.items()}
        peak_hours = sorted(hourly_activity.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æŒ‰å·¥ä½œæ—¥åˆ†æ
        weekday_patterns = defaultdict(lambda: defaultdict(int))
        
        for record in self.decision_history:
            weekday = record.timestamp.weekday()  # 0=Monday
            weekday_patterns[weekday][record.decision_type.value] += 1
        
        weekday_names = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        weekday_analysis = {}
        
        for day_num, decisions in weekday_patterns.items():
            day_name = weekday_names[day_num]
            total_decisions = sum(decisions.values())
            
            weekday_analysis[day_name] = {
                "total_decisions": total_decisions,
                "decision_distribution": dict(decisions),
                "activity_score": total_decisions / max(sum(hourly_activity.values()), 1)
            }
        
        return {
            "hourly_patterns": dict(hourly_patterns),
            "peak_activity_hours": [f"{hour:02d}:00" for hour, _ in peak_hours],
            "weekday_analysis": weekday_analysis,
            "busiest_day": max(weekday_analysis.items(), key=lambda x: x[1]["total_decisions"])[0] if weekday_analysis else None
        }
    
    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """è¨ˆç®—æ€§èƒ½æŒ‡æ¨™"""
        if not self.decision_history:
            return {}
        
        # æ±ºç­–å»¶é²åˆ†æ
        decision_latencies = []
        for record in self.decision_history:
            # æ¨¡æ“¬æ±ºç­–å»¶é²ï¼ˆå¯¦éš›æ‡‰è©²å¾åŸ·è¡Œè©³æƒ…ä¸­ç²å–ï¼‰
            latency = record.execution_details.get('decision_latency', 0)
            if latency > 0:
                decision_latencies.append(latency)
        
        # ç½®ä¿¡åº¦åˆ†æ
        confidence_scores = [record.confidence_score for record in self.decision_history]
        
        # é¢¨éšªè©•ä¼°åˆ†æ
        risk_scores = []
        for record in self.decision_history:
            overall_risk = record.risk_assessment.get('overall_risk', 0.5)
            risk_scores.append(overall_risk)
        
        return {
            "decision_latency_metrics": {
                "average_latency": statistics.mean(decision_latencies) if decision_latencies else 0,
                "median_latency": statistics.median(decision_latencies) if decision_latencies else 0,
                "max_latency": max(decision_latencies) if decision_latencies else 0
            },
            "confidence_metrics": {
                "average_confidence": statistics.mean(confidence_scores),
                "confidence_distribution": self._create_confidence_distribution(confidence_scores),
                "low_confidence_rate": len([c for c in confidence_scores if c < 0.5]) / len(confidence_scores)
            },
            "risk_metrics": {
                "average_risk_score": statistics.mean(risk_scores),
                "high_risk_decisions": len([r for r in risk_scores if r > 0.7]) / len(risk_scores),
                "risk_distribution": self._create_risk_distribution(risk_scores)
            },
            "throughput_metrics": {
                "decisions_per_hour": len(self.decision_history) / max((datetime.now() - min(r.timestamp for r in self.decision_history)).total_seconds() / 3600, 1),
                "total_decisions": len(self.decision_history),
                "decisions_with_outcomes": len(self.outcome_history)
            }
        }
    
    def _create_confidence_distribution(self, scores: List[float]) -> Dict[str, int]:
        """å‰µå»ºç½®ä¿¡åº¦åˆ†ä½ˆ"""
        distribution = {"0.0-0.2": 0, "0.2-0.4": 0, "0.4-0.6": 0, "0.6-0.8": 0, "0.8-1.0": 0}
        
        for score in scores:
            if score < 0.2:
                distribution["0.0-0.2"] += 1
            elif score < 0.4:
                distribution["0.2-0.4"] += 1
            elif score < 0.6:
                distribution["0.4-0.6"] += 1
            elif score < 0.8:
                distribution["0.6-0.8"] += 1
            else:
                distribution["0.8-1.0"] += 1
        
        return distribution
    
    def _create_risk_distribution(self, scores: List[float]) -> Dict[str, int]:
        """å‰µå»ºé¢¨éšªåˆ†ä½ˆ"""
        distribution = {"Low": 0, "Medium": 0, "High": 0}
        
        for score in scores:
            if score < 0.3:
                distribution["Low"] += 1
            elif score < 0.7:
                distribution["Medium"] += 1
            else:
                distribution["High"] += 1
        
        return distribution
    
    def _analyze_risk_patterns(self) -> Dict[str, Any]:
        """åˆ†æé¢¨éšªæ¨¡å¼"""
        risk_by_decision_type = defaultdict(list)
        risk_by_priority = defaultdict(list)
        
        for record in self.decision_history:
            overall_risk = record.risk_assessment.get('overall_risk', 0.5)
            risk_by_decision_type[record.decision_type.value].append(overall_risk)
            risk_by_priority[record.signal_priority.value].append(overall_risk)
        
        # è¨ˆç®—å„æ±ºç­–é¡å‹çš„å¹³å‡é¢¨éšª
        risk_by_type_summary = {}
        for decision_type, risks in risk_by_decision_type.items():
            if risks:
                risk_by_type_summary[decision_type] = {
                    "average_risk": statistics.mean(risks),
                    "max_risk": max(risks),
                    "risk_variance": statistics.variance(risks) if len(risks) > 1 else 0
                }
        
        # è¨ˆç®—å„å„ªå…ˆç´šçš„å¹³å‡é¢¨éšª
        risk_by_priority_summary = {}
        for priority, risks in risk_by_priority.items():
            if risks:
                risk_by_priority_summary[priority] = {
                    "average_risk": statistics.mean(risks),
                    "max_risk": max(risks),
                    "risk_variance": statistics.variance(risks) if len(risks) > 1 else 0
                }
        
        return {
            "risk_by_decision_type": risk_by_type_summary,
            "risk_by_priority": risk_by_priority_summary,
            "riskiest_decision_type": max(risk_by_type_summary.items(), key=lambda x: x[1]["average_risk"])[0] if risk_by_type_summary else None,
            "safest_decision_type": min(risk_by_type_summary.items(), key=lambda x: x[1]["average_risk"])[0] if risk_by_type_summary else None
        }
    
    def _analyze_decision_quality_trends(self) -> Dict[str, Any]:
        """åˆ†ææ±ºç­–è³ªé‡è¶¨å‹¢"""
        # æŒ‰æ™‚é–“é †åºåˆ†ææ±ºç­–è³ªé‡
        sorted_decisions = sorted(self.decision_history, key=lambda x: x.timestamp)
        
        if len(sorted_decisions) < 10:
            return {"trend_analysis": "insufficient_data"}
        
        # å°‡æ±ºç­–åˆ†æˆæ™‚é–“æ®µ
        total_decisions = len(sorted_decisions)
        segment_size = max(total_decisions // 5, 10)  # åˆ†æˆ5æ®µï¼Œæ¯æ®µè‡³å°‘10å€‹æ±ºç­–
        
        segments = []
        for i in range(0, total_decisions, segment_size):
            segment = sorted_decisions[i:i + segment_size]
            if len(segment) >= 5:  # ç¢ºä¿æ¯æ®µæœ‰è¶³å¤ çš„æ•¸æ“š
                segments.append(segment)
        
        trend_data = []
        for i, segment in enumerate(segments):
            avg_confidence = statistics.mean(record.confidence_score for record in segment)
            avg_risk = statistics.mean(record.risk_assessment.get('overall_risk', 0.5) for record in segment)
            
            # è¨ˆç®—è©²æ®µçš„æˆåŠŸç‡
            segment_outcomes = [
                self.outcome_history[record.decision_id].success
                for record in segment
                if record.decision_id in self.outcome_history
            ]
            
            success_rate = statistics.mean(segment_outcomes) if segment_outcomes else None
            
            trend_data.append({
                "segment": i + 1,
                "time_range": {
                    "start": segment[0].timestamp.isoformat(),
                    "end": segment[-1].timestamp.isoformat()
                },
                "average_confidence": avg_confidence,
                "average_risk": avg_risk,
                "success_rate": success_rate,
                "decision_count": len(segment)
            })
        
        return {
            "trend_segments": trend_data,
            "quality_improvement": self._calculate_quality_improvement(trend_data),
            "trend_summary": self._summarize_trends(trend_data)
        }
    
    def _calculate_quality_improvement(self, trend_data: List[Dict]) -> Dict[str, Any]:
        """è¨ˆç®—è³ªé‡æ”¹å–„"""
        if len(trend_data) < 2:
            return {"improvement_analysis": "insufficient_data"}
        
        first_segment = trend_data[0]
        last_segment = trend_data[-1]
        
        confidence_change = last_segment["average_confidence"] - first_segment["average_confidence"]
        risk_change = last_segment["average_risk"] - first_segment["average_risk"]
        
        success_rate_change = None
        if first_segment["success_rate"] is not None and last_segment["success_rate"] is not None:
            success_rate_change = last_segment["success_rate"] - first_segment["success_rate"]
        
        return {
            "confidence_improvement": confidence_change,
            "risk_reduction": -risk_change,  # è² å€¼è¡¨ç¤ºé¢¨éšªé™ä½
            "success_rate_improvement": success_rate_change,
            "overall_quality_trend": "improving" if confidence_change > 0 and risk_change < 0 else "declining" if confidence_change < 0 and risk_change > 0 else "mixed"
        }
    
    def _summarize_trends(self, trend_data: List[Dict]) -> str:
        """ç¸½çµè¶¨å‹¢"""
        if len(trend_data) < 2:
            return "æ•¸æ“šä¸è¶³ä»¥åˆ†æè¶¨å‹¢"
        
        # åˆ†æç½®ä¿¡åº¦è¶¨å‹¢
        confidence_values = [segment["average_confidence"] for segment in trend_data]
        confidence_trend = "ä¸Šå‡" if confidence_values[-1] > confidence_values[0] else "ä¸‹é™"
        
        # åˆ†æé¢¨éšªè¶¨å‹¢
        risk_values = [segment["average_risk"] for segment in trend_data]
        risk_trend = "ä¸Šå‡" if risk_values[-1] > risk_values[0] else "ä¸‹é™"
        
        return f"æ±ºç­–ç½®ä¿¡åº¦å‘ˆç¾{confidence_trend}è¶¨å‹¢ï¼Œé¢¨éšªæ°´å¹³{risk_trend}ã€‚"
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æˆåŠŸç‡åˆ†æç”Ÿæˆå»ºè­°
        if self.success_rates:
            best_performing = max(self.success_rates.items(), key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 0)
            worst_performing = min(self.success_rates.items(), key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 1)
            
            best_rate = sum(best_performing[1]) / len(best_performing[1]) if best_performing[1] else 0
            worst_rate = sum(worst_performing[1]) / len(worst_performing[1]) if worst_performing[1] else 0
            
            if best_rate > 0.8:
                recommendations.append(f"æ±ºç­–é¡å‹ {best_performing[0]} è¡¨ç¾å„ªç•°ï¼ˆæˆåŠŸç‡ {best_rate:.1%}ï¼‰ï¼Œå»ºè­°å¢åŠ æ­¤é¡æ±ºç­–çš„ä½¿ç”¨é »ç‡")
            
            if worst_rate < 0.6:
                recommendations.append(f"æ±ºç­–é¡å‹ {worst_performing[0]} è¡¨ç¾ä¸ä½³ï¼ˆæˆåŠŸç‡ {worst_rate:.1%}ï¼‰ï¼Œå»ºè­°æª¢è¨æ±ºç­–æ¨™æº–")
        
        # åŸºæ–¼ç½®ä¿¡åº¦åˆ†æç”Ÿæˆå»ºè­°
        if self.decision_history:
            avg_confidence = statistics.mean(record.confidence_score for record in self.decision_history)
            
            if avg_confidence < 0.6:
                recommendations.append("æ•´é«”æ±ºç­–ç½®ä¿¡åº¦åä½ï¼Œå»ºè­°å„ªåŒ–ä¿¡è™Ÿè³ªé‡ç¯©é¸æ¨™æº–")
            elif avg_confidence > 0.8:
                recommendations.append("æ±ºç­–ç½®ä¿¡åº¦è¡¨ç¾è‰¯å¥½ï¼Œå¯è€ƒæ…®æé«˜æ±ºç­–é »ç‡")
        
        # åŸºæ–¼é¢¨éšªåˆ†æç”Ÿæˆå»ºè­°
        high_risk_decisions = len([
            record for record in self.decision_history
            if record.risk_assessment.get('overall_risk', 0.5) > 0.7
        ])
        
        if high_risk_decisions / len(self.decision_history) > 0.3:
            recommendations.append("é«˜é¢¨éšªæ±ºç­–æ¯”ä¾‹åé«˜ï¼Œå»ºè­°åŠ å¼·é¢¨éšªæ§åˆ¶æ©Ÿåˆ¶")
        
        return recommendations if recommendations else ["ç³»çµ±é‹è¡Œæ­£å¸¸ï¼Œæš«ç„¡ç‰¹æ®Šå»ºè­°"]
    
    async def get_recent_decisions(self, hours: int = 24) -> List[Dict[str, Any]]:
        """ç²å–æœ€è¿‘çš„æ±ºç­–è¨˜éŒ„"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_decisions = [
            record for record in self.decision_history
            if record.timestamp > cutoff_time
        ]
        
        return [
            {
                "decision_id": record.decision_id,
                "timestamp": record.timestamp.isoformat(),
                "symbol": record.symbol,
                "priority": record.signal_priority.value,
                "decision_type": record.decision_type.value,
                "confidence_score": record.confidence_score,
                "risk_assessment": record.risk_assessment,
                "has_outcome": record.decision_id in self.outcome_history
            }
            for record in sorted(recent_decisions, key=lambda x: x.timestamp, reverse=True)
        ]
    
    def get_decision_by_id(self, decision_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ“šIDç²å–æ±ºç­–è©³æƒ…"""
        record = self._find_decision_by_id(decision_id)
        if not record:
            return None
        
        result = {
            "decision_record": asdict(record),
            "outcome": None
        }
        
        if decision_id in self.outcome_history:
            result["outcome"] = asdict(self.outcome_history[decision_id])
        
        return result
    
    async def update_decision_outcome(self, decision_id: str, outcome_data: Dict[str, Any]) -> bool:
        """æ›´æ–°æ±ºç­–çµæœ (åˆ¥åæ–¹æ³•)"""
        return await self.record_decision_outcome(decision_id, outcome_data)
    
    async def get_decisions_by_type(self, decision_type: str) -> List[Dict[str, Any]]:
        """æŒ‰é¡å‹ç²å–æ±ºç­–"""
        try:
            decisions = [
                record for record in self.decision_history
                if record.decision_type.value == decision_type
            ]
            
            return [
                {
                    "decision_id": record.decision_id,
                    "timestamp": record.timestamp.isoformat(),
                    "symbol": record.symbol,
                    "priority": record.signal_priority.value,
                    "confidence_score": record.confidence_score,
                    "risk_assessment": record.risk_assessment,
                    "has_outcome": record.decision_id in self.outcome_history
                }
                for record in sorted(decisions, key=lambda x: x.timestamp, reverse=True)
            ]
            
        except Exception as e:
            logger.error(f"æŒ‰é¡å‹ç²å–æ±ºç­–å¤±æ•—: {e}")
            return []
    
    async def get_comprehensive_analytics(self) -> Dict[str, Any]:
        """ç²å–ç¶œåˆåˆ†æ (åˆ¥åæ–¹æ³•)"""
        return await self.get_comprehensive_decision_analysis()
    
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """ç²å–æ€§èƒ½æŒ‡æ¨™"""
        try:
            if not self.decision_history:
                return {"message": "æš«ç„¡æ±ºç­–æ•¸æ“š"}
            
            # åŸºæœ¬çµ±è¨ˆ
            total_decisions = len(self.decision_history)
            decisions_with_outcomes = len(self.outcome_history)
            
            # è¨ˆç®—æˆåŠŸç‡
            successful_outcomes = sum(
                1 for outcome in self.outcome_history.values()
                if outcome.success
            )
            
            success_rate = successful_outcomes / decisions_with_outcomes if decisions_with_outcomes > 0 else 0
            
            # è¨ˆç®—å¹³å‡ PnL
            pnl_values = [
                outcome.pnl for outcome in self.outcome_history.values()
                if outcome.pnl is not None
            ]
            
            avg_pnl = statistics.mean(pnl_values) if pnl_values else 0
            
            # é¢¨éšªæŒ‡æ¨™
            risk_realizations = [
                outcome.risk_realized for outcome in self.outcome_history.values()
                if outcome.risk_realized is not None
            ]
            
            avg_risk_realized = statistics.mean(risk_realizations) if risk_realizations else 0
            
            return {
                "total_decisions": total_decisions,
                "decisions_with_outcomes": decisions_with_outcomes,
                "success_rate": success_rate,
                "average_pnl": avg_pnl,
                "average_risk_realized": avg_risk_realized,
                "completion_rate": decisions_with_outcomes / total_decisions if total_decisions > 0 else 0,
                "performance_summary": {
                    "profitable_decisions": successful_outcomes,
                    "unprofitable_decisions": decisions_with_outcomes - successful_outcomes,
                    "pending_decisions": total_decisions - decisions_with_outcomes
                }
            }
            
        except Exception as e:
            logger.error(f"ç²å–æ€§èƒ½æŒ‡æ¨™å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def analyze_decision_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ±ºç­–æ¨¡å¼"""
        try:
            if not self.decision_history:
                return {"message": "æš«ç„¡æ±ºç­–æ•¸æ“šé€²è¡Œæ¨¡å¼åˆ†æ"}
            
            # æˆåŠŸæ¨¡å¼åˆ†æ
            successful_decisions = [
                record for record in self.decision_history
                if record.decision_id in self.outcome_history and 
                self.outcome_history[record.decision_id].success
            ]
            
            # å¤±æ•—æ¨¡å¼åˆ†æ  
            failed_decisions = [
                record for record in self.decision_history
                if record.decision_id in self.outcome_history and 
                not self.outcome_history[record.decision_id].success
            ]
            
            return {
                "successful_patterns": {
                    "count": len(successful_decisions),
                    "avg_confidence": statistics.mean([d.confidence_score for d in successful_decisions]) if successful_decisions else 0,
                    "priority_distribution": self._analyze_priority_distribution(successful_decisions),
                    "decision_type_distribution": self._analyze_decision_type_distribution(successful_decisions)
                },
                "failure_patterns": {
                    "count": len(failed_decisions),
                    "avg_confidence": statistics.mean([d.confidence_score for d in failed_decisions]) if failed_decisions else 0,
                    "priority_distribution": self._analyze_priority_distribution(failed_decisions),
                    "decision_type_distribution": self._analyze_decision_type_distribution(failed_decisions)
                },
                "insights": self._generate_pattern_insights(successful_decisions, failed_decisions)
            }
            
        except Exception as e:
            logger.error(f"æ±ºç­–æ¨¡å¼åˆ†æå¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def get_decision_timeline(self) -> List[Dict[str, Any]]:
        """ç²å–æ±ºç­–æ™‚é–“ç·š"""
        try:
            timeline = []
            
            for record in sorted(self.decision_history, key=lambda x: x.timestamp):
                timeline_entry = {
                    "timestamp": record.timestamp.isoformat(),
                    "decision_id": record.decision_id,
                    "symbol": record.symbol,
                    "decision_type": record.decision_type.value,
                    "priority": record.signal_priority.value,
                    "confidence_score": record.confidence_score
                }
                
                # æ·»åŠ çµæœä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if record.decision_id in self.outcome_history:
                    outcome = self.outcome_history[record.decision_id]
                    timeline_entry["outcome"] = {
                        "success": outcome.success,
                        "pnl": outcome.pnl,
                        "outcome_timestamp": outcome.timestamp.isoformat()
                    }
                
                timeline.append(timeline_entry)
            
            return timeline
            
        except Exception as e:
            logger.error(f"ç²å–æ±ºç­–æ™‚é–“ç·šå¤±æ•—: {e}")
            return []
    
    async def get_risk_analytics(self) -> Dict[str, Any]:
        """ç²å–é¢¨éšªåˆ†æ"""
        try:
            if not self.outcome_history:
                return {"message": "æš«ç„¡çµæœæ•¸æ“šé€²è¡Œé¢¨éšªåˆ†æ"}
            
            # é¢¨éšªå¯¦ç¾åˆ†æ
            risk_data = []
            return_data = []
            
            for outcome in self.outcome_history.values():
                if outcome.risk_realized is not None:
                    risk_data.append(outcome.risk_realized)
                if outcome.pnl is not None:
                    return_data.append(outcome.pnl)
            
            risk_analysis = {
                "risk_vs_return": {
                    "avg_risk_realized": statistics.mean(risk_data) if risk_data else 0,
                    "avg_return": statistics.mean(return_data) if return_data else 0,
                    "risk_return_ratio": abs(statistics.mean(return_data) / statistics.mean(risk_data)) if risk_data and statistics.mean(risk_data) != 0 else 0
                },
                "risk_realization": {
                    "total_outcomes": len(self.outcome_history),
                    "high_risk_outcomes": len([r for r in risk_data if r > 0.02]),
                    "low_risk_outcomes": len([r for r in risk_data if r <= 0.01]),
                    "risk_distribution": {
                        "low_risk": len([r for r in risk_data if r <= 0.01]),
                        "medium_risk": len([r for r in risk_data if 0.01 < r <= 0.02]),
                        "high_risk": len([r for r in risk_data if r > 0.02])
                    }
                }
            }
            
            return risk_analysis
            
        except Exception as e:
            logger.error(f"é¢¨éšªåˆ†æå¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        try:
            return {
                "system_health": "healthy" if self.tracking_enabled else "disabled",
                "total_decisions": len(self.decision_history),
                "total_outcomes": len(self.outcome_history),
                "last_update": self.last_update.isoformat(),
                "memory_usage": f"{len(self.decision_history)} decisions in memory",
                "configuration": {
                    "retention_days": getattr(self, 'retention_days', 30),
                    "track_all_decisions": getattr(self, 'track_all_decisions', True),
                    "outcome_follow_up_hours": getattr(self, 'outcome_follow_up_hours', 24)
                },
                "recent_activity": {
                    "decisions_last_hour": len([
                        d for d in self.decision_history
                        if d.timestamp > datetime.now() - timedelta(hours=1)
                    ]),
                    "outcomes_last_hour": len([
                        o for o in self.outcome_history.values()
                        if o.timestamp > datetime.now() - timedelta(hours=1)
                    ])
                }
            }
            
        except Exception as e:
            logger.error(f"ç²å–ç³»çµ±ç‹€æ…‹å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _analyze_priority_distribution(self, decisions: List[EPLDecisionRecord]) -> Dict[str, int]:
        """åˆ†æå„ªå…ˆç´šåˆ†ä½ˆ"""
        distribution = defaultdict(int)
        for decision in decisions:
            distribution[decision.signal_priority.value] += 1
        return dict(distribution)
    
    def _analyze_decision_type_distribution(self, decisions: List[EPLDecisionRecord]) -> Dict[str, int]:
        """åˆ†ææ±ºç­–é¡å‹åˆ†ä½ˆ"""
        distribution = defaultdict(int)
        for decision in decisions:
            distribution[decision.decision_type.value] += 1
        return dict(distribution)
    
    def _generate_pattern_insights(self, successful_decisions: List, failed_decisions: List) -> List[str]:
        """ç”Ÿæˆæ¨¡å¼æ´å¯Ÿ"""
        insights = []
        
        if successful_decisions and failed_decisions:
            # æˆåŠŸæ±ºç­–çš„å¹³å‡ä¿¡å¿ƒåˆ†æ•¸
            success_confidence = statistics.mean([d.confidence_score for d in successful_decisions])
            failure_confidence = statistics.mean([d.confidence_score for d in failed_decisions])
            
            if success_confidence > failure_confidence + 0.1:
                insights.append(f"æˆåŠŸæ±ºç­–çš„å¹³å‡ä¿¡å¿ƒåˆ†æ•¸ ({success_confidence:.2f}) æ˜é¡¯é«˜æ–¼å¤±æ•—æ±ºç­– ({failure_confidence:.2f})")
            
            # å„ªå…ˆç´šåˆ†æ
            success_critical = len([d for d in successful_decisions if d.signal_priority == SignalPriority.CRITICAL])
            failure_critical = len([d for d in failed_decisions if d.signal_priority == SignalPriority.CRITICAL])
            
            if success_critical > 0 and failure_critical > 0:
                success_critical_rate = success_critical / len(successful_decisions)
                failure_critical_rate = failure_critical / len(failed_decisions)
                
                if success_critical_rate > failure_critical_rate + 0.2:
                    insights.append("CRITICAL å„ªå…ˆç´šæ±ºç­–åœ¨æˆåŠŸæ¡ˆä¾‹ä¸­å æ¯”æ›´é«˜")
        
        return insights if insights else ["éœ€è¦æ›´å¤šæ•¸æ“šä¾†è­˜åˆ¥æ˜ç¢ºæ¨¡å¼"]

# å…¨å±€å¯¦ä¾‹
epl_decision_tracker = EPLDecisionHistoryTracker()

@dataclass
class MarketSnapshot:
    """å¸‚å ´å¿«ç…§æ•¸æ“š"""
    timestamp: datetime
    symbol: str
    price: float
    volume: float
    volatility: float
    market_conditions: Dict[str, Any]
    technical_indicators: Dict[str, float]
    sentiment_score: Optional[float] = None

@dataclass  
class PortfolioState:
    """æŠ•è³‡çµ„åˆç‹€æ…‹"""
    timestamp: datetime
    total_value: float
    available_cash: float
    positions: Dict[str, Dict[str, Any]]
    risk_metrics: Dict[str, float]
    correlation_matrix: Optional[Dict[str, Dict[str, float]]] = None
    exposure_limits: Optional[Dict[str, float]] = None

@dataclass
class ExecutionMetrics:
    """åŸ·è¡ŒæŒ‡æ¨™"""
    decision_id: str
    execution_timestamp: datetime
    planned_price: float
    actual_price: float
    slippage: float
    execution_latency: float
    market_impact: float
    execution_quality_score: float
    fees_and_costs: Dict[str, float]

    async def track_execution_lifecycle(self, decision_id: str, execution_data: Dict[str, Any]) -> bool:
        """è¿½è¹¤åŸ·è¡Œç”Ÿå‘½é€±æœŸ"""
        try:
            # å‰µå»ºåŸ·è¡ŒæŒ‡æ¨™
            execution_metrics = ExecutionMetrics(
                decision_id=decision_id,
                execution_timestamp=datetime.fromisoformat(execution_data.get('timestamp', datetime.now().isoformat())),
                planned_price=execution_data.get('planned_price', 0.0),
                actual_price=execution_data.get('actual_price', 0.0),
                slippage=execution_data.get('slippage', 0.0),
                execution_latency=execution_data.get('latency', 0.0),
                market_impact=execution_data.get('market_impact', 0.0),
                execution_quality_score=execution_data.get('quality_score', 0.0),
                fees_and_costs=execution_data.get('costs', {})
            )
            
            # æ›´æ–°æ±ºç­–è¨˜éŒ„
            decision_record = self._find_decision_by_id(decision_id)
            if decision_record:
                decision_record.execution_details['execution_metrics'] = asdict(execution_metrics)
                logger.info(f"æ›´æ–°åŸ·è¡Œç”Ÿå‘½é€±æœŸ: {decision_id}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"è¿½è¹¤åŸ·è¡Œç”Ÿå‘½é€±æœŸå¤±æ•—: {e}")
            return False
    
    async def capture_market_context(self, symbol: str) -> MarketSnapshot:
        """æ“·å–å¸‚å ´ä¸Šä¸‹æ–‡"""
        try:
            # æ¨¡æ“¬å¸‚å ´æ•¸æ“šæ“·å– (å¯¦éš›æ‡‰è©²å¾å¸‚å ´æ•¸æ“šæºç²å–)
            market_snapshot = MarketSnapshot(
                timestamp=datetime.now(),
                symbol=symbol,
                price=0.0,  # æ‡‰è©²å¾å¯¦éš›æ•¸æ“šæºç²å–
                volume=0.0,
                volatility=0.0,
                market_conditions={"trend": "neutral", "session": "active"},
                technical_indicators={"rsi": 50.0, "macd": 0.0},
                sentiment_score=0.5
            )
            
            logger.info(f"æ“·å–å¸‚å ´ä¸Šä¸‹æ–‡: {symbol}")
            return market_snapshot
            
        except Exception as e:
            logger.error(f"æ“·å–å¸‚å ´ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            # è¿”å›é»˜èªå¿«ç…§
            return MarketSnapshot(
                timestamp=datetime.now(),
                symbol=symbol,
                price=0.0, volume=0.0, volatility=0.0,
                market_conditions={}, technical_indicators={}
            )
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        """é©—è­‰æ•¸æ“šå®Œæ•´æ€§"""
        try:
            integrity_report = {
                "validation_timestamp": datetime.now().isoformat(),
                "total_decisions": len(self.decision_history),
                "total_outcomes": len(self.outcome_history),
                "integrity_checks": {}
            }
            
            # æª¢æŸ¥æ±ºç­–è¨˜éŒ„å®Œæ•´æ€§
            missing_outcomes = 0
            invalid_records = 0
            
            for record in self.decision_history:
                # æª¢æŸ¥å¿…è¦æ¬„ä½
                if not all([record.decision_id, record.symbol, record.timestamp]):
                    invalid_records += 1
                
                # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„çµæœ
                if record.decision_id not in self.outcome_history:
                    missing_outcomes += 1
            
            integrity_report["integrity_checks"] = {
                "invalid_records": invalid_records,
                "missing_outcomes": missing_outcomes,
                "data_consistency": "good" if invalid_records == 0 else "issues_found",
                "outcome_coverage": (len(self.outcome_history) / max(len(self.decision_history), 1)) * 100
            }
            
            logger.info("æ•¸æ“šå®Œæ•´æ€§é©—è­‰å®Œæˆ")
            return integrity_report
            
        except Exception as e:
            logger.error(f"æ•¸æ“šå®Œæ•´æ€§é©—è­‰å¤±æ•—: {e}")
            return {"error": str(e)}

    async def analyze_replacement_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ›¿æ›æ±ºç­–æ¨¡å¼"""
        try:
            replacement_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.REPLACE_POSITION
            ]
            
            if not replacement_decisions:
                return {"message": "æš«ç„¡æ›¿æ›æ±ºç­–æ•¸æ“š"}
            
            # åˆ†ææ›¿æ›é »ç‡
            total_decisions = len(self.decision_history)
            replacement_rate = len(replacement_decisions) / total_decisions
            
            # åˆ†ææ›¿æ›æˆåŠŸç‡
            successful_replacements = 0
            for record in replacement_decisions:
                if record.decision_id in self.outcome_history:
                    outcome = self.outcome_history[record.decision_id]
                    if outcome.success:
                        successful_replacements += 1
            
            replacement_success_rate = successful_replacements / len(replacement_decisions) if replacement_decisions else 0
            
            # åˆ†æä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ
            confidence_scores = [r.confidence_score for r in replacement_decisions]
            avg_confidence = statistics.mean(confidence_scores) if confidence_scores else 0
            
            return {
                "replacement_frequency": {
                    "total_replacements": len(replacement_decisions),
                    "replacement_rate": replacement_rate,
                    "average_confidence": avg_confidence
                },
                "replacement_effectiveness": {
                    "success_rate": replacement_success_rate,
                    "successful_count": successful_replacements,
                    "failed_count": len(replacement_decisions) - successful_replacements
                },
                "insights": self._generate_replacement_insights(replacement_decisions)
            }
            
        except Exception as e:
            logger.error(f"åˆ†ææ›¿æ›æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def analyze_strengthening_patterns(self) -> Dict[str, Any]:
        """åˆ†æå¼·åŒ–æ±ºç­–æ¨¡å¼"""
        try:
            strengthening_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.STRENGTHEN_POSITION
            ]
            
            if not strengthening_decisions:
                return {"message": "æš«ç„¡å¼·åŒ–æ±ºç­–æ•¸æ“š"}
            
            # åŸºæœ¬çµ±è¨ˆ
            total_decisions = len(self.decision_history)
            strengthening_rate = len(strengthening_decisions) / total_decisions
            
            # æˆåŠŸç‡åˆ†æ
            successful_strengthenings = sum(
                1 for record in strengthening_decisions
                if record.decision_id in self.outcome_history and 
                self.outcome_history[record.decision_id].success
            )
            
            success_rate = successful_strengthenings / len(strengthening_decisions) if strengthening_decisions else 0
            
            return {
                "strengthening_frequency": {
                    "total_strengthenings": len(strengthening_decisions),
                    "strengthening_rate": strengthening_rate,
                    "average_confidence": statistics.mean([r.confidence_score for r in strengthening_decisions])
                },
                "strengthening_effectiveness": {
                    "success_rate": success_rate,
                    "successful_count": successful_strengthenings
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†æå¼·åŒ–æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def analyze_new_position_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ–°å€‰ä½æ±ºç­–æ¨¡å¼"""
        try:
            new_position_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.CREATE_NEW_POSITION
            ]
            
            if not new_position_decisions:
                return {"message": "æš«ç„¡æ–°å€‰ä½æ±ºç­–æ•¸æ“š"}
            
            # åŸºæœ¬çµ±è¨ˆ
            creation_rate = len(new_position_decisions) / len(self.decision_history)
            
            # æˆåŠŸç‡åˆ†æ
            successful_creations = sum(
                1 for record in new_position_decisions
                if record.decision_id in self.outcome_history and
                self.outcome_history[record.decision_id].success
            )
            
            success_rate = successful_creations / len(new_position_decisions) if new_position_decisions else 0
            
            return {
                "creation_frequency": {
                    "total_new_positions": len(new_position_decisions),
                    "creation_rate": creation_rate,
                    "average_confidence": statistics.mean([r.confidence_score for r in new_position_decisions])
                },
                "creation_effectiveness": {
                    "success_rate": success_rate,
                    "successful_count": successful_creations
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†ææ–°å€‰ä½æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def analyze_ignore_patterns(self) -> Dict[str, Any]:
        """åˆ†æå¿½ç•¥æ±ºç­–æ¨¡å¼"""
        try:
            ignore_decisions = [
                record for record in self.decision_history
                if record.decision_type == EPLDecisionType.IGNORE_SIGNAL
            ]
            
            if not ignore_decisions:
                return {"message": "æš«ç„¡å¿½ç•¥æ±ºç­–æ•¸æ“š"}
            
            # å¿½ç•¥ç‡åˆ†æ
            ignore_rate = len(ignore_decisions) / len(self.decision_history)
            
            # å¿½ç•¥åŸå› åˆ†æ
            ignore_reasons = defaultdict(int)
            for record in ignore_decisions:
                reason = record.position_context.get('reason', 'unknown')
                ignore_reasons[reason] += 1
            
            return {
                "ignore_frequency": {
                    "total_ignores": len(ignore_decisions),
                    "ignore_rate": ignore_rate,
                    "average_confidence": statistics.mean([r.confidence_score for r in ignore_decisions])
                },
                "ignore_reasons": dict(ignore_reasons),
                "filtering_effectiveness": self._calculate_filtering_effectiveness(ignore_decisions)
            }
            
        except Exception as e:
            logger.error(f"åˆ†æå¿½ç•¥æ¨¡å¼å¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def generate_learning_insights(self) -> Dict[str, Any]:
        """ç”Ÿæˆå­¸ç¿’æ´å¯Ÿ"""
        try:
            if not self.decision_history:
                return {"message": "æš«ç„¡æ±ºç­–æ•¸æ“šç”Ÿæˆå­¸ç¿’æ´å¯Ÿ"}
            
            # æ¨¡å¼è­˜åˆ¥
            successful_patterns = await self._identify_successful_patterns()
            failure_patterns = await self._identify_failure_patterns()
            
            # é©æ‡‰æ€§å­¸ç¿’å»ºè­°
            adaptive_recommendations = self._generate_adaptive_recommendations()
            
            # å›é¥‹æ•´åˆåˆ†æ
            feedback_integration = self._analyze_feedback_integration()
            
            return {
                "pattern_recognition": {
                    "successful_patterns": successful_patterns,
                    "failure_patterns": failure_patterns
                },
                "adaptive_learning": {
                    "recommendations": adaptive_recommendations,
                    "threshold_adjustments": self._suggest_threshold_adjustments()
                },
                "feedback_integration": feedback_integration,
                "learning_summary": self._summarize_learning_insights()
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå­¸ç¿’æ´å¯Ÿå¤±æ•—: {e}")
            return {"error": str(e)}

    async def integrate_phase1_signals(self, signal_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆ Phase1 ä¿¡è™Ÿæ•¸æ“š"""
        try:
            # æå– Phase1 ä¿¡è™Ÿå€™é¸è³‡æ–™
            signal_candidate = signal_data.get('signal_candidate', {})
            
            integrated_data = {
                "original_signal_candidate": signal_candidate,
                "signal_quality_metrics": {
                    "technical_strength": signal_candidate.get('technical_strength', 0.5),
                    "market_timing": signal_candidate.get('market_timing', 0.5),
                    "source_reliability": signal_candidate.get('source_reliability', 0.5)
                },
                "market_context": await self.capture_market_context(signal_candidate.get('symbol', 'UNKNOWN'))
            }
            
            logger.info("Phase1 ä¿¡è™Ÿæ•¸æ“šæ•´åˆå®Œæˆ")
            return integrated_data
            
        except Exception as e:
            logger.error(f"æ•´åˆ Phase1 ä¿¡è™Ÿå¤±æ•—: {e}")
            return {}
    
    async def integrate_phase2_evaluation(self, evaluation_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆ Phase2 é è©•ä¼°çµæœ"""
        try:
            # æå– Phase2 é è©•ä¼°çµæœ
            pre_evaluation = evaluation_data.get('pre_evaluation_result', {})
            
            integrated_data = {
                "pre_evaluation_result": pre_evaluation,
                "embedded_scoring": pre_evaluation.get('embedded_scoring', {}),
                "correlation_analysis": pre_evaluation.get('correlation_analysis', {}),
                "portfolio_state": self._extract_portfolio_state(evaluation_data)
            }
            
            logger.info("Phase2 é è©•ä¼°æ•¸æ“šæ•´åˆå®Œæˆ")
            return integrated_data
            
        except Exception as e:
            logger.error(f"æ•´åˆ Phase2 è©•ä¼°å¤±æ•—: {e}")
            return {}
    
    async def integrate_phase3_execution(self, execution_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆ Phase3 åŸ·è¡Œæ•¸æ“š"""
        try:
            # æå–åŸ·è¡Œç›¸é—œæ•¸æ“š
            execution_result = execution_data.get('execution_result', {})
            
            integrated_data = {
                "execution_initiation": {
                    "execution_timestamp": execution_result.get('timestamp'),
                    "execution_latency": execution_result.get('latency'),
                    "market_conditions_at_execution": execution_result.get('market_conditions'),
                    "slippage_measurement": execution_result.get('slippage')
                },
                "execution_monitoring": {
                    "position_establishment": execution_result.get('position_changes'),
                    "risk_parameter_application": execution_result.get('risk_parameters'),
                    "portfolio_impact": execution_result.get('portfolio_impact'),
                    "correlation_effects": execution_result.get('correlation_impact')
                }
            }
            
            logger.info("Phase3 åŸ·è¡Œæ•¸æ“šæ•´åˆå®Œæˆ")
            return integrated_data
            
        except Exception as e:
            logger.error(f"æ•´åˆ Phase3 åŸ·è¡Œå¤±æ•—: {e}")
            return {}
    
    async def export_phase4_analytics(self) -> Dict[str, Any]:
        """åŒ¯å‡º Phase4 åˆ†æçµæœ"""
        try:
            # åŒ¯å‡ºæ‰€æœ‰åˆ†æçµæœä¾›å…¶ä»–ç³»çµ±ä½¿ç”¨
            analytics_export = {
                "export_timestamp": datetime.now().isoformat(),
                "decision_analytics": await self.get_comprehensive_decision_analysis(),
                "performance_metrics": await self.get_performance_metrics(),
                "pattern_insights": {
                    "replacement_patterns": await self.analyze_replacement_patterns(),
                    "strengthening_patterns": await self.analyze_strengthening_patterns(),
                    "new_position_patterns": await self.analyze_new_position_patterns(),
                    "ignore_patterns": await self.analyze_ignore_patterns()
                },
                "learning_insights": await self.generate_learning_insights(),
                "system_status": await self.get_system_status()
            }
            
            logger.info("Phase4 åˆ†æçµæœåŒ¯å‡ºå®Œæˆ")
            return analytics_export
            
        except Exception as e:
            logger.error(f"åŒ¯å‡º Phase4 åˆ†æå¤±æ•—: {e}")
            return {"error": str(e)}

    def _generate_replacement_insights(self, replacement_decisions: List) -> List[str]:
        """ç”Ÿæˆæ›¿æ›æ´å¯Ÿ"""
        insights = []
        
        if len(replacement_decisions) > 10:
            avg_confidence = statistics.mean([r.confidence_score for r in replacement_decisions])
            if avg_confidence > 0.8:
                insights.append("æ›¿æ›æ±ºç­–æ™®éå…·æœ‰é«˜ä¿¡å¿ƒåˆ†æ•¸")
            elif avg_confidence < 0.6:
                insights.append("æ›¿æ›æ±ºç­–ä¿¡å¿ƒåˆ†æ•¸åä½ï¼Œå»ºè­°æª¢è¨æ¨™æº–")
        
        return insights if insights else ["éœ€è¦æ›´å¤šæ•¸æ“šç”Ÿæˆæ´å¯Ÿ"]
    
    def _extract_portfolio_state(self, evaluation_data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æŠ•è³‡çµ„åˆç‹€æ…‹"""
        return {
            "current_positions": evaluation_data.get('portfolio', {}).get('positions', {}),
            "risk_metrics": evaluation_data.get('portfolio', {}).get('risk_metrics', {}),
            "available_cash": evaluation_data.get('portfolio', {}).get('cash', 0.0),
            "total_value": evaluation_data.get('portfolio', {}).get('total_value', 0.0)
        }
    
    def _calculate_filtering_effectiveness(self, ignore_decisions: List) -> Dict[str, Any]:
        """è¨ˆç®—éæ¿¾æ•ˆæœ"""
        if not ignore_decisions:
            return {"effectiveness": "no_data"}
        
        # è¨ˆç®—ä½å“è³ªä¿¡è™Ÿéæ¿¾ç‡
        low_quality_ignores = len([d for d in ignore_decisions if d.confidence_score < 0.5])
        filtering_rate = low_quality_ignores / len(ignore_decisions)
        
        return {
            "low_quality_filtering_rate": filtering_rate,
            "total_filtered": len(ignore_decisions),
            "effectiveness_score": min(filtering_rate * 2, 1.0)  # æ­¸ä¸€åŒ–åˆ°0-1
        }
    
    async def _identify_successful_patterns(self) -> Dict[str, Any]:
        """è­˜åˆ¥æˆåŠŸæ¨¡å¼"""
        successful_decisions = [
            record for record in self.decision_history
            if record.decision_id in self.outcome_history and
            self.outcome_history[record.decision_id].success
        ]
        
        if not successful_decisions:
            return {"patterns": "insufficient_data"}
        
        # åˆ†ææˆåŠŸæ±ºç­–çš„å…±åŒç‰¹å¾µ
        avg_confidence = statistics.mean([d.confidence_score for d in successful_decisions])
        priority_distribution = defaultdict(int)
        
        for decision in successful_decisions:
            priority_distribution[decision.signal_priority.value] += 1
        
        return {
            "average_confidence": avg_confidence,
            "priority_distribution": dict(priority_distribution),
            "pattern_count": len(successful_decisions)
        }
    
    async def _identify_failure_patterns(self) -> Dict[str, Any]:
        """è­˜åˆ¥å¤±æ•—æ¨¡å¼"""
        failed_decisions = [
            record for record in self.decision_history
            if record.decision_id in self.outcome_history and
            not self.outcome_history[record.decision_id].success
        ]
        
        if not failed_decisions:
            return {"patterns": "insufficient_data"}
        
        # åˆ†æå¤±æ•—æ±ºç­–çš„å…±åŒç‰¹å¾µ
        avg_confidence = statistics.mean([d.confidence_score for d in failed_decisions])
        priority_distribution = defaultdict(int)
        
        for decision in failed_decisions:
            priority_distribution[decision.signal_priority.value] += 1
        
        return {
            "average_confidence": avg_confidence,
            "priority_distribution": dict(priority_distribution),
            "pattern_count": len(failed_decisions)
        }
    
    def _generate_adaptive_recommendations(self) -> List[str]:
        """ç”Ÿæˆé©æ‡‰æ€§å»ºè­°"""
        recommendations = []
        
        if self.success_rates:
            overall_success_rate = sum(
                sum(successes) / len(successes) for successes in self.success_rates.values() if successes
            ) / len(self.success_rates)
            
            if overall_success_rate < 0.7:
                recommendations.append("æ•´é«”æˆåŠŸç‡åä½ï¼Œå»ºè­°èª¿æ•´æ±ºç­–é–¾å€¼")
            elif overall_success_rate > 0.9:
                recommendations.append("æˆåŠŸç‡æ¥µé«˜ï¼Œå¯è€ƒæ…®é™ä½æ±ºç­–é–€æª»ä»¥å¢åŠ æ©Ÿæœƒ")
        
        return recommendations if recommendations else ["ç³»çµ±è¡¨ç¾ç©©å®šï¼Œç¹¼çºŒç›£æ§"]
    
    def _suggest_threshold_adjustments(self) -> Dict[str, float]:
        """å»ºè­°é–¾å€¼èª¿æ•´"""
        adjustments = {}
        
        # åŸºæ–¼æˆåŠŸç‡å»ºè­°ç½®ä¿¡åº¦é–¾å€¼èª¿æ•´
        if self.success_rates:
            for decision_type, successes in self.success_rates.items():
                if successes:
                    success_rate = sum(successes) / len(successes)
                    if success_rate < 0.6:
                        adjustments[f"{decision_type}_confidence_threshold"] = 0.1  # æé«˜é–¾å€¼
                    elif success_rate > 0.9:
                        adjustments[f"{decision_type}_confidence_threshold"] = -0.05  # é™ä½é–¾å€¼
        
        return adjustments
    
    def _analyze_feedback_integration(self) -> Dict[str, Any]:
        """åˆ†æå›é¥‹æ•´åˆ"""
        return {
            "total_outcomes_tracked": len(self.outcome_history),
            "feedback_coverage": len(self.outcome_history) / max(len(self.decision_history), 1),
            "learning_effectiveness": "good" if len(self.outcome_history) > 0 else "limited",
            "improvement_areas": ["å¢åŠ çµæœè¿½è¹¤è¦†è“‹ç‡", "åŠ å¼·å³æ™‚å›é¥‹æ©Ÿåˆ¶"]
        }
    
    def _summarize_learning_insights(self) -> str:
        """ç¸½çµå­¸ç¿’æ´å¯Ÿ"""
        if not self.decision_history:
            return "æš«ç„¡è¶³å¤ æ•¸æ“šç”Ÿæˆå­¸ç¿’ç¸½çµ"
        
        total_decisions = len(self.decision_history)
        total_outcomes = len(self.outcome_history)
        
        return f"å·²è¿½è¹¤ {total_decisions} å€‹æ±ºç­–ï¼Œå…¶ä¸­ {total_outcomes} å€‹æœ‰çµæœå›é¥‹ã€‚ç³»çµ±æ­£åœ¨æŒçºŒå­¸ç¿’å’Œå„ªåŒ–æ±ºç­–æ¨¡å¼ã€‚"
