"""
ğŸ¯ Phase4 EPL Decision History Tracking
======================================

EPLæ±ºç­–æ­·å²è¿½è¹¤å¯¦ç¾ - åŸºæ–¼é…ç½®é©…å‹•çš„æ±ºç­–éç¨‹è¨˜éŒ„èˆ‡åˆ†æ
èˆ‡ epl_decision_history_tracking_config.json é…ç½®æ–‡ä»¶å°æ‡‰
"""

import asyncio
import logging
import json
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import defaultdict, deque
from enum import Enum
import statistics

logger = logging.getLogger(__name__)

class EPLDecisionType(Enum):
    """EPLæ±ºç­–é¡å‹"""
    REPLACE_POSITION = "REPLACE_POSITION"
    STRENGTHEN_POSITION = "STRENGTHEN_POSITION"
    CREATE_NEW_POSITION = "CREATE_NEW_POSITION"
    IGNORE_SIGNAL = "IGNORE_SIGNAL"

class SignalPriority(Enum):
    """ä¿¡è™Ÿå„ªå…ˆç´š"""
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"

@dataclass
class EPLDecisionRecord:
    """EPLæ±ºç­–è¨˜éŒ„"""
    decision_id: str
    timestamp: datetime
    symbol: str
    signal_priority: SignalPriority
    decision_type: EPLDecisionType
    confidence_score: float
    risk_assessment: Dict[str, float]
    position_context: Dict[str, Any]
    execution_details: Dict[str, Any]
    outcome_tracking: Optional[Dict[str, Any]] = None

@dataclass
class DecisionOutcome:
    """æ±ºç­–çµæœè¿½è¹¤"""
    decision_id: str
    execution_success: bool
    performance_metrics: Dict[str, float]
    actual_vs_expected: Dict[str, float]
    lessons_learned: List[str]
    timestamp: datetime

class EPLDecisionHistoryTracker:
    """EPLæ±ºç­–æ­·å²è¿½è¹¤ç³»çµ±"""
    
    def __init__(self):
        # è¼‰å…¥é…ç½®æ–‡ä»¶
        self.config = self._load_config()
        
        # æ±ºç­–æ­·å²å­˜å„²
        self.decision_history: deque = deque(maxlen=50000)  # ä¿ç•™æœ€è¿‘50000å€‹æ±ºç­–
        self.outcome_history: Dict[str, DecisionOutcome] = {}
        
        # çµ±è¨ˆæ•¸æ“š
        self.decision_type_stats: Dict[str, int] = defaultdict(int)
        self.priority_stats: Dict[str, int] = defaultdict(int)
        self.success_rates: Dict[str, List[bool]] = defaultdict(list)
        
        # æ€§èƒ½è¿½è¹¤
        self.tracking_enabled = True
        self.last_update = datetime.now()
        
        # åˆå§‹åŒ–è¿½è¹¤ç³»çµ±
        self._initialize_tracking()
        
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            config_path = Path(__file__).parent / "epl_decision_history_tracking_config.json"
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥EPLè¿½è¹¤é…ç½®å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ç²å–é»˜èªé…ç½®"""
        return {
            "PHASE4_EPL_DECISION_HISTORY_TRACKING": {
                "decision_tracking": {
                    "track_all_decisions": True,
                    "retention_period_days": 30,
                    "outcome_follow_up_hours": 24
                },
                "performance_analysis": {
                    "success_rate_calculation": True,
                    "confidence_correlation_analysis": True,
                    "decision_pattern_recognition": True
                }
            }
        }
    
    def _initialize_tracking(self):
        """åˆå§‹åŒ–è¿½è¹¤ç³»çµ±"""
        logger.info("åˆå§‹åŒ–EPLæ±ºç­–æ­·å²è¿½è¹¤ç³»çµ±")
        
        # æ¸…ç†éèˆŠçš„è¨˜éŒ„
        self._cleanup_old_records()
        
        # è¨­ç½®è¿½è¹¤åƒæ•¸
        config = self.config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        tracking_config = config.get("decision_tracking", {})
        
        self.track_all_decisions = tracking_config.get("track_all_decisions", True)
        self.retention_days = tracking_config.get("retention_period_days", 30)
        self.outcome_follow_up_hours = tracking_config.get("outcome_follow_up_hours", 24)
        
    def _cleanup_old_records(self):
        """æ¸…ç†éèˆŠçš„è¨˜éŒ„"""
        cutoff_time = datetime.now() - timedelta(days=self.retention_days)
        
        # æ¸…ç†æ±ºç­–æ­·å²
        self.decision_history = deque(
            [record for record in self.decision_history 
             if record.timestamp > cutoff_time],
            maxlen=50000
        )
        
        # æ¸…ç†çµæœæ­·å²
        old_decision_ids = [
            decision_id for decision_id, outcome in self.outcome_history.items()
            if outcome.timestamp < cutoff_time
        ]
        for decision_id in old_decision_ids:
            del self.outcome_history[decision_id]
    
    async def record_epl_decision(self, decision_data: Dict[str, Any]) -> str:
        """è¨˜éŒ„EPLæ±ºç­–"""
        try:
            # ç”Ÿæˆæ±ºç­–ID
            decision_id = f"epl_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{decision_data.get('symbol', 'UNK')}"
            
            # å‰µå»ºæ±ºç­–è¨˜éŒ„
            decision_record = EPLDecisionRecord(
                decision_id=decision_id,
                timestamp=datetime.fromisoformat(decision_data.get('timestamp', datetime.now().isoformat())),
                symbol=decision_data.get('symbol', 'UNKNOWN'),
                signal_priority=SignalPriority(decision_data.get('priority', 'MEDIUM')),
                decision_type=EPLDecisionType(decision_data.get('decision_type', 'IGNORE_SIGNAL')),
                confidence_score=float(decision_data.get('confidence_score', 0.5)),
                risk_assessment=decision_data.get('risk_assessment', {}),
                position_context=decision_data.get('position_context', {}),
                execution_details=decision_data.get('execution_details', {})
            )
            
            # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
            self.decision_history.append(decision_record)
            
            # æ›´æ–°çµ±è¨ˆ
            self._update_decision_statistics(decision_record)
            
            self.last_update = datetime.now()
            
            logger.info(f"è¨˜éŒ„EPLæ±ºç­–: {decision_id}, é¡å‹: {decision_record.decision_type.value}")
            return decision_id
            
        except Exception as e:
            logger.error(f"è¨˜éŒ„EPLæ±ºç­–å¤±æ•—: {e}")
            return ""
    
    def _update_decision_statistics(self, record: EPLDecisionRecord):
        """æ›´æ–°æ±ºç­–çµ±è¨ˆ"""
        self.decision_type_stats[record.decision_type.value] += 1
        self.priority_stats[record.signal_priority.value] += 1
    
    async def record_decision_outcome(self, decision_id: str, outcome_data: Dict[str, Any]) -> bool:
        """è¨˜éŒ„æ±ºç­–çµæœ"""
        try:
            outcome = DecisionOutcome(
                decision_id=decision_id,
                execution_success=outcome_data.get('execution_success', False),
                performance_metrics=outcome_data.get('performance_metrics', {}),
                actual_vs_expected=outcome_data.get('actual_vs_expected', {}),
                lessons_learned=outcome_data.get('lessons_learned', []),
                timestamp=datetime.now()
            )
            
            self.outcome_history[decision_id] = outcome
            
            # æ›´æ–°æˆåŠŸç‡çµ±è¨ˆ
            decision_record = self._find_decision_by_id(decision_id)
            if decision_record:
                decision_type = decision_record.decision_type.value
                self.success_rates[decision_type].append(outcome.execution_success)
                
                # é™åˆ¶æˆåŠŸç‡æ­·å²è¨˜éŒ„é•·åº¦
                if len(self.success_rates[decision_type]) > 1000:
                    self.success_rates[decision_type].pop(0)
            
            logger.info(f"è¨˜éŒ„æ±ºç­–çµæœ: {decision_id}, æˆåŠŸ: {outcome.execution_success}")
            return True
            
        except Exception as e:
            logger.error(f"è¨˜éŒ„æ±ºç­–çµæœå¤±æ•—: {e}")
            return False
    
    def _find_decision_by_id(self, decision_id: str) -> Optional[EPLDecisionRecord]:
        """æ ¹æ“šIDæŸ¥æ‰¾æ±ºç­–è¨˜éŒ„"""
        for record in self.decision_history:
            if record.decision_id == decision_id:
                return record
        return None
    
    async def get_comprehensive_decision_analysis(self) -> Dict[str, Any]:
        """ç²å–ç¶œåˆæ±ºç­–åˆ†æ"""
        try:
            current_time = datetime.now()
            
            if not self.decision_history:
                return self._get_empty_analysis()
            
            # åŸºæœ¬çµ±è¨ˆ
            total_decisions = len(self.decision_history)
            decisions_with_outcomes = len(self.outcome_history)
            
            # æ±ºç­–é¡å‹åˆ†æ
            decision_type_analysis = self._analyze_decision_types()
            
            # å„ªå…ˆç´šåˆ†æ
            priority_analysis = self._analyze_priority_patterns()
            
            # æˆåŠŸç‡åˆ†æ
            success_rate_analysis = self._analyze_success_rates()
            
            # ç½®ä¿¡åº¦ç›¸é—œæ€§åˆ†æ
            confidence_analysis = self._analyze_confidence_correlation()
            
            # æ™‚é–“æ¨¡å¼åˆ†æ
            temporal_analysis = self._analyze_temporal_patterns()
            
            # æ€§èƒ½æŒ‡æ¨™
            performance_metrics = self._calculate_performance_metrics()
            
            # é¢¨éšªè©•ä¼°åˆ†æ
            risk_analysis = self._analyze_risk_patterns()
            
            return {
                "analysis_metadata": {
                    "generated_at": current_time.isoformat(),
                    "total_decisions_tracked": total_decisions,
                    "decisions_with_outcomes": decisions_with_outcomes,
                    "tracking_period": {
                        "start": min(r.timestamp for r in self.decision_history).isoformat(),
                        "end": max(r.timestamp for r in self.decision_history).isoformat()
                    },
                    "last_update": self.last_update.isoformat()
                },
                "decision_type_distribution": decision_type_analysis,
                "priority_level_insights": priority_analysis,
                "success_rate_analysis": success_rate_analysis,
                "confidence_correlation_analysis": confidence_analysis,
                "temporal_decision_patterns": temporal_analysis,
                "performance_benchmarks": performance_metrics,
                "risk_assessment_insights": risk_analysis,
                "decision_quality_trends": self._analyze_decision_quality_trends(),
                "recommendations": self._generate_recommendations()
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¶œåˆæ±ºç­–åˆ†æå¤±æ•—: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}
    
    def _get_empty_analysis(self) -> Dict[str, Any]:
        """ç²å–ç©ºåˆ†ææ•¸æ“š"""
        return {
            "analysis_metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_decisions_tracked": 0,
                "status": "no_data"
            },
            "message": "æ²’æœ‰è¶³å¤ çš„æ±ºç­–æ•¸æ“šé€²è¡Œåˆ†æ"
        }
    
    def _analyze_decision_types(self) -> Dict[str, Any]:
        """åˆ†ææ±ºç­–é¡å‹"""
        type_distribution = dict(self.decision_type_stats)
        total = sum(type_distribution.values())
        
        if total == 0:
            return {"distribution": {}, "trends": []}
        
        # è¨ˆç®—ç™¾åˆ†æ¯”
        type_percentages = {
            decision_type: (count / total) * 100
            for decision_type, count in type_distribution.items()
        }
        
        # åˆ†æè¶¨å‹¢ï¼ˆæœ€è¿‘24å°æ™‚ vs ä¹‹å‰ï¼‰
        cutoff_time = datetime.now() - timedelta(hours=24)
        recent_decisions = [r for r in self.decision_history if r.timestamp > cutoff_time]
        older_decisions = [r for r in self.decision_history if r.timestamp <= cutoff_time]
        
        recent_type_stats = defaultdict(int)
        older_type_stats = defaultdict(int)
        
        for record in recent_decisions:
            recent_type_stats[record.decision_type.value] += 1
        
        for record in older_decisions:
            older_type_stats[record.decision_type.value] += 1
        
        # è¨ˆç®—è¶¨å‹¢è®ŠåŒ–
        trend_changes = {}
        for decision_type in type_distribution.keys():
            recent_ratio = recent_type_stats[decision_type] / max(len(recent_decisions), 1)
            older_ratio = older_type_stats[decision_type] / max(len(older_decisions), 1)
            
            trend_changes[decision_type] = {
                "recent_ratio": recent_ratio,
                "previous_ratio": older_ratio,
                "change_percentage": ((recent_ratio - older_ratio) / max(older_ratio, 0.01)) * 100
            }
        
        return {
            "distribution": type_distribution,
            "percentages": type_percentages,
            "trend_analysis": trend_changes,
            "dominant_decision_type": max(type_distribution.items(), key=lambda x: x[1])[0] if type_distribution else None
        }
    
    def _analyze_priority_patterns(self) -> Dict[str, Any]:
        """åˆ†æå„ªå…ˆç´šæ¨¡å¼"""
        priority_distribution = dict(self.priority_stats)
        
        # æŒ‰å„ªå…ˆç´šåˆ†ææ±ºç­–é¡å‹
        priority_decision_matrix = defaultdict(lambda: defaultdict(int))
        
        for record in self.decision_history:
            priority_decision_matrix[record.signal_priority.value][record.decision_type.value] += 1
        
        # è¨ˆç®—å„å„ªå…ˆç´šçš„è™•ç†æ•ˆç‡
        priority_efficiency = {}
        for priority, decisions in priority_decision_matrix.items():
            total = sum(decisions.values())
            efficiency_score = 0
            
            if total > 0:
                # é«˜æ•ˆæ±ºç­–æ¬Šé‡: CREATE_NEW_POSITION=1.0, STRENGTHEN=0.8, REPLACE=0.6, IGNORE=0.2
                weights = {
                    "CREATE_NEW_POSITION": 1.0,
                    "STRENGTHEN_POSITION": 0.8,
                    "REPLACE_POSITION": 0.6,
                    "IGNORE_SIGNAL": 0.2
                }
                
                weighted_sum = sum(decisions[dt] * weights.get(dt, 0.5) for dt in decisions)
                efficiency_score = weighted_sum / total
            
            priority_efficiency[priority] = efficiency_score
        
        return {
            "distribution": priority_distribution,
            "decision_matrix": dict(priority_decision_matrix),
            "processing_efficiency": priority_efficiency,
            "priority_ranking": sorted(priority_efficiency.items(), key=lambda x: x[1], reverse=True)
        }
    
    def _analyze_success_rates(self) -> Dict[str, Any]:
        """åˆ†ææˆåŠŸç‡"""
        success_rate_summary = {}
        
        for decision_type, successes in self.success_rates.items():
            if successes:
                success_rate = sum(successes) / len(successes)
                recent_successes = successes[-10:] if len(successes) >= 10 else successes
                recent_success_rate = sum(recent_successes) / len(recent_successes)
                
                success_rate_summary[decision_type] = {
                    "overall_success_rate": success_rate,
                    "recent_success_rate": recent_success_rate,
                    "total_attempts": len(successes),
                    "trend": "improving" if recent_success_rate > success_rate else "declining" if recent_success_rate < success_rate else "stable"
                }
        
        # è¨ˆç®—æ•´é«”æˆåŠŸç‡
        all_successes = []
        for successes in self.success_rates.values():
            all_successes.extend(successes)
        
        overall_success_rate = sum(all_successes) / len(all_successes) if all_successes else 0
        
        return {
            "by_decision_type": success_rate_summary,
            "overall_success_rate": overall_success_rate,
            "total_tracked_outcomes": len(all_successes),
            "best_performing_decision": max(success_rate_summary.items(), key=lambda x: x[1]["overall_success_rate"])[0] if success_rate_summary else None
        }
    
    def _analyze_confidence_correlation(self) -> Dict[str, Any]:
        """åˆ†æç½®ä¿¡åº¦ç›¸é—œæ€§"""
        decisions_with_outcomes = []
        
        for record in self.decision_history:
            if record.decision_id in self.outcome_history:
                outcome = self.outcome_history[record.decision_id]
                decisions_with_outcomes.append({
                    "confidence": record.confidence_score,
                    "success": outcome.execution_success,
                    "decision_type": record.decision_type.value
                })
        
        if not decisions_with_outcomes:
            return {"correlation_analysis": "insufficient_data"}
        
        # æŒ‰ç½®ä¿¡åº¦å€é–“åˆ†ææˆåŠŸç‡
        confidence_bins = {
            "low_confidence": [d for d in decisions_with_outcomes if d["confidence"] < 0.5],
            "medium_confidence": [d for d in decisions_with_outcomes if 0.5 <= d["confidence"] < 0.8],
            "high_confidence": [d for d in decisions_with_outcomes if d["confidence"] >= 0.8]
        }
        
        bin_analysis = {}
        for bin_name, decisions in confidence_bins.items():
            if decisions:
                success_rate = sum(d["success"] for d in decisions) / len(decisions)
                avg_confidence = statistics.mean(d["confidence"] for d in decisions)
                
                bin_analysis[bin_name] = {
                    "count": len(decisions),
                    "success_rate": success_rate,
                    "average_confidence": avg_confidence
                }
        
        # è¨ˆç®—ç›¸é—œæ€§ä¿‚æ•¸ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        confidences = [d["confidence"] for d in decisions_with_outcomes]
        successes = [1 if d["success"] else 0 for d in decisions_with_outcomes]
        
        correlation_coefficient = 0
        if len(confidences) > 1:
            try:
                mean_conf = statistics.mean(confidences)
                mean_succ = statistics.mean(successes)
                
                numerator = sum((c - mean_conf) * (s - mean_succ) for c, s in zip(confidences, successes))
                denominator = (sum((c - mean_conf) ** 2 for c in confidences) * sum((s - mean_succ) ** 2 for s in successes)) ** 0.5
                
                if denominator > 0:
                    correlation_coefficient = numerator / denominator
            except:
                pass
        
        return {
            "confidence_bin_analysis": bin_analysis,
            "correlation_coefficient": correlation_coefficient,
            "correlation_strength": self._interpret_correlation(correlation_coefficient),
            "total_analyzed": len(decisions_with_outcomes)
        }
    
    def _interpret_correlation(self, coefficient: float) -> str:
        """è§£é‡‹ç›¸é—œæ€§ä¿‚æ•¸"""
        abs_coeff = abs(coefficient)
        if abs_coeff >= 0.8:
            return "very_strong"
        elif abs_coeff >= 0.6:
            return "strong"
        elif abs_coeff >= 0.4:
            return "moderate"
        elif abs_coeff >= 0.2:
            return "weak"
        else:
            return "very_weak"
    
    def _analyze_temporal_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ™‚é–“æ¨¡å¼"""
        # æŒ‰å°æ™‚åˆ†ææ±ºç­–æ¨¡å¼
        hourly_patterns = defaultdict(lambda: defaultdict(int))
        
        for record in self.decision_history:
            hour = record.timestamp.hour
            hourly_patterns[hour][record.decision_type.value] += 1
        
        # æ‰¾å‡ºæœ€æ´»èºçš„æ™‚æ®µ
        hourly_activity = {hour: sum(decisions.values()) for hour, decisions in hourly_patterns.items()}
        peak_hours = sorted(hourly_activity.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # æŒ‰å·¥ä½œæ—¥åˆ†æ
        weekday_patterns = defaultdict(lambda: defaultdict(int))
        
        for record in self.decision_history:
            weekday = record.timestamp.weekday()  # 0=Monday
            weekday_patterns[weekday][record.decision_type.value] += 1
        
        weekday_names = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        weekday_analysis = {}
        
        for day_num, decisions in weekday_patterns.items():
            day_name = weekday_names[day_num]
            total_decisions = sum(decisions.values())
            
            weekday_analysis[day_name] = {
                "total_decisions": total_decisions,
                "decision_distribution": dict(decisions),
                "activity_score": total_decisions / max(sum(hourly_activity.values()), 1)
            }
        
        return {
            "hourly_patterns": dict(hourly_patterns),
            "peak_activity_hours": [f"{hour:02d}:00" for hour, _ in peak_hours],
            "weekday_analysis": weekday_analysis,
            "busiest_day": max(weekday_analysis.items(), key=lambda x: x[1]["total_decisions"])[0] if weekday_analysis else None
        }
    
    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """è¨ˆç®—æ€§èƒ½æŒ‡æ¨™"""
        if not self.decision_history:
            return {}
        
        # æ±ºç­–å»¶é²åˆ†æ
        decision_latencies = []
        for record in self.decision_history:
            # æ¨¡æ“¬æ±ºç­–å»¶é²ï¼ˆå¯¦éš›æ‡‰è©²å¾åŸ·è¡Œè©³æƒ…ä¸­ç²å–ï¼‰
            latency = record.execution_details.get('decision_latency', 0)
            if latency > 0:
                decision_latencies.append(latency)
        
        # ç½®ä¿¡åº¦åˆ†æ
        confidence_scores = [record.confidence_score for record in self.decision_history]
        
        # é¢¨éšªè©•ä¼°åˆ†æ
        risk_scores = []
        for record in self.decision_history:
            overall_risk = record.risk_assessment.get('overall_risk', 0.5)
            risk_scores.append(overall_risk)
        
        return {
            "decision_latency_metrics": {
                "average_latency": statistics.mean(decision_latencies) if decision_latencies else 0,
                "median_latency": statistics.median(decision_latencies) if decision_latencies else 0,
                "max_latency": max(decision_latencies) if decision_latencies else 0
            },
            "confidence_metrics": {
                "average_confidence": statistics.mean(confidence_scores),
                "confidence_distribution": self._create_confidence_distribution(confidence_scores),
                "low_confidence_rate": len([c for c in confidence_scores if c < 0.5]) / len(confidence_scores)
            },
            "risk_metrics": {
                "average_risk_score": statistics.mean(risk_scores),
                "high_risk_decisions": len([r for r in risk_scores if r > 0.7]) / len(risk_scores),
                "risk_distribution": self._create_risk_distribution(risk_scores)
            },
            "throughput_metrics": {
                "decisions_per_hour": len(self.decision_history) / max((datetime.now() - min(r.timestamp for r in self.decision_history)).total_seconds() / 3600, 1),
                "total_decisions": len(self.decision_history),
                "decisions_with_outcomes": len(self.outcome_history)
            }
        }
    
    def _create_confidence_distribution(self, scores: List[float]) -> Dict[str, int]:
        """å‰µå»ºç½®ä¿¡åº¦åˆ†ä½ˆ"""
        distribution = {"0.0-0.2": 0, "0.2-0.4": 0, "0.4-0.6": 0, "0.6-0.8": 0, "0.8-1.0": 0}
        
        for score in scores:
            if score < 0.2:
                distribution["0.0-0.2"] += 1
            elif score < 0.4:
                distribution["0.2-0.4"] += 1
            elif score < 0.6:
                distribution["0.4-0.6"] += 1
            elif score < 0.8:
                distribution["0.6-0.8"] += 1
            else:
                distribution["0.8-1.0"] += 1
        
        return distribution
    
    def _create_risk_distribution(self, scores: List[float]) -> Dict[str, int]:
        """å‰µå»ºé¢¨éšªåˆ†ä½ˆ"""
        distribution = {"Low": 0, "Medium": 0, "High": 0}
        
        for score in scores:
            if score < 0.3:
                distribution["Low"] += 1
            elif score < 0.7:
                distribution["Medium"] += 1
            else:
                distribution["High"] += 1
        
        return distribution
    
    def _analyze_risk_patterns(self) -> Dict[str, Any]:
        """åˆ†æé¢¨éšªæ¨¡å¼"""
        risk_by_decision_type = defaultdict(list)
        risk_by_priority = defaultdict(list)
        
        for record in self.decision_history:
            overall_risk = record.risk_assessment.get('overall_risk', 0.5)
            risk_by_decision_type[record.decision_type.value].append(overall_risk)
            risk_by_priority[record.signal_priority.value].append(overall_risk)
        
        # è¨ˆç®—å„æ±ºç­–é¡å‹çš„å¹³å‡é¢¨éšª
        risk_by_type_summary = {}
        for decision_type, risks in risk_by_decision_type.items():
            if risks:
                risk_by_type_summary[decision_type] = {
                    "average_risk": statistics.mean(risks),
                    "max_risk": max(risks),
                    "risk_variance": statistics.variance(risks) if len(risks) > 1 else 0
                }
        
        # è¨ˆç®—å„å„ªå…ˆç´šçš„å¹³å‡é¢¨éšª
        risk_by_priority_summary = {}
        for priority, risks in risk_by_priority.items():
            if risks:
                risk_by_priority_summary[priority] = {
                    "average_risk": statistics.mean(risks),
                    "max_risk": max(risks),
                    "risk_variance": statistics.variance(risks) if len(risks) > 1 else 0
                }
        
        return {
            "risk_by_decision_type": risk_by_type_summary,
            "risk_by_priority": risk_by_priority_summary,
            "riskiest_decision_type": max(risk_by_type_summary.items(), key=lambda x: x[1]["average_risk"])[0] if risk_by_type_summary else None,
            "safest_decision_type": min(risk_by_type_summary.items(), key=lambda x: x[1]["average_risk"])[0] if risk_by_type_summary else None
        }
    
    def _analyze_decision_quality_trends(self) -> Dict[str, Any]:
        """åˆ†ææ±ºç­–è³ªé‡è¶¨å‹¢"""
        # æŒ‰æ™‚é–“é †åºåˆ†ææ±ºç­–è³ªé‡
        sorted_decisions = sorted(self.decision_history, key=lambda x: x.timestamp)
        
        if len(sorted_decisions) < 10:
            return {"trend_analysis": "insufficient_data"}
        
        # å°‡æ±ºç­–åˆ†æˆæ™‚é–“æ®µ
        total_decisions = len(sorted_decisions)
        segment_size = max(total_decisions // 5, 10)  # åˆ†æˆ5æ®µï¼Œæ¯æ®µè‡³å°‘10å€‹æ±ºç­–
        
        segments = []
        for i in range(0, total_decisions, segment_size):
            segment = sorted_decisions[i:i + segment_size]
            if len(segment) >= 5:  # ç¢ºä¿æ¯æ®µæœ‰è¶³å¤ çš„æ•¸æ“š
                segments.append(segment)
        
        trend_data = []
        for i, segment in enumerate(segments):
            avg_confidence = statistics.mean(record.confidence_score for record in segment)
            avg_risk = statistics.mean(record.risk_assessment.get('overall_risk', 0.5) for record in segment)
            
            # è¨ˆç®—è©²æ®µçš„æˆåŠŸç‡
            segment_outcomes = [
                self.outcome_history[record.decision_id].execution_success
                for record in segment
                if record.decision_id in self.outcome_history
            ]
            
            success_rate = statistics.mean(segment_outcomes) if segment_outcomes else None
            
            trend_data.append({
                "segment": i + 1,
                "time_range": {
                    "start": segment[0].timestamp.isoformat(),
                    "end": segment[-1].timestamp.isoformat()
                },
                "average_confidence": avg_confidence,
                "average_risk": avg_risk,
                "success_rate": success_rate,
                "decision_count": len(segment)
            })
        
        return {
            "trend_segments": trend_data,
            "quality_improvement": self._calculate_quality_improvement(trend_data),
            "trend_summary": self._summarize_trends(trend_data)
        }
    
    def _calculate_quality_improvement(self, trend_data: List[Dict]) -> Dict[str, Any]:
        """è¨ˆç®—è³ªé‡æ”¹å–„"""
        if len(trend_data) < 2:
            return {"improvement_analysis": "insufficient_data"}
        
        first_segment = trend_data[0]
        last_segment = trend_data[-1]
        
        confidence_change = last_segment["average_confidence"] - first_segment["average_confidence"]
        risk_change = last_segment["average_risk"] - first_segment["average_risk"]
        
        success_rate_change = None
        if first_segment["success_rate"] is not None and last_segment["success_rate"] is not None:
            success_rate_change = last_segment["success_rate"] - first_segment["success_rate"]
        
        return {
            "confidence_improvement": confidence_change,
            "risk_reduction": -risk_change,  # è² å€¼è¡¨ç¤ºé¢¨éšªé™ä½
            "success_rate_improvement": success_rate_change,
            "overall_quality_trend": "improving" if confidence_change > 0 and risk_change < 0 else "declining" if confidence_change < 0 and risk_change > 0 else "mixed"
        }
    
    def _summarize_trends(self, trend_data: List[Dict]) -> str:
        """ç¸½çµè¶¨å‹¢"""
        if len(trend_data) < 2:
            return "æ•¸æ“šä¸è¶³ä»¥åˆ†æè¶¨å‹¢"
        
        # åˆ†æç½®ä¿¡åº¦è¶¨å‹¢
        confidence_values = [segment["average_confidence"] for segment in trend_data]
        confidence_trend = "ä¸Šå‡" if confidence_values[-1] > confidence_values[0] else "ä¸‹é™"
        
        # åˆ†æé¢¨éšªè¶¨å‹¢
        risk_values = [segment["average_risk"] for segment in trend_data]
        risk_trend = "ä¸Šå‡" if risk_values[-1] > risk_values[0] else "ä¸‹é™"
        
        return f"æ±ºç­–ç½®ä¿¡åº¦å‘ˆç¾{confidence_trend}è¶¨å‹¢ï¼Œé¢¨éšªæ°´å¹³{risk_trend}ã€‚"
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æˆåŠŸç‡åˆ†æç”Ÿæˆå»ºè­°
        if self.success_rates:
            best_performing = max(self.success_rates.items(), key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 0)
            worst_performing = min(self.success_rates.items(), key=lambda x: sum(x[1]) / len(x[1]) if x[1] else 1)
            
            best_rate = sum(best_performing[1]) / len(best_performing[1]) if best_performing[1] else 0
            worst_rate = sum(worst_performing[1]) / len(worst_performing[1]) if worst_performing[1] else 0
            
            if best_rate > 0.8:
                recommendations.append(f"æ±ºç­–é¡å‹ {best_performing[0]} è¡¨ç¾å„ªç•°ï¼ˆæˆåŠŸç‡ {best_rate:.1%}ï¼‰ï¼Œå»ºè­°å¢åŠ æ­¤é¡æ±ºç­–çš„ä½¿ç”¨é »ç‡")
            
            if worst_rate < 0.6:
                recommendations.append(f"æ±ºç­–é¡å‹ {worst_performing[0]} è¡¨ç¾ä¸ä½³ï¼ˆæˆåŠŸç‡ {worst_rate:.1%}ï¼‰ï¼Œå»ºè­°æª¢è¨æ±ºç­–æ¨™æº–")
        
        # åŸºæ–¼ç½®ä¿¡åº¦åˆ†æç”Ÿæˆå»ºè­°
        if self.decision_history:
            avg_confidence = statistics.mean(record.confidence_score for record in self.decision_history)
            
            if avg_confidence < 0.6:
                recommendations.append("æ•´é«”æ±ºç­–ç½®ä¿¡åº¦åä½ï¼Œå»ºè­°å„ªåŒ–ä¿¡è™Ÿè³ªé‡ç¯©é¸æ¨™æº–")
            elif avg_confidence > 0.8:
                recommendations.append("æ±ºç­–ç½®ä¿¡åº¦è¡¨ç¾è‰¯å¥½ï¼Œå¯è€ƒæ…®æé«˜æ±ºç­–é »ç‡")
        
        # åŸºæ–¼é¢¨éšªåˆ†æç”Ÿæˆå»ºè­°
        high_risk_decisions = len([
            record for record in self.decision_history
            if record.risk_assessment.get('overall_risk', 0.5) > 0.7
        ])
        
        if high_risk_decisions / len(self.decision_history) > 0.3:
            recommendations.append("é«˜é¢¨éšªæ±ºç­–æ¯”ä¾‹åé«˜ï¼Œå»ºè­°åŠ å¼·é¢¨éšªæ§åˆ¶æ©Ÿåˆ¶")
        
        return recommendations if recommendations else ["ç³»çµ±é‹è¡Œæ­£å¸¸ï¼Œæš«ç„¡ç‰¹æ®Šå»ºè­°"]
    
    async def get_recent_decisions(self, hours: int = 24) -> List[Dict[str, Any]]:
        """ç²å–æœ€è¿‘çš„æ±ºç­–è¨˜éŒ„"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_decisions = [
            record for record in self.decision_history
            if record.timestamp > cutoff_time
        ]
        
        return [
            {
                "decision_id": record.decision_id,
                "timestamp": record.timestamp.isoformat(),
                "symbol": record.symbol,
                "priority": record.signal_priority.value,
                "decision_type": record.decision_type.value,
                "confidence_score": record.confidence_score,
                "risk_assessment": record.risk_assessment,
                "has_outcome": record.decision_id in self.outcome_history
            }
            for record in sorted(recent_decisions, key=lambda x: x.timestamp, reverse=True)
        ]
    
    def get_decision_by_id(self, decision_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ“šIDç²å–æ±ºç­–è©³æƒ…"""
        record = self._find_decision_by_id(decision_id)
        if not record:
            return None
        
        result = {
            "decision_record": asdict(record),
            "outcome": None
        }
        
        if decision_id in self.outcome_history:
            result["outcome"] = asdict(self.outcome_history[decision_id])
        
        return result

# å…¨å±€å¯¦ä¾‹
epl_decision_tracker = EPLDecisionHistoryTracker()
