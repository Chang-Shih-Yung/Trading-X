"""
ğŸ“Š Phase4 Signal Processing Statistics
=====================================

ä¿¡è™Ÿè™•ç†çµ±è¨ˆå¯¦ç¾ - åŸºæ–¼é…ç½®é©…å‹•çš„å¤šç¶­åº¦ä¿¡è™Ÿåˆ†æ
èˆ‡ signal_processing_statistics_config.json é…ç½®æ–‡ä»¶å°æ‡‰
"""

import asyncio
import logging
import json
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
from collections import defaultdict, deque
import statistics

logger = logging.getLogger(__name__)

@dataclass
class SignalMetrics:
    """ä¿¡è™ŸæŒ‡æ¨™æ•¸æ“š"""
    timestamp: datetime
    symbol: str
    priority: str
    quality_score: float
    processing_latency: float
    source: str
    phase1_duration: float
    phase2_duration: float
    phase3_duration: float

@dataclass
class StatisticalSummary:
    """çµ±è¨ˆæ‘˜è¦"""
    count: int
    mean: float
    median: float
    std_dev: float
    min_value: float
    max_value: float
    percentile_95: float
    percentile_99: float

class SignalProcessingStatistics:
    """ä¿¡è™Ÿè™•ç†çµ±è¨ˆç³»çµ±"""
    
    def __init__(self):
        # è¼‰å…¥é…ç½®æ–‡ä»¶
        self.config = self._load_config()
        
        # çµ±è¨ˆæ•¸æ“šå­˜å„²
        self.signal_history: deque = deque(maxlen=10000)  # ä¿ç•™æœ€è¿‘10000å€‹ä¿¡è™Ÿ
        self.quality_distribution: Dict[str, int] = defaultdict(int)
        self.priority_distribution: Dict[str, int] = defaultdict(int)
        self.source_distribution: Dict[str, int] = defaultdict(int)
        
        # æ™‚é–“çª—å£çµ±è¨ˆ
        self.hourly_stats: Dict[str, List] = defaultdict(list)
        self.daily_stats: Dict[str, List] = defaultdict(list)
        
        # æ€§èƒ½ç›£æ§
        self.last_update = datetime.now()
        self.statistics_enabled = True
        
        # åˆå§‹åŒ–çµ±è¨ˆæ¨¡å¡Š
        self._initialize_statistics()
        
        # é©—è­‰é…ç½®
        if not self.validate_configuration():
            logger.warning("ä½¿ç”¨é»˜èªé…ç½®")
            self.config = self._get_default_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            config_path = Path(__file__).parent / "signal_processing_statistics_config.json"
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥çµ±è¨ˆé…ç½®å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ç²å–é»˜èªé…ç½®"""
        return {
            "PHASE4_SIGNAL_PROCESSING_STATISTICS": {
                "statistical_analysis": {
                    "quality_distribution_tracking": {
                        "bin_count": 10,
                        "quality_thresholds": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                        "trend_analysis_enabled": True,
                        "quality_target": 0.8
                    },
                    "priority_level_analytics": {
                        "categories": ["CRITICAL", "HIGH", "MEDIUM", "LOW"],
                        "success_rate_tracking": True,
                        "processing_time_correlation": True
                    },
                    "processing_time_analysis": {
                        "percentiles": [50, 90, 95, 99],
                        "phases": {
                            "phase1_signal_generation": {"target_latency_ms": 200},
                            "phase2_pre_evaluation": {"target_latency_ms": 15},
                            "phase3_execution_policy": {"target_latency_ms": 450}
                        },
                        "total_target_latency_ms": 800
                    },
                    "source_performance_comparison": {
                        "enable_benchmarking": True,
                        "reliability_tracking": True,
                        "supported_sources": ["binance", "coinbase", "kraken", "huobi", "unknown"]
                    }
                },
                "real_time_monitoring": {
                    "update_intervals": {"comprehensive_statistics_seconds": 5},
                    "data_retention": {"signal_history_max_count": 10000}
                }
            }
        }
    
    def _initialize_statistics(self):
        """åˆå§‹åŒ–çµ±è¨ˆæ¨¡å¡Š"""
        logger.info("åˆå§‹åŒ–ä¿¡è™Ÿè™•ç†çµ±è¨ˆç³»çµ±")
        
        # æ¸…ç†èˆŠæ•¸æ“šï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._cleanup_old_data()
        
        # è¨­ç½®çµ±è¨ˆé–“éš”
        self.update_interval = 5  # 5ç§’æ›´æ–°ä¸€æ¬¡çµ±è¨ˆ
        
    def validate_configuration(self) -> bool:
        """é©—è­‰é…ç½®å®Œæ•´æ€§"""
        try:
            required_sections = [
                "statistical_analysis",
                "real_time_monitoring"
            ]
            
            stats_config = self.config.get("PHASE4_SIGNAL_PROCESSING_STATISTICS", {})
            
            for section in required_sections:
                if section not in stats_config:
                    logger.warning(f"é…ç½®ç¼ºå°‘å¿…è¦éƒ¨åˆ†: {section}")
                    return False
            
            logger.info("é…ç½®é©—è­‰é€šé")
            return True
            
        except Exception as e:
            logger.error(f"é…ç½®é©—è­‰å¤±æ•—: {e}")
            return False
    
    def get_config_value(self, key_path: str, default_value: Any = None) -> Any:
        """å®‰å…¨ç²å–é…ç½®å€¼"""
        try:
            keys = key_path.split('.')
            current = self.config
            
            for key in keys:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    return default_value
            
            return current
            
        except Exception:
            return default_value
        
    def _cleanup_old_data(self):
        """æ¸…ç†éèˆŠçš„æ•¸æ“š"""
        cutoff_time = datetime.now() - timedelta(days=7)  # ä¿ç•™7å¤©æ•¸æ“š
        
        # æ¸…ç†æ­·å²æ•¸æ“š
        self.signal_history = deque(
            [signal for signal in self.signal_history 
             if signal.timestamp > cutoff_time],
            maxlen=10000
        )
        
    async def record_signal_metrics(self, signal_data: Dict[str, Any]) -> bool:
        """è¨˜éŒ„ä¿¡è™ŸæŒ‡æ¨™"""
        try:
            # å‰µå»ºä¿¡è™ŸæŒ‡æ¨™å°è±¡
            metrics = SignalMetrics(
                timestamp=datetime.fromisoformat(signal_data.get('timestamp', datetime.now().isoformat())),
                symbol=signal_data.get('symbol', 'UNKNOWN'),
                priority=signal_data.get('priority', 'MEDIUM'),
                quality_score=float(signal_data.get('quality_score', 0.5)),
                processing_latency=float(signal_data.get('total_latency', 0)),
                source=signal_data.get('source', 'unknown'),
                phase1_duration=float(signal_data.get('phase1_duration', 0)),
                phase2_duration=float(signal_data.get('phase2_duration', 0)),
                phase3_duration=float(signal_data.get('phase3_duration', 0))
            )
            
            # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
            self.signal_history.append(metrics)
            
            # æ›´æ–°åˆ†ä½ˆçµ±è¨ˆ
            self._update_distributions(metrics)
            
            # æ›´æ–°æ™‚é–“çª—å£çµ±è¨ˆ
            self._update_time_window_stats(metrics)
            
            self.last_update = datetime.now()
            return True
            
        except Exception as e:
            logger.error(f"è¨˜éŒ„ä¿¡è™ŸæŒ‡æ¨™å¤±æ•—: {e}")
            return False
    
    def _update_distributions(self, metrics: SignalMetrics):
        """æ›´æ–°åˆ†ä½ˆçµ±è¨ˆ"""
        # è³ªé‡åˆ†ä½ˆï¼ˆåˆ†æˆ10å€‹å€é–“ï¼‰
        quality_bin = min(int(metrics.quality_score * 10), 9)
        self.quality_distribution[f"bin_{quality_bin}"] += 1
        
        # å„ªå…ˆç´šåˆ†ä½ˆ
        self.priority_distribution[metrics.priority] += 1
        
        # ä¾†æºåˆ†ä½ˆ
        self.source_distribution[metrics.source] += 1
    
    def _update_time_window_stats(self, metrics: SignalMetrics):
        """æ›´æ–°æ™‚é–“çª—å£çµ±è¨ˆ"""
        current_hour = metrics.timestamp.replace(minute=0, second=0, microsecond=0)
        current_day = metrics.timestamp.replace(hour=0, minute=0, second=0, microsecond=0)
        
        # å°æ™‚çµ±è¨ˆ
        hour_key = current_hour.isoformat()
        self.hourly_stats[hour_key].append({
            'quality_score': metrics.quality_score,
            'processing_latency': metrics.processing_latency,
            'priority': metrics.priority
        })
        
        # æ—¥çµ±è¨ˆ
        day_key = current_day.isoformat()
        self.daily_stats[day_key].append({
            'quality_score': metrics.quality_score,
            'processing_latency': metrics.processing_latency,
            'priority': metrics.priority
        })
        
        # é™åˆ¶è¨˜éŒ„æ•¸é‡
        if len(self.hourly_stats) > 48:  # ä¿ç•™48å°æ™‚
            oldest_hour = min(self.hourly_stats.keys())
            del self.hourly_stats[oldest_hour]
            
        if len(self.daily_stats) > 30:  # ä¿ç•™30å¤©
            oldest_day = min(self.daily_stats.keys())
            del self.daily_stats[oldest_day]
    
    async def get_comprehensive_statistics(self) -> Dict[str, Any]:
        """ç²å–ç¶œåˆçµ±è¨ˆæ•¸æ“š"""
        try:
            current_time = datetime.now()
            
            # åŸºæœ¬çµ±è¨ˆ
            total_signals = len(self.signal_history)
            if total_signals == 0:
                return self._get_empty_statistics()
            
            # è³ªé‡åˆ†æ•¸çµ±è¨ˆ
            quality_scores = [signal.quality_score for signal in self.signal_history]
            quality_stats = self._calculate_statistical_summary(quality_scores)
            
            # è™•ç†å»¶é²çµ±è¨ˆ
            latencies = [signal.processing_latency for signal in self.signal_history]
            latency_stats = self._calculate_statistical_summary(latencies)
            
            # å„éšæ®µå»¶é²çµ±è¨ˆ
            phase1_latencies = [signal.phase1_duration for signal in self.signal_history]
            phase2_latencies = [signal.phase2_duration for signal in self.signal_history]
            phase3_latencies = [signal.phase3_duration for signal in self.signal_history]
            
            # æœ€è¿‘24å°æ™‚è¶¨å‹¢
            recent_trend = self._calculate_recent_trends()
            
            # æ€§èƒ½åŸºæº–æ¸¬è©¦
            performance_benchmarks = self._calculate_performance_benchmarks()
            
            return {
                "statistics_metadata": {
                    "generated_at": current_time.isoformat(),
                    "total_signals_analyzed": total_signals,
                    "time_range": {
                        "earliest": min(signal.timestamp for signal in self.signal_history).isoformat(),
                        "latest": max(signal.timestamp for signal in self.signal_history).isoformat()
                    },
                    "last_update": self.last_update.isoformat()
                },
                "quality_distribution_analysis": {
                    "overall_statistics": asdict(quality_stats),
                    "distribution_bins": dict(self.quality_distribution),
                    "quality_trends": recent_trend.get("quality_trend", [])
                },
                "priority_level_analytics": {
                    "distribution": dict(self.priority_distribution),
                    "processing_time_by_priority": self._get_latency_by_priority(),
                    "success_rate_by_priority": self._get_success_rate_by_priority()
                },
                "processing_time_analysis": {
                    "overall_latency": asdict(latency_stats),
                    "phase_breakdown": {
                        "phase1_signal_generation": asdict(self._calculate_statistical_summary(phase1_latencies)),
                        "phase2_pre_evaluation": asdict(self._calculate_statistical_summary(phase2_latencies)),
                        "phase3_execution_policy": asdict(self._calculate_statistical_summary(phase3_latencies))
                    },
                    "latency_trends": recent_trend.get("latency_trend", [])
                },
                "source_performance_comparison": {
                    "source_distribution": dict(self.source_distribution),
                    "performance_by_source": self._get_performance_by_source(),
                    "reliability_metrics": self._get_source_reliability()
                },
                "temporal_analysis": {
                    "hourly_patterns": self._analyze_hourly_patterns(),
                    "daily_patterns": self._analyze_daily_patterns(),
                    "peak_performance_windows": self._identify_peak_windows()
                },
                "performance_benchmarks": performance_benchmarks
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¶œåˆçµ±è¨ˆå¤±æ•—: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}
    
    def _calculate_statistical_summary(self, values: List[float]) -> StatisticalSummary:
        """è¨ˆç®—çµ±è¨ˆæ‘˜è¦"""
        if not values:
            return StatisticalSummary(0, 0, 0, 0, 0, 0, 0, 0)
        
        sorted_values = sorted(values)
        count = len(values)
        
        return StatisticalSummary(
            count=count,
            mean=statistics.mean(values),
            median=statistics.median(values),
            std_dev=statistics.stdev(values) if count > 1 else 0,
            min_value=min(values),
            max_value=max(values),
            percentile_95=sorted_values[int(0.95 * count)] if count > 0 else 0,
            percentile_99=sorted_values[int(0.99 * count)] if count > 0 else 0
        )
    
    def _get_empty_statistics(self) -> Dict[str, Any]:
        """ç²å–ç©ºçµ±è¨ˆæ•¸æ“š"""
        return {
            "statistics_metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_signals_analyzed": 0,
                "status": "no_data"
            },
            "message": "æ²’æœ‰è¶³å¤ çš„ä¿¡è™Ÿæ•¸æ“šç”Ÿæˆçµ±è¨ˆ"
        }
    
    def _calculate_recent_trends(self) -> Dict[str, List]:
        """è¨ˆç®—æœ€è¿‘è¶¨å‹¢"""
        # æœ€è¿‘24å°æ™‚çš„è¶¨å‹¢
        cutoff_time = datetime.now() - timedelta(hours=24)
        recent_signals = [s for s in self.signal_history if s.timestamp > cutoff_time]
        
        # æŒ‰å°æ™‚åˆ†çµ„
        hourly_buckets = defaultdict(list)
        for signal in recent_signals:
            hour_key = signal.timestamp.replace(minute=0, second=0, microsecond=0)
            hourly_buckets[hour_key].append(signal)
        
        # ç”Ÿæˆè¶¨å‹¢æ•¸æ“š
        quality_trend = []
        latency_trend = []
        
        for hour in sorted(hourly_buckets.keys()):
            signals = hourly_buckets[hour]
            if signals:
                avg_quality = statistics.mean([s.quality_score for s in signals])
                avg_latency = statistics.mean([s.processing_latency for s in signals])
                
                quality_trend.append({
                    "timestamp": hour.isoformat(),
                    "average_quality": round(avg_quality, 3),
                    "signal_count": len(signals)
                })
                
                latency_trend.append({
                    "timestamp": hour.isoformat(),
                    "average_latency": round(avg_latency, 2),
                    "signal_count": len(signals)
                })
        
        return {
            "quality_trend": quality_trend,
            "latency_trend": latency_trend
        }
    
    def _get_latency_by_priority(self) -> Dict[str, float]:
        """ç²å–å„å„ªå…ˆç´šçš„å»¶é²"""
        priority_latencies = defaultdict(list)
        
        for signal in self.signal_history:
            priority_latencies[signal.priority].append(signal.processing_latency)
        
        return {
            priority: statistics.mean(latencies) if latencies else 0
            for priority, latencies in priority_latencies.items()
        }
    
    def _get_success_rate_by_priority(self) -> Dict[str, float]:
        """ç²å–å„å„ªå…ˆç´šçš„æˆåŠŸç‡ï¼ˆæ¨¡æ“¬ï¼‰"""
        priority_counts = defaultdict(int)
        
        for signal in self.signal_history:
            priority_counts[signal.priority] += 1
        
        # æ¨¡æ“¬æˆåŠŸç‡ï¼ˆå¯¦éš›æ‡‰è©²å¾å¯¦éš›çµæœè¨ˆç®—ï¼‰
        success_rates = {
            "CRITICAL": 0.95,
            "HIGH": 0.88,
            "MEDIUM": 0.82,
            "LOW": 0.75
        }
        
        return {
            priority: success_rates.get(priority, 0.8)
            for priority in priority_counts.keys()
        }
    
    def _get_performance_by_source(self) -> Dict[str, Dict[str, float]]:
        """ç²å–å„ä¾†æºçš„æ€§èƒ½"""
        source_metrics = defaultdict(lambda: {"latencies": [], "qualities": []})
        
        for signal in self.signal_history:
            source_metrics[signal.source]["latencies"].append(signal.processing_latency)
            source_metrics[signal.source]["qualities"].append(signal.quality_score)
        
        result = {}
        for source, metrics in source_metrics.items():
            result[source] = {
                "average_latency": statistics.mean(metrics["latencies"]) if metrics["latencies"] else 0,
                "average_quality": statistics.mean(metrics["qualities"]) if metrics["qualities"] else 0,
                "signal_count": len(metrics["latencies"])
            }
        
        return result
    
    def _get_source_reliability(self) -> Dict[str, float]:
        """ç²å–ä¾†æºå¯é æ€§ï¼ˆæ¨¡æ“¬ï¼‰"""
        reliability_scores = {
            "binance": 0.98,
            "coinbase": 0.96,
            "kraken": 0.94,
            "huobi": 0.92,
            "unknown": 0.80
        }
        
        return {
            source: reliability_scores.get(source, 0.85)
            for source in self.source_distribution.keys()
        }
    
    def _analyze_hourly_patterns(self) -> Dict[str, Any]:
        """åˆ†æå°æ™‚æ¨¡å¼"""
        hourly_performance = defaultdict(lambda: {"count": 0, "quality_sum": 0, "latency_sum": 0})
        
        for signal in self.signal_history:
            hour = signal.timestamp.hour
            hourly_performance[hour]["count"] += 1
            hourly_performance[hour]["quality_sum"] += signal.quality_score
            hourly_performance[hour]["latency_sum"] += signal.processing_latency
        
        patterns = {}
        for hour, data in hourly_performance.items():
            if data["count"] > 0:
                patterns[f"hour_{hour:02d}"] = {
                    "signal_count": data["count"],
                    "average_quality": data["quality_sum"] / data["count"],
                    "average_latency": data["latency_sum"] / data["count"]
                }
        
        return patterns
    
    def _analyze_daily_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ—¥æ¨¡å¼"""
        daily_performance = defaultdict(lambda: {"count": 0, "quality_sum": 0, "latency_sum": 0})
        
        for signal in self.signal_history:
            weekday = signal.timestamp.weekday()  # 0=Monday, 6=Sunday
            daily_performance[weekday]["count"] += 1
            daily_performance[weekday]["quality_sum"] += signal.quality_score
            daily_performance[weekday]["latency_sum"] += signal.processing_latency
        
        weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        patterns = {}
        
        for day_num, data in daily_performance.items():
            if data["count"] > 0:
                day_name = weekdays[day_num]
                patterns[day_name] = {
                    "signal_count": data["count"],
                    "average_quality": data["quality_sum"] / data["count"],
                    "average_latency": data["latency_sum"] / data["count"]
                }
        
        return patterns
    
    def _identify_peak_windows(self) -> List[Dict[str, Any]]:
        """è­˜åˆ¥é«˜å³°æ™‚æ®µ"""
        # åŸºæ–¼æ­·å²æ•¸æ“šè­˜åˆ¥ä¿¡è™Ÿé‡å’Œè³ªé‡çš„é«˜å³°æ™‚æ®µ
        hourly_stats = self._analyze_hourly_patterns()
        
        if not hourly_stats:
            return []
        
        # æ‰¾å‡ºä¿¡è™Ÿé‡æœ€å¤šçš„æ™‚æ®µ
        peak_volume_hours = sorted(
            hourly_stats.items(),
            key=lambda x: x[1]["signal_count"],
            reverse=True
        )[:3]
        
        # æ‰¾å‡ºè³ªé‡æœ€é«˜çš„æ™‚æ®µ
        peak_quality_hours = sorted(
            hourly_stats.items(),
            key=lambda x: x[1]["average_quality"],
            reverse=True
        )[:3]
        
        return [
            {
                "type": "peak_volume",
                "hours": [hour for hour, _ in peak_volume_hours],
                "description": "ä¿¡è™Ÿé‡æœ€é«˜çš„æ™‚æ®µ"
            },
            {
                "type": "peak_quality",
                "hours": [hour for hour, _ in peak_quality_hours],
                "description": "ä¿¡è™Ÿè³ªé‡æœ€é«˜çš„æ™‚æ®µ"
            }
        ]
    
    def _calculate_performance_benchmarks(self) -> Dict[str, Any]:
        """è¨ˆç®—æ€§èƒ½åŸºæº–"""
        if not self.signal_history:
            return {}
        
        # è¨ˆç®—å„ç¨®åŸºæº–æŒ‡æ¨™
        all_latencies = [s.processing_latency for s in self.signal_history]
        all_qualities = [s.quality_score for s in self.signal_history]
        
        return {
            "throughput_benchmarks": {
                "signals_per_minute": len(self.signal_history) / max((datetime.now() - min(s.timestamp for s in self.signal_history)).total_seconds() / 60, 1),
                "target_throughput": 60,  # ç›®æ¨™æ¯åˆ†é˜60å€‹ä¿¡è™Ÿ
                "efficiency_ratio": min(1.0, (len(self.signal_history) / 60) / max(1, (datetime.now() - min(s.timestamp for s in self.signal_history)).total_seconds() / 60))
            },
            "quality_benchmarks": {
                "average_quality": statistics.mean(all_qualities),
                "target_quality": 0.8,
                "quality_achievement_rate": len([q for q in all_qualities if q >= 0.8]) / len(all_qualities)
            },
            "latency_benchmarks": {
                "average_latency": statistics.mean(all_latencies),
                "target_latency": 500,  # ç›®æ¨™500ms
                "latency_compliance_rate": len([l for l in all_latencies if l <= 500]) / len(all_latencies)
            }
        }
    
    async def get_real_time_metrics(self) -> Dict[str, Any]:
        """ç²å–å¯¦æ™‚æŒ‡æ¨™"""
        current_time = datetime.now()
        recent_cutoff = current_time - timedelta(minutes=5)
        
        recent_signals = [s for s in self.signal_history if s.timestamp > recent_cutoff]
        
        if not recent_signals:
            return {
                "real_time_status": "no_recent_data",
                "timestamp": current_time.isoformat()
            }
        
        return {
            "real_time_status": "active",
            "timestamp": current_time.isoformat(),
            "recent_5min_metrics": {
                "signal_count": len(recent_signals),
                "average_quality": statistics.mean([s.quality_score for s in recent_signals]),
                "average_latency": statistics.mean([s.processing_latency for s in recent_signals]),
                "priority_distribution": {
                    priority: len([s for s in recent_signals if s.priority == priority])
                    for priority in set(s.priority for s in recent_signals)
                }
            },
            "processing_rate": {
                "signals_per_minute": len(recent_signals),
                "target_rate": 12,  # ç›®æ¨™æ¯5åˆ†é˜60å€‹ä¿¡è™Ÿ
                "performance_ratio": len(recent_signals) / 12
            }
        }

# å…¨å±€å¯¦ä¾‹
signal_statistics = SignalProcessingStatistics()
