#!/usr/bin/env python3
"""
ğŸ—„ï¸ Enhanced Signal Database - Priority 3 Support
å¢å¼·ç‰ˆä¿¡è™Ÿè³‡æ–™åº« - æ”¯æ´å„ªå…ˆç´š3æ™‚é–“æ¡†æ¶æ„ŸçŸ¥

åŠŸèƒ½ç‰¹è‰²ï¼š
- æ”¯æ´æ™‚é–“æ¡†æ¶å¢å¼·ä¿¡è™Ÿå­˜å„²
- ä¸‰ç¶­å­¸ç¿’æ¬Šé‡è¿½è¹¤ï¼ˆæ™‚é–“è¡°æ¸› Ã— å¹£ç¨® Ã— æ™‚é–“æ¡†æ¶ï¼‰
- è·¨æ™‚é–“æ¡†æ¶æŸ¥è©¢å’Œåˆ†æ
- ç”¢å“ç´šæ•¸æ“šå®Œæ•´æ€§ä¿è­‰
"""

import asyncio
import aiosqlite
import json
import os
import tempfile
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import logging

# å°å…¥å„ªå…ˆç´š3å¢å¼·ä¿¡è™Ÿ
try:
    from ..priority3_timeframe_learning.timeframe_enhanced_signal import (
        TimeFrameEnhancedSignal, TimeFrameConsensus
    )
    ENHANCED_SIGNAL_AVAILABLE = True
except ImportError:
    ENHANCED_SIGNAL_AVAILABLE = False
    TimeFrameEnhancedSignal = None
    TimeFrameConsensus = None

logger = logging.getLogger(__name__)

class SignalStatus(Enum):
    """ä¿¡è™Ÿç‹€æ…‹æšèˆ‰"""
    PENDING = "PENDING"          # ç­‰å¾…
    EXECUTED = "EXECUTED"        # å·²åŸ·è¡Œ
    SUCCESS = "SUCCESS"          # æˆåŠŸ
    FAILED = "FAILED"            # å¤±æ•—
    CANCELLED = "CANCELLED"      # å–æ¶ˆ

class EnhancedSignalDatabase:
    """å¢å¼·ç‰ˆä¿¡è™Ÿè³‡æ–™åº« - æ”¯æ´å„ªå…ˆç´š3"""
    
    def __init__(self, db_path: Optional[str] = None):
        """åˆå§‹åŒ–å¢å¼·ç‰ˆä¿¡è™Ÿè³‡æ–™åº«"""
        if db_path is None:
            # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨æª”æ¡ˆç›¸å°è·¯å¾‘ï¼Œé¿å…ç¡¬ç·¨ç¢¼
            current_file_dir = os.path.dirname(os.path.abspath(__file__))
            # å¾ç•¶å‰æª”æ¡ˆè·¯å¾‘æ¨å°å‡ºå°ˆæ¡ˆæ ¹ç›®éŒ„
            # enhanced_signal_database.py ä½æ–¼ X/backend/phase2_adaptive_learning/priority3_timeframe_learning/
            # éœ€è¦å›åˆ° X/ ç›®éŒ„
            x_dir = os.path.join(current_file_dir, "..", "..", "..", "..")
            x_dir = os.path.normpath(x_dir)  # æ­£è¦åŒ–è·¯å¾‘
            
            # ç¢ºä¿ databases ç›®éŒ„åœ¨ X/ ä¸‹
            databases_dir = os.path.join(x_dir, "X", "databases")
            project_db_path = os.path.join(databases_dir, "learning_records.db")
            
            # æª¢æŸ¥æ¨™æº–è³‡æ–™åº«è·¯å¾‘æ˜¯å¦å­˜åœ¨å’Œå¯å¯«
            try:
                # ç¢ºä¿ç›®éŒ„å­˜åœ¨
                os.makedirs(databases_dir, exist_ok=True)
                
                # æ¸¬è©¦å¯«å…¥æ¬Šé™
                test_file = project_db_path + ".test"
                with open(test_file, 'w') as f:
                    f.write("test")
                os.remove(test_file)
                
                db_path = project_db_path
                logger.info(f"âœ… ä½¿ç”¨å­¸ç¿’è¨˜éŒ„è³‡æ–™åº«: {db_path}")
            except (OSError, PermissionError) as e:
                # å¦‚æœæ¨™æº–è·¯å¾‘ä¸å¯ç”¨ï¼Œä½¿ç”¨è‡¨æ™‚ç›®éŒ„
                logger.warning(f"âš ï¸ æ¨™æº–è³‡æ–™åº«è·¯å¾‘ä¸å¯ç”¨: {e}")
                import tempfile
                db_path = os.path.join(tempfile.gettempdir(), "learning_records.db")
        
        self.db_path = str(db_path)
        self.connection: Optional[aiosqlite.Connection] = None
        
        logger.info(f"ğŸ—„ï¸ åˆå§‹åŒ–å¢å¼·ç‰ˆä¿¡è™Ÿè³‡æ–™åº«: {self.db_path}")
        
        if not ENHANCED_SIGNAL_AVAILABLE:
            logger.error("âŒ å„ªå…ˆç´š3å¢å¼·ä¿¡è™Ÿæ¨¡çµ„ä¸å¯ç”¨")
            raise ImportError("ç„¡æ³•å°å…¥TimeFrameEnhancedSignal")
    
    def _safe_json_serialize(self, obj):
        """
        å®‰å…¨çš„ JSON åºåˆ—åŒ–ï¼Œè™•ç† SignalType å’Œå…¶ä»–ç‰¹æ®Šå°è±¡
        """
        def convert_obj(item):
            # è™•ç† SignalType æšèˆ‰
            if hasattr(item, 'value'):  # æšèˆ‰å°è±¡
                return item.value
            elif hasattr(item, '__dict__'):  # è‡ªå®šç¾©å°è±¡
                return str(item)
            return item
        
        def recursive_convert(data):
            if isinstance(data, dict):
                return {k: recursive_convert(v) for k, v in data.items()}
            elif isinstance(data, list):
                return [recursive_convert(item) for item in data]
            elif isinstance(data, tuple):
                return [recursive_convert(item) for item in data]
            else:
                return convert_obj(data)
        
        try:
            cleaned_obj = recursive_convert(obj)
            return json.dumps(cleaned_obj)
        except Exception as e:
            logger.warning(f"JSON åºåˆ—åŒ–å¤±æ•—ï¼Œä½¿ç”¨ç©ºå°è±¡: {e}")
            return json.dumps({})
    
    async def initialize(self):
        """åˆå§‹åŒ–è³‡æ–™åº«çµæ§‹"""
        max_retries = 3
        retry_delay = 0.5
        
        for attempt in range(max_retries):
            try:
                # ç¢ºä¿è³‡æ–™åº«æª”æ¡ˆç›®éŒ„å­˜åœ¨
                import os
                import asyncio
                db_dir = os.path.dirname(self.db_path)
                if db_dir and not os.path.exists(db_dir):
                    os.makedirs(db_dir, mode=0o755, exist_ok=True)
                
                # ä½¿ç”¨WALæ¨¡å¼å’Œæ›´é•·çš„è¶…æ™‚ï¼Œæ”¯æŒä¸¦ç™¼è¨ªå•
                self.connection = await aiosqlite.connect(
                    self.db_path,
                    timeout=10.0  # 10ç§’è¶…æ™‚
                )
                
                # è¨­ç½®WALæ¨¡å¼ä»¥æ”¯æŒä¸¦ç™¼è®€å¯«
                await self.connection.execute("PRAGMA journal_mode=WAL")
                await self.connection.execute("PRAGMA synchronous=NORMAL")
                await self.connection.execute("PRAGMA cache_size=10000")
                await self.connection.execute("PRAGMA temp_store=memory")
                
                await self._create_tables()
                logger.info(f"âœ… å¢å¼·ç‰ˆä¿¡è™Ÿè³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ (å˜—è©¦ {attempt + 1}/{max_retries})")
                return
                
            except Exception as e:
                logger.warning(f"âš ï¸ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
                
                if self.connection:
                    try:
                        await self.connection.close()
                    except:
                        pass
                    self.connection = None
                
                if attempt < max_retries - 1:
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•¸é€€é¿
                else:
                    # æœ€å¾Œå˜—è©¦ä½¿ç”¨å‚™ç”¨è·¯å¾‘
                    try:
                        import tempfile
                        backup_path = os.path.join(tempfile.gettempdir(), f"enhanced_signals_backup_{os.getpid()}.db")
                        logger.warning(f"ğŸ”„ å˜—è©¦å‚™ç”¨è³‡æ–™åº«è·¯å¾‘: {backup_path}")
                        self.db_path = backup_path
                        self.connection = await aiosqlite.connect(self.db_path, timeout=10.0)
                        await self.connection.execute("PRAGMA journal_mode=WAL")
                        await self._create_tables()
                        logger.info("âœ… å‚™ç”¨è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
                        return
                    except Exception as backup_error:
                        logger.error(f"âŒ å‚™ç”¨è³‡æ–™åº«ä¹Ÿå¤±æ•—: {backup_error}")
                        raise
    
    async def _create_tables(self):
        """å‰µå»ºå¢å¼·ç‰ˆä¿¡è™Ÿè¡¨çµæ§‹"""
        
        if not self.connection:
            raise RuntimeError("Database connection not initialized")
        
        # ä¸»è¦å¢å¼·ä¿¡è™Ÿè¡¨
        await self.connection.execute("""
            CREATE TABLE IF NOT EXISTS enhanced_signals (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                signal_id TEXT UNIQUE NOT NULL,
                symbol TEXT NOT NULL,
                signal_type TEXT NOT NULL,
                signal_strength REAL NOT NULL,
                timestamp TEXT NOT NULL,
                features TEXT NOT NULL,
                market_conditions TEXT NOT NULL,
                tier TEXT NOT NULL,
                
                -- å„ªå…ˆç´š1ï¼šæ™‚é–“è¡°æ¸›
                time_decay_weight REAL DEFAULT 1.0,
                hours_since_generation REAL DEFAULT 0.0,
                
                -- å„ªå…ˆç´š2ï¼šå¹£ç¨®åˆ†é¡  
                coin_category TEXT DEFAULT 'alt',
                category_weight REAL DEFAULT 1.0,
                category_risk_multiplier REAL DEFAULT 1.0,
                
                -- å„ªå…ˆç´š3ï¼šæ™‚é–“æ¡†æ¶æ„ŸçŸ¥
                primary_timeframe TEXT DEFAULT '5m',
                timeframe_consensus TEXT DEFAULT '{}',
                cross_timeframe_weight REAL DEFAULT 1.0,
                
                -- æœ€çµ‚èåˆæ¬Šé‡
                final_learning_weight REAL DEFAULT 1.0,
                
                -- çµæœæ•¸æ“š
                status TEXT DEFAULT 'PENDING',
                actual_outcome REAL,
                performance_score REAL,
                execution_time TEXT,
                
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # æ™‚é–“æ¡†æ¶åˆ†æè¡¨
        await self.connection.execute("""
            CREATE TABLE IF NOT EXISTS timeframe_analysis (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                signal_id TEXT NOT NULL,
                timeframe TEXT NOT NULL,
                signal_strength REAL NOT NULL,
                weight REAL NOT NULL,
                consensus_score REAL DEFAULT 0.0,
                dominant_frame TEXT,
                conflict_level REAL DEFAULT 0.0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                
                FOREIGN KEY (signal_id) REFERENCES enhanced_signals (signal_id)
            )
        """)
        
        # å­¸ç¿’æ¬Šé‡æ­·å²è¡¨
        await self.connection.execute("""
            CREATE TABLE IF NOT EXISTS learning_weight_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                signal_id TEXT NOT NULL,
                weight_type TEXT NOT NULL,  -- time_decay/category/timeframe/final
                weight_value REAL NOT NULL,
                calculation_details TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                
                FOREIGN KEY (signal_id) REFERENCES enhanced_signals (signal_id)
            )
        """)
        
        # å‰µå»ºç´¢å¼•
        await self.connection.execute("CREATE INDEX IF NOT EXISTS idx_symbol ON enhanced_signals(symbol)")
        await self.connection.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON enhanced_signals(timestamp)")
        await self.connection.execute("CREATE INDEX IF NOT EXISTS idx_category ON enhanced_signals(coin_category)")
        await self.connection.execute("CREATE INDEX IF NOT EXISTS idx_timeframe ON enhanced_signals(primary_timeframe)")
        await self.connection.execute("CREATE INDEX IF NOT EXISTS idx_final_weight ON enhanced_signals(final_learning_weight)")
        
        await self.connection.commit()
        
        # ğŸ”§ è¡¨æ ¼å‰µå»ºå®Œæˆå¾Œï¼Œé€²è¡Œè³‡æ–™åº«ç‰ˆæœ¬æª¢æŸ¥èˆ‡å‡ç´š
        await self._check_and_upgrade_database()
        
        logger.info("âœ… å¢å¼·ç‰ˆä¿¡è™Ÿè¡¨çµæ§‹å‰µå»ºå®Œæˆ")
    
    async def store_enhanced_signal(self, signal) -> bool:
        """å­˜å„²å¢å¼·ç‰ˆä¿¡è™Ÿ - ç”¢å“ç´šå®¹éŒ¯"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                if not self.connection:
                    await self.initialize()
                
                # ğŸ”§ ç”¢å“ç´šä¿®æ­£ï¼šä¿¡è™Ÿå­˜å„²å‰é©—è­‰è¡¨çµæ§‹
                if attempt == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡å˜—è©¦æ™‚æª¢æŸ¥
                    await self._validate_and_fix_table_structure()
                
                # æå–ä¿¡è™Ÿæ•¸æ“š - ğŸ¯ ç”¢å“ç´šåºåˆ—åŒ–è™•ç†
                signal_data = (
                    str(signal.signal_id),                                                # TEXT
                    str(signal.symbol),                                                   # TEXT
                    str(signal.signal_type.value if hasattr(signal.signal_type, 'value') else str(signal.signal_type)), # TEXT - æšèˆ‰å®‰å…¨è™•ç†
                    float(signal.signal_strength),                                       # REAL
                    str(signal.timestamp.isoformat()),                                  # TEXT
                    self._safe_json_serialize(signal.features if hasattr(signal, 'features') else {}), # TEXT - features å®‰å…¨åºåˆ—åŒ–
                    self._safe_json_serialize(signal.market_conditions if hasattr(signal, 'market_conditions') else {}), # TEXT - market_conditions å®‰å…¨åºåˆ—åŒ–
                    str(signal.tier),                                                    # TEXT
                    
                    # å„ªå…ˆç´š1ï¼šæ™‚é–“è¡°æ¸›
                    float(getattr(signal, 'time_decay_weight', 1.0)),                   # REAL
                    float(getattr(signal, 'hours_since_generation', 0.0)),              # REAL
                    
                    # å„ªå…ˆç´š2ï¼šå¹£ç¨®åˆ†é¡
                    str(getattr(signal, 'coin_category', 'alt')),                       # TEXT
                    float(getattr(signal, 'category_weight', 1.0)),                     # REAL
                    float(getattr(signal, 'category_risk_multiplier', 1.0)),           # REAL
                    
                    # å„ªå…ˆç´š3ï¼šæ™‚é–“æ¡†æ¶æ„ŸçŸ¥
                    str(getattr(signal, 'primary_timeframe', '5m')),                    # TEXT
                    self._safe_json_serialize(getattr(signal, 'timeframe_consensus', {})),             # TEXT - å®‰å…¨åºåˆ—åŒ–
                    float(getattr(signal, 'cross_timeframe_weight', 1.0)),              # REAL
                    
                    # æœ€çµ‚æ¬Šé‡
                    float(getattr(signal, 'final_learning_weight', 1.0)),               # REAL
                    
                    # çµæœ
                    str(signal.status),                                                   # TEXT
                    float(signal.actual_outcome) if signal.actual_outcome is not None else None,  # REAL or NULL
                    float(signal.performance_score) if signal.performance_score is not None else None,  # REAL or NULL
                    str(signal.execution_time.isoformat()) if signal.execution_time else None  # TEXT or NULL
                )
                
                await self.connection.execute("""
                    INSERT OR REPLACE INTO enhanced_signals (
                        signal_id, symbol, signal_type, signal_strength, timestamp,
                        features, market_conditions, tier,
                        time_decay_weight, hours_since_generation,
                        coin_category, category_weight, category_risk_multiplier,
                        primary_timeframe, timeframe_consensus, cross_timeframe_weight,
                        final_learning_weight,
                        status, actual_outcome, performance_score, execution_time
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, signal_data)
                
                # å­˜å„²æ™‚é–“æ¡†æ¶åˆ†ææ•¸æ“š
                await self._store_timeframe_analysis(signal)
                
                # å­˜å„²å­¸ç¿’æ¬Šé‡æ­·å²
                await self._store_weight_history(signal)
                
                await self.connection.commit()
                logger.debug(f"âœ… å¢å¼·ä¿¡è™Ÿå·²å­˜å„²: {signal.signal_id}")
                return True
                
            except Exception as e:
                error_msg = str(e)
                logger.warning(f"âš ï¸ å­˜å„²å¢å¼·ä¿¡è™Ÿå¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {error_msg}")
                
                # ğŸ”§ ç”¢å“ç´šéŒ¯èª¤è™•ç†ï¼šå¦‚æœæ˜¯è¡¨çµæ§‹å•é¡Œï¼Œå˜—è©¦ä¿®æ­£
                if "has no column named" in error_msg and attempt < max_retries - 1:
                    logger.info("ğŸ”§ æª¢æ¸¬åˆ°è¡¨çµæ§‹å•é¡Œï¼Œå˜—è©¦å‹•æ…‹ä¿®æ­£...")
                    try:
                        await self._validate_and_fix_table_structure()
                        logger.info("âœ… è¡¨çµæ§‹ä¿®æ­£å®Œæˆï¼Œå°‡é‡è©¦å­˜å„²")
                        continue
                    except Exception as fix_error:
                        logger.error(f"âŒ è¡¨çµæ§‹ä¿®æ­£å¤±æ•—: {fix_error}")
                
                if self.connection:
                    try:
                        await self.connection.rollback()
                    except:
                        pass
                
                if attempt < max_retries - 1:
                    import asyncio
                    await asyncio.sleep(0.1 * (attempt + 1))
                else:
                    logger.error(f"âŒ å­˜å„²å¢å¼·ä¿¡è™Ÿæœ€çµ‚å¤±æ•—: {error_msg}")
                    return False
        
        return False
    
    async def _store_timeframe_analysis(self, signal):
        """å­˜å„²æ™‚é–“æ¡†æ¶åˆ†ææ•¸æ“š - ç”¢å“ç´šå®¹éŒ¯"""
        try:
            # ğŸ”§ ç”¢å“ç´šå®¹éŒ¯ï¼šæª¢æŸ¥å¿…éœ€å±¬æ€§
            if not hasattr(signal, 'timeframe_consensus'):
                logger.debug(f"âš ï¸ ä¿¡è™Ÿ {signal.signal_id} ç¼ºå°‘ timeframe_consensus å±¬æ€§ï¼Œè·³éæ™‚é–“æ¡†æ¶åˆ†æå­˜å„²")
                return
                
            consensus = signal.timeframe_consensus
            
            # å®¹éŒ¯æª¢æŸ¥ï¼šç¢ºä¿ consensus æœ‰å¿…éœ€çš„å±¬æ€§
            if not hasattr(consensus, 'timeframe_signals'):
                logger.debug(f"âš ï¸ å…±è­˜å°è±¡ç¼ºå°‘ timeframe_signals å±¬æ€§ï¼Œè·³éå­˜å„²")
                return
            
            # å­˜å„²å„æ™‚é–“æ¡†æ¶ä¿¡è™Ÿ
            for timeframe, strength in consensus.timeframe_signals.items():
                await self.connection.execute("""
                    INSERT INTO timeframe_analysis (
                        signal_id, timeframe, signal_strength, weight,
                        consensus_score, dominant_frame, conflict_level
                    ) VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    str(signal.signal_id),                     # TEXT
                    str(timeframe),                            # TEXT
                    float(strength),                           # REAL
                    float(getattr(signal, 'cross_timeframe_weight', 1.0)),      # REAL - å®¹éŒ¯
                    float(getattr(consensus, 'consensus_score', 0.0)),          # REAL - å®¹éŒ¯
                    str(getattr(consensus, 'dominant_timeframe', 'unknown')),   # TEXT - å®¹éŒ¯
                    float(getattr(consensus, 'conflict_level', 0.0))            # REAL - å®¹éŒ¯
                ))
                
        except Exception as e:
            logger.error(f"âŒ å­˜å„²æ™‚é–“æ¡†æ¶åˆ†æå¤±æ•—: {e}")
    
    async def _store_weight_history(self, signal):
        """å­˜å„²å­¸ç¿’æ¬Šé‡æ­·å² - ç”¢å“ç´šå®¹éŒ¯"""
        try:
            # ğŸ”§ ç”¢å“ç´šå®¹éŒ¯ï¼šå®‰å…¨ç²å–å±¬æ€§å€¼
            time_decay_weight = getattr(signal, 'time_decay_weight', 1.0)
            category_weight = getattr(signal, 'category_weight', 1.0)
            cross_timeframe_weight = getattr(signal, 'cross_timeframe_weight', 1.0)
            final_learning_weight = getattr(signal, 'final_learning_weight', 1.0)
            hours_since_generation = getattr(signal, 'hours_since_generation', 0.0)
            coin_category = getattr(signal, 'coin_category', 'unknown')
            
            # å®¹éŒ¯è™•ç† timeframe_consensus
            consensus_score = 0.0
            if hasattr(signal, 'timeframe_consensus') and signal.timeframe_consensus:
                consensus_score = getattr(signal.timeframe_consensus, 'consensus_score', 0.0)
            
            weights = [
                ('time_decay', time_decay_weight, {'hours_elapsed': hours_since_generation}),
                ('category', category_weight, {'coin_category': coin_category}),
                ('timeframe', cross_timeframe_weight, {'consensus_score': consensus_score}),
                ('final', final_learning_weight, {'calculation': 'time_decay Ã— category Ã— timeframe'})
            ]
            
            for weight_type, weight_value, details in weights:
                await self.connection.execute("""
                    INSERT INTO learning_weight_history (
                        signal_id, weight_type, weight_value, calculation_details
                    ) VALUES (?, ?, ?, ?)
                """, (
                    str(signal.signal_id),                     # TEXT
                    str(weight_type),                          # TEXT
                    float(weight_value),                       # REAL
                    self._safe_json_serialize(details)         # TEXT - å®‰å…¨åºåˆ—åŒ–
                ))
                
        except Exception as e:
            logger.error(f"âŒ å­˜å„²æ¬Šé‡æ­·å²å¤±æ•—: {e}")
    
    async def get_signals_by_timeframe(self, timeframe: str, limit: int = 100) -> List[Dict[str, Any]]:
        """æŒ‰æ™‚é–“æ¡†æ¶æŸ¥è©¢ä¿¡è™Ÿ"""
        try:
            if not self.connection:
                await self.initialize()
            
            cursor = await self.connection.execute("""
                SELECT * FROM enhanced_signals 
                WHERE primary_timeframe = ? 
                ORDER BY timestamp DESC 
                LIMIT ?
            """, (timeframe, limit))
            
            rows = await cursor.fetchall()
            
            signals = []
            for row in rows:
                signal_dict = dict(row)
                # è§£æJSONå­—æ®µ
                signal_dict['features'] = json.loads(signal_dict['features'])
                signal_dict['market_conditions'] = json.loads(signal_dict['market_conditions'])
                signal_dict['timeframe_consensus'] = json.loads(signal_dict['timeframe_consensus'])
                signals.append(signal_dict)
            
            return signals
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰æ™‚é–“æ¡†æ¶æŸ¥è©¢å¤±æ•—: {e}")
            return []
    
    async def get_signals_by_category(self, category: str, limit: int = 100) -> List[Dict[str, Any]]:
        """æŒ‰å¹£ç¨®åˆ†é¡æŸ¥è©¢ä¿¡è™Ÿ"""
        try:
            if not self.connection:
                await self.initialize()
            
            cursor = await self.connection.execute("""
                SELECT * FROM enhanced_signals 
                WHERE coin_category = ? 
                ORDER BY timestamp DESC 
                LIMIT ?
            """, (category, limit))
            
            rows = await cursor.fetchall()
            
            signals = []
            for row in rows:
                signal_dict = dict(row)
                signal_dict['features'] = json.loads(signal_dict['features'])
                signal_dict['market_conditions'] = json.loads(signal_dict['market_conditions'])
                signal_dict['timeframe_consensus'] = json.loads(signal_dict['timeframe_consensus'])
                signals.append(signal_dict)
            
            return signals
            
        except Exception as e:
            logger.error(f"âŒ æŒ‰åˆ†é¡æŸ¥è©¢å¤±æ•—: {e}")
            return []
    
    async def get_weight_analysis(self, hours: int = 24) -> Dict[str, Any]:
        """ç²å–æ¬Šé‡åˆ†æå ±å‘Š"""
        try:
            if not self.connection:
                await self.initialize()
            
            since_time = (datetime.now() - timedelta(hours=hours)).isoformat()
            
            # å¹³å‡æ¬Šé‡çµ±è¨ˆ
            cursor = await self.connection.execute("""
                SELECT 
                    AVG(time_decay_weight) as avg_time_decay,
                    AVG(category_weight) as avg_category,
                    AVG(cross_timeframe_weight) as avg_timeframe,
                    AVG(final_learning_weight) as avg_final,
                    COUNT(*) as total_signals
                FROM enhanced_signals 
                WHERE timestamp > ?
            """, (since_time,))
            
            weight_stats = dict(await cursor.fetchone())
            
            # å„åˆ†é¡æ¬Šé‡åˆ†ä½ˆ
            cursor = await self.connection.execute("""
                SELECT 
                    coin_category,
                    AVG(final_learning_weight) as avg_weight,
                    COUNT(*) as count
                FROM enhanced_signals 
                WHERE timestamp > ?
                GROUP BY coin_category
            """, (since_time,))
            
            category_weights = {row[0]: {'avg_weight': row[1], 'count': row[2]} 
                              for row in await cursor.fetchall()}
            
            # å„æ™‚é–“æ¡†æ¶æ¬Šé‡åˆ†ä½ˆ
            cursor = await self.connection.execute("""
                SELECT 
                    primary_timeframe,
                    AVG(final_learning_weight) as avg_weight,
                    COUNT(*) as count
                FROM enhanced_signals 
                WHERE timestamp > ?
                GROUP BY primary_timeframe
            """, (since_time,))
            
            timeframe_weights = {row[0]: {'avg_weight': row[1], 'count': row[2]} 
                               for row in await cursor.fetchall()}
            
            return {
                'analysis_period_hours': hours,
                'weight_statistics': weight_stats,
                'category_distribution': category_weights,
                'timeframe_distribution': timeframe_weights,
                'analysis_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¬Šé‡åˆ†æå¤±æ•—: {e}")
            return {}
    
    async def update_signal_outcome(self, signal_id: str, outcome: float, performance_score: float = None) -> bool:
        """æ›´æ–°ä¿¡è™Ÿçµæœ"""
        try:
            if not self.connection:
                await self.initialize()
            
            await self.connection.execute("""
                UPDATE enhanced_signals 
                SET actual_outcome = ?, 
                    performance_score = ?,
                    status = 'SUCCESS',
                    execution_time = ?
                WHERE signal_id = ?
            """, (outcome, performance_score, datetime.now().isoformat(), signal_id))
            
            await self.connection.commit()
            logger.debug(f"âœ… ä¿¡è™Ÿçµæœå·²æ›´æ–°: {signal_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä¿¡è™Ÿçµæœå¤±æ•—: {e}")
            return False
    
    async def close(self):
        """é—œé–‰è³‡æ–™åº«é€£æ¥"""
        if self.connection:
            await self.connection.close()
            logger.info("ğŸ—„ï¸ å¢å¼·ç‰ˆä¿¡è™Ÿè³‡æ–™åº«é€£æ¥å·²é—œé–‰")
    
    async def _check_and_upgrade_database(self):
        """æª¢æŸ¥ä¸¦å‡ç´šè³‡æ–™åº«çµæ§‹"""
        try:
            # æª¢æŸ¥ç‰ˆæœ¬è¡¨æ˜¯å¦å­˜åœ¨
            cursor = await self.connection.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='database_version'
            """)
            table_exists = await cursor.fetchone()
            await cursor.close()
            
            if not table_exists:
                # å‰µå»ºç‰ˆæœ¬è¡¨
                await self.connection.execute("""
                    CREATE TABLE database_version (
                        version INTEGER PRIMARY KEY,
                        upgraded_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                await self.connection.execute("INSERT INTO database_version (version) VALUES (1)")
                logger.info("âœ… å‰µå»ºè³‡æ–™åº«ç‰ˆæœ¬è¡¨ (ç‰ˆæœ¬: 1)")
            
            # ç²å–ç•¶å‰ç‰ˆæœ¬
            cursor = await self.connection.execute("SELECT MAX(version) FROM database_version")
            current_version = (await cursor.fetchone())[0] or 0
            await cursor.close()
            
            # å®šç¾©æœ€æ–°ç‰ˆæœ¬
            LATEST_VERSION = 4
            
            if current_version < LATEST_VERSION:
                logger.info(f"ğŸ”„ æª¢æ¸¬åˆ°è³‡æ–™åº«éœ€è¦å‡ç´š: {current_version} â†’ {LATEST_VERSION}")
                await self._perform_database_upgrade(current_version, LATEST_VERSION)
            else:
                logger.debug(f"âœ… è³‡æ–™åº«ç‰ˆæœ¬å·²æ˜¯æœ€æ–°: {current_version}")
                
            # ğŸ”§ ç”¢å“ç´šä¿®æ­£ï¼šå‡ç´šå®Œæˆå¾Œæª¢æŸ¥è¡¨çµæ§‹
            await self._validate_and_fix_table_structure()
                
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«ç‰ˆæœ¬æª¢æŸ¥å¤±æ•—: {e}")
            raise
    
    async def _validate_and_fix_table_structure(self):
        """é©—è­‰ä¸¦ä¿®æ­£è¡¨çµæ§‹ - ç”¢å“ç´šä¿®æ­£"""
        try:
            # æª¢æŸ¥ enhanced_signals è¡¨æ˜¯å¦å­˜åœ¨
            cursor = await self.connection.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='enhanced_signals'
            """)
            table_exists = await cursor.fetchone()
            await cursor.close()
            
            if not table_exists:
                logger.info("ğŸ”§ enhanced_signals è¡¨ä¸å­˜åœ¨ï¼Œå°‡åœ¨ _create_tables ä¸­å‰µå»º")
                return
            
            # æª¢æŸ¥è¡¨çµæ§‹
            cursor = await self.connection.execute("PRAGMA table_info(enhanced_signals)")
            columns = await cursor.fetchall()
            await cursor.close()
            
            column_names = [col[1] for col in columns]
            
            # ğŸ¯ ç”¢å“ç´šå¿…éœ€æ¬„ä½åˆ—è¡¨
            required_columns = {
                'features': "TEXT DEFAULT '{}'",
                'market_conditions': "TEXT DEFAULT '{}'", 
                'tier': "TEXT DEFAULT 'MEDIUM'",
                'coin_category': "TEXT DEFAULT 'alt'",
                'category_weight': "REAL DEFAULT 1.0",
                'category_risk_multiplier': "REAL DEFAULT 1.0",
                'primary_timeframe': "TEXT DEFAULT '5m'",
                'timeframe_consensus': "TEXT DEFAULT '{}'",
                'time_decay_weight': "REAL DEFAULT 1.0",
                'hours_since_generation': "REAL DEFAULT 0.0",
                'cross_timeframe_weight': "REAL DEFAULT 1.0",
                'final_learning_weight': "REAL DEFAULT 1.0",
                'status': "TEXT DEFAULT 'PENDING'",
                'actual_outcome': "REAL DEFAULT NULL",
                'performance_score': "REAL DEFAULT NULL",
                'execution_time': "TEXT DEFAULT NULL"
            }
            
            # æª¢æŸ¥ä¸¦æ·»åŠ ç¼ºå¤±çš„æ¬„ä½
            missing_columns = []
            for col_name, col_def in required_columns.items():
                if col_name not in column_names:
                    missing_columns.append((col_name, col_def))
            
            if missing_columns:
                logger.warning(f"ğŸ”§ æª¢æ¸¬åˆ° {len(missing_columns)} å€‹ç¼ºå¤±æ¬„ä½ï¼Œæ­£åœ¨ä¿®æ­£...")
                for col_name, col_def in missing_columns:
                    await self.connection.execute(f"""
                        ALTER TABLE enhanced_signals 
                        ADD COLUMN {col_name} {col_def}
                    """)
                    logger.info(f"âœ… æ·»åŠ ç¼ºå¤±æ¬„ä½: {col_name}")
                
                await self.connection.commit()
                logger.info(f"âœ… è¡¨çµæ§‹ä¿®æ­£å®Œæˆï¼Œæ·»åŠ äº† {len(missing_columns)} å€‹æ¬„ä½")
            else:
                logger.debug("âœ… è¡¨çµæ§‹å®Œæ•´ï¼Œç„¡éœ€ä¿®æ­£")
                
        except Exception as e:
            logger.error(f"âŒ è¡¨çµæ§‹é©—è­‰å¤±æ•—: {e}")
            raise

    async def _perform_database_upgrade(self, from_version: int, to_version: int):
        """åŸ·è¡Œè³‡æ–™åº«å‡ç´š"""
        try:
            # ç‰ˆæœ¬ 1 â†’ 2: æ·»åŠ  coin_category æ¬„ä½
            if from_version < 2:
                await self._upgrade_to_version_2()
                
            # ç‰ˆæœ¬ 2 â†’ 3: ç¢ºä¿æ‰€æœ‰å¿…éœ€æ¬„ä½å®Œæ•´ (åŒ…å«åœ¨v2å‡ç´šä¸­)
            if from_version < 3:
                # v2å‡ç´šå·²åŒ…å«æ‰€æœ‰å¿…éœ€æ¬„ä½ï¼Œç„¡éœ€é¡å¤–æ“ä½œ
                pass
                
            # ç‰ˆæœ¬ 3 â†’ 4: å¼·åˆ¶è¡¨çµæ§‹å®Œæ•´æ€§é©—è­‰
            if from_version < 4:
                logger.info("ğŸ”§ åŸ·è¡Œç‰ˆæœ¬4å‡ç´šï¼šå¼·åˆ¶è¡¨çµæ§‹å®Œæ•´æ€§é©—è­‰")
                await self._validate_and_fix_table_structure()
                
            # æ›´æ–°ç‰ˆæœ¬è™Ÿ
            await self.connection.execute(
                "INSERT INTO database_version (version) VALUES (?)", 
                (to_version,)
            )
            await self.connection.commit()
            
            logger.info(f"âœ… è³‡æ–™åº«å‡ç´šå®Œæˆ: {from_version} â†’ {to_version}")
            
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«å‡ç´šå¤±æ•—: {e}")
            # ç›´æ¥å ±éŒ¯ï¼Œä¸é™ç´š
            raise RuntimeError(f"è³‡æ–™åº«å‡ç´šå¤±æ•—: {e}")
    
    async def _upgrade_to_version_2(self):
        """å‡ç´šåˆ°ç‰ˆæœ¬2: æ·»åŠ å¹£ç¨®åˆ†é¡æ”¯æ´å’Œç¼ºå¤±æ¬„ä½"""
        try:
            # ğŸ”§ é¦–å…ˆæª¢æŸ¥ enhanced_signals è¡¨æ˜¯å¦å­˜åœ¨
            cursor = await self.connection.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name='enhanced_signals'
            """)
            table_exists = await cursor.fetchone()
            await cursor.close()
            
            if not table_exists:
                # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œè·³éæ­¤å‡ç´šï¼ˆè¡¨å°‡åœ¨ _create_tables ä¸­å‰µå»ºï¼‰
                logger.info("ğŸ”§ enhanced_signals è¡¨ä¸å­˜åœ¨ï¼Œè·³éç‰ˆæœ¬2å‡ç´š")
                return
            
            # æª¢æŸ¥æ‰€æœ‰å¿…éœ€æ¬„ä½æ˜¯å¦å­˜åœ¨
            cursor = await self.connection.execute("PRAGMA table_info(enhanced_signals)")
            columns = await cursor.fetchall()
            await cursor.close()
            
            column_names = [col[1] for col in columns]
            
            # æª¢æŸ¥ä¸¦æ·»åŠ  features æ¬„ä½ (å¦‚æœä¸å­˜åœ¨)
            if 'features' not in column_names:
                await self.connection.execute("""
                    ALTER TABLE enhanced_signals 
                    ADD COLUMN features TEXT DEFAULT '{}'
                """)
                logger.info("âœ… æ·»åŠ  features æ¬„ä½")
            
            # æª¢æŸ¥ä¸¦æ·»åŠ  market_conditions æ¬„ä½ (å¦‚æœä¸å­˜åœ¨)
            if 'market_conditions' not in column_names:
                await self.connection.execute("""
                    ALTER TABLE enhanced_signals 
                    ADD COLUMN market_conditions TEXT DEFAULT '{}'
                """)
                logger.info("âœ… æ·»åŠ  market_conditions æ¬„ä½")
            
            # æª¢æŸ¥ä¸¦æ·»åŠ å¹£ç¨®åˆ†é¡ç›¸é—œæ¬„ä½
            if 'coin_category' not in column_names:
                await self.connection.execute("""
                    ALTER TABLE enhanced_signals 
                    ADD COLUMN coin_category TEXT DEFAULT 'alt'
                """)
                logger.info("âœ… æ·»åŠ  coin_category æ¬„ä½")
            
            if 'category_weight' not in column_names:
                await self.connection.execute("""
                    ALTER TABLE enhanced_signals 
                    ADD COLUMN category_weight REAL DEFAULT 1.0
                """)
                logger.info("âœ… æ·»åŠ  category_weight æ¬„ä½")
            
            if 'category_risk_multiplier' not in column_names:
                await self.connection.execute("""
                    ALTER TABLE enhanced_signals 
                    ADD COLUMN category_risk_multiplier REAL DEFAULT 1.0
                """)
                logger.info("âœ… æ·»åŠ  category_risk_multiplier æ¬„ä½")
            
            # æª¢æŸ¥ä¸¦æ·»åŠ æ™‚é–“æ¡†æ¶ç›¸é—œæ¬„ä½
            if 'primary_timeframe' not in column_names:
                await self.connection.execute("""
                    ALTER TABLE enhanced_signals 
                    ADD COLUMN primary_timeframe TEXT DEFAULT '5m'
                """)
                logger.info("âœ… æ·»åŠ  primary_timeframe æ¬„ä½")
            
            if 'timeframe_consensus' not in column_names:
                await self.connection.execute("""
                    ALTER TABLE enhanced_signals 
                    ADD COLUMN timeframe_consensus TEXT DEFAULT '{}'
                """)
                logger.info("âœ… æ·»åŠ  timeframe_consensus æ¬„ä½")
                
            # å‰µå»ºç´¢å¼•
            await self.connection.execute("""
                CREATE INDEX IF NOT EXISTS idx_category 
                ON enhanced_signals(coin_category)
            """)
            
            logger.info("âœ… ç‰ˆæœ¬2/3å‡ç´šå®Œæˆ: å¹£ç¨®åˆ†é¡æ”¯æ´å’Œæ‰€æœ‰å¿…éœ€æ¬„ä½å·²æ·»åŠ ")
            
        except Exception as e:
            logger.error(f"âŒ ç‰ˆæœ¬2å‡ç´šå¤±æ•—: {e}")
            raise

# å…¨å±€å¯¦ä¾‹
enhanced_signal_db = EnhancedSignalDatabase()

async def main():
    """æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ—„ï¸ å¢å¼·ç‰ˆä¿¡è™Ÿè³‡æ–™åº«æ¸¬è©¦")
    
    # æ¨¡æ“¬æ¸¬è©¦ï¼ˆéœ€è¦å¯¦éš›çš„TimeFrameEnhancedSignalå¯¦ä¾‹ï¼‰
    await enhanced_signal_db.initialize()
    
    # ç²å–æ¬Šé‡åˆ†æ
    analysis = await enhanced_signal_db.get_weight_analysis(24)
    print(f"æ¬Šé‡åˆ†æ: {enhanced_signal_db._safe_json_serialize(analysis)}")
    
    await enhanced_signal_db.close()

if __name__ == "__main__":
    asyncio.run(main())
