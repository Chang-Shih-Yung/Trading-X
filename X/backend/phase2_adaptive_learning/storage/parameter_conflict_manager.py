#!/usr/bin/env python3
"""
âš–ï¸ Parameter Conflict Resolution Manager
åƒæ•¸è¡çªè§£æ±ºç®¡ç†å™¨ - æ”¹é€²åƒæ•¸è¡çªè¨˜éŒ„èˆ‡è™•ç†

åŠŸèƒ½ï¼š
- è©³ç´°è¨˜éŒ„ Phase2 å’Œ Phase5 åƒæ•¸è¡çª
- è¡çªè§£æ±ºç­–ç•¥è¿½è¹¤
- è‡ªå‹•å›æ»¾æ©Ÿåˆ¶
- A/B æ¸¬è©¦æ”¯æ´
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np

logger = logging.getLogger(__name__)

class ConflictResolutionMethod(Enum):
    """è¡çªè§£æ±ºæ–¹æ³•"""
    WEIGHTED_AVERAGE = "weighted_average"
    PHASE2_DOMINANT = "phase2_dominant"
    PHASE5_DOMINANT = "phase5_dominant"
    A_B_TESTING = "a_b_testing"
    PERFORMANCE_BASED = "performance_based"

class ConflictSeverity(Enum):
    """è¡çªåš´é‡ç¨‹åº¦"""
    LOW = "low"           # å·®ç•° < 10%
    MEDIUM = "medium"     # å·®ç•° 10-25%
    HIGH = "high"         # å·®ç•° 25-50%
    CRITICAL = "critical" # å·®ç•° > 50%

@dataclass
class ParameterConflict:
    """åƒæ•¸è¡çªè¨˜éŒ„"""
    parameter_name: str
    phase2_value: float
    phase5_value: float
    conflict_timestamp: datetime
    severity: ConflictSeverity
    resolution_method: ConflictResolutionMethod
    final_value: float
    confidence_score: float
    
    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    market_conditions: Dict[str, Any]
    signal_count_context: int
    recent_performance: float
    
    # è§£æ±ºçµæœè¿½è¹¤
    resolution_success: Optional[bool] = None
    performance_impact: Optional[float] = None
    rollback_triggered: bool = False

@dataclass
class ConflictResolutionStats:
    """è¡çªè§£æ±ºçµ±è¨ˆ"""
    total_conflicts: int
    resolution_success_rate: float
    most_conflicted_parameters: List[str]
    avg_resolution_time: float
    rollback_rate: float
    performance_improvement: float

class ParameterConflictManager:
    """åƒæ•¸è¡çªè§£æ±ºç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¡çªç®¡ç†å™¨"""
        self.storage_dir = Path(__file__).parent
        self.storage_dir.mkdir(exist_ok=True)
        
        self.conflicts_file = self.storage_dir / "parameter_conflicts.json"
        self.stats_file = self.storage_dir / "conflict_resolution_stats.json"
        
        # è¡çªæ­·å²
        self.conflict_history: List[ParameterConflict] = []
        self.load_conflict_history()
        
        # è§£æ±ºç­–ç•¥é…ç½®
        self.resolution_config = {
            'default_method': ConflictResolutionMethod.WEIGHTED_AVERAGE,
            'severity_thresholds': {
                'low': 0.10,
                'medium': 0.25,
                'high': 0.50
            },
            'performance_monitor_window': 100,  # 100 å€‹ä¿¡è™Ÿ
            'rollback_threshold': 0.15,  # 15% æ€§èƒ½ä¸‹é™è§¸ç™¼å›æ»¾
            'a_b_test_duration': 50  # A/B æ¸¬è©¦æŒçºŒ 50 å€‹ä¿¡è™Ÿ
        }
        
        # ç•¶å‰ A/B æ¸¬è©¦
        self.active_ab_tests: Dict[str, Dict] = {}
        
        logger.info("âš–ï¸ åƒæ•¸è¡çªç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def detect_conflict(self, parameter_name: str, phase2_value: float, 
                       phase5_value: float, context: Dict[str, Any] = None) -> ParameterConflict:
        """æª¢æ¸¬åƒæ•¸è¡çª"""
        try:
            # è¨ˆç®—å·®ç•°ç™¾åˆ†æ¯”
            if phase5_value != 0:
                diff_percentage = abs(phase2_value - phase5_value) / abs(phase5_value)
            else:
                diff_percentage = abs(phase2_value - phase5_value)
            
            # ç¢ºå®šåš´é‡ç¨‹åº¦
            if diff_percentage < self.resolution_config['severity_thresholds']['low']:
                severity = ConflictSeverity.LOW
            elif diff_percentage < self.resolution_config['severity_thresholds']['medium']:
                severity = ConflictSeverity.MEDIUM
            elif diff_percentage < self.resolution_config['severity_thresholds']['high']:
                severity = ConflictSeverity.HIGH
            else:
                severity = ConflictSeverity.CRITICAL
            
            # é¸æ“‡è§£æ±ºæ–¹æ³•
            resolution_method = self._select_resolution_method(parameter_name, severity, context)
            
            # è¨ˆç®—æœ€çµ‚å€¼
            final_value = self._resolve_conflict_value(
                parameter_name, phase2_value, phase5_value, resolution_method, context
            )
            
            # è¨ˆç®—ä¿¡å¿ƒåº¦
            confidence_score = self._calculate_confidence_score(
                parameter_name, severity, resolution_method, context
            )
            
            # å‰µå»ºè¡çªè¨˜éŒ„
            conflict = ParameterConflict(
                parameter_name=parameter_name,
                phase2_value=phase2_value,
                phase5_value=phase5_value,
                conflict_timestamp=datetime.now(),
                severity=severity,
                resolution_method=resolution_method,
                final_value=final_value,
                confidence_score=confidence_score,
                market_conditions=context.get('market_conditions', {}),
                signal_count_context=context.get('signal_count', 0),
                recent_performance=context.get('recent_performance', 0.0)
            )
            
            # è¨˜éŒ„è¡çª
            self.conflict_history.append(conflict)
            self.save_conflict_history()
            
            logger.info(f"âš–ï¸ æª¢æ¸¬åˆ° {severity.value} ç´šåˆ¥è¡çª: {parameter_name}")
            logger.info(f"   Phase2: {phase2_value:.4f}, Phase5: {phase5_value:.4f}")
            logger.info(f"   è§£æ±ºæ–¹æ³•: {resolution_method.value}, æœ€çµ‚å€¼: {final_value:.4f}")
            
            return conflict
            
        except Exception as e:
            logger.error(f"âŒ è¡çªæª¢æ¸¬å¤±æ•—: {e}")
            raise
    
    def _select_resolution_method(self, parameter_name: str, severity: ConflictSeverity, 
                                 context: Dict[str, Any] = None) -> ConflictResolutionMethod:
        """é¸æ“‡è¡çªè§£æ±ºæ–¹æ³•"""
        # æ ¹æ“šåƒæ•¸é¡å‹å’Œåš´é‡ç¨‹åº¦é¸æ“‡æ–¹æ³•
        if severity == ConflictSeverity.CRITICAL:
            # åš´é‡è¡çªå•Ÿå‹• A/B æ¸¬è©¦
            return ConflictResolutionMethod.A_B_TESTING
        
        # æ ¹æ“šåƒæ•¸æ¬Šå¨æ€§é…ç½®é¸æ“‡
        phase2_dominant_params = ['signal_threshold', 'momentum_weight', 'volatility_adjustment']
        phase5_dominant_params = ['risk_multiplier', 'position_sizing_factor']
        
        if parameter_name in phase2_dominant_params:
            return ConflictResolutionMethod.PHASE2_DOMINANT
        elif parameter_name in phase5_dominant_params:
            return ConflictResolutionMethod.PHASE5_DOMINANT
        else:
            return ConflictResolutionMethod.WEIGHTED_AVERAGE
    
    def _resolve_conflict_value(self, parameter_name: str, phase2_value: float, 
                               phase5_value: float, method: ConflictResolutionMethod,
                               context: Dict[str, Any] = None) -> float:
        """è§£æ±ºè¡çªè¨ˆç®—æœ€çµ‚å€¼"""
        if method == ConflictResolutionMethod.PHASE2_DOMINANT:
            weight = 0.8  # Phase2 ä½” 80%
            return phase2_value * weight + phase5_value * (1 - weight)
        
        elif method == ConflictResolutionMethod.PHASE5_DOMINANT:
            weight = 0.3  # Phase2 åªä½” 30%
            return phase2_value * weight + phase5_value * (1 - weight)
        
        elif method == ConflictResolutionMethod.WEIGHTED_AVERAGE:
            # åŸºæ–¼æœ€è¿‘æ€§èƒ½å‹•æ…‹èª¿æ•´æ¬Šé‡
            recent_performance = context.get('recent_performance', 0.5)
            if recent_performance > 0.6:
                weight = 0.7  # æ€§èƒ½å¥½æ™‚æ›´ä¿¡ä»» Phase2
            else:
                weight = 0.4  # æ€§èƒ½å·®æ™‚æ›´ä¿å®ˆï¼Œä¿¡ä»» Phase5
            return phase2_value * weight + phase5_value * (1 - weight)
        
        elif method == ConflictResolutionMethod.A_B_TESTING:
            # A/B æ¸¬è©¦åˆå§‹ä½¿ç”¨ Phase5 å€¼
            self._start_ab_test(parameter_name, phase2_value, phase5_value)
            return phase5_value
        
        else:
            # é»˜èªä½¿ç”¨åŠ æ¬Šå¹³å‡
            return (phase2_value + phase5_value) / 2
    
    def _calculate_confidence_score(self, parameter_name: str, severity: ConflictSeverity,
                                   method: ConflictResolutionMethod, context: Dict[str, Any] = None) -> float:
        """è¨ˆç®—è§£æ±ºæ–¹æ¡ˆä¿¡å¿ƒåº¦"""
        base_confidence = {
            ConflictSeverity.LOW: 0.9,
            ConflictSeverity.MEDIUM: 0.7,
            ConflictSeverity.HIGH: 0.5,
            ConflictSeverity.CRITICAL: 0.3
        }[severity]
        
        # æ ¹æ“šä¿¡è™Ÿæ•¸é‡èª¿æ•´ä¿¡å¿ƒåº¦
        signal_count = context.get('signal_count', 0) if context else 0
        if signal_count > 200:
            confidence_boost = 0.1
        elif signal_count > 100:
            confidence_boost = 0.05
        else:
            confidence_boost = 0.0
        
        # æ ¹æ“šè§£æ±ºæ–¹æ³•èª¿æ•´
        method_confidence = {
            ConflictResolutionMethod.WEIGHTED_AVERAGE: 0.8,
            ConflictResolutionMethod.PHASE2_DOMINANT: 0.7,
            ConflictResolutionMethod.PHASE5_DOMINANT: 0.9,
            ConflictResolutionMethod.A_B_TESTING: 0.6,
            ConflictResolutionMethod.PERFORMANCE_BASED: 0.8
        }[method]
        
        final_confidence = min(1.0, base_confidence + confidence_boost) * method_confidence
        return final_confidence
    
    def _start_ab_test(self, parameter_name: str, value_a: float, value_b: float):
        """å•Ÿå‹• A/B æ¸¬è©¦"""
        test_id = f"{parameter_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.active_ab_tests[test_id] = {
            'parameter_name': parameter_name,
            'value_a': value_a,  # Phase2 å€¼
            'value_b': value_b,  # Phase5 å€¼
            'start_time': datetime.now(),
            'signals_count': 0,
            'performance_a': [],
            'performance_b': [],
            'current_variant': 'b',  # é–‹å§‹ä½¿ç”¨ Phase5 å€¼
            'duration': self.resolution_config['a_b_test_duration']
        }
        
        logger.info(f"ğŸ§ª å•Ÿå‹• A/B æ¸¬è©¦: {parameter_name}")
        logger.info(f"   è®Šé«” A (Phase2): {value_a:.4f}")
        logger.info(f"   è®Šé«” B (Phase5): {value_b:.4f}")
    
    def update_ab_test_performance(self, parameter_name: str, performance_score: float):
        """æ›´æ–° A/B æ¸¬è©¦æ€§èƒ½"""
        for test_id, test_data in self.active_ab_tests.items():
            if test_data['parameter_name'] == parameter_name:
                variant = test_data['current_variant']
                
                if variant == 'a':
                    test_data['performance_a'].append(performance_score)
                else:
                    test_data['performance_b'].append(performance_score)
                
                test_data['signals_count'] += 1
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ›è®Šé«”
                if test_data['signals_count'] % 10 == 0:
                    test_data['current_variant'] = 'a' if variant == 'b' else 'b'
                    logger.debug(f"ğŸ§ª A/B æ¸¬è©¦åˆ‡æ›åˆ°è®Šé«” {test_data['current_variant']}")
                
                # æª¢æŸ¥æ¸¬è©¦æ˜¯å¦å®Œæˆ
                if test_data['signals_count'] >= test_data['duration']:
                    self._complete_ab_test(test_id)
                
                break
    
    def _complete_ab_test(self, test_id: str):
        """å®Œæˆ A/B æ¸¬è©¦"""
        test_data = self.active_ab_tests[test_id]
        
        # è¨ˆç®—å¹³å‡æ€§èƒ½
        avg_performance_a = np.mean(test_data['performance_a']) if test_data['performance_a'] else 0
        avg_performance_b = np.mean(test_data['performance_b']) if test_data['performance_b'] else 0
        
        # é¸æ“‡ç²å‹è€…
        winner = 'a' if avg_performance_a > avg_performance_b else 'b'
        winning_value = test_data['value_a'] if winner == 'a' else test_data['value_b']
        
        logger.info(f"ğŸ† A/B æ¸¬è©¦å®Œæˆ: {test_data['parameter_name']}")
        logger.info(f"   è®Šé«” A æ€§èƒ½: {avg_performance_a:.4f}")
        logger.info(f"   è®Šé«” B æ€§èƒ½: {avg_performance_b:.4f}")
        logger.info(f"   ç²å‹è€…: è®Šé«” {winner}, å€¼: {winning_value:.4f}")
        
        # ç§»é™¤å·²å®Œæˆçš„æ¸¬è©¦
        del self.active_ab_tests[test_id]
        
        return winning_value
    
    def check_rollback_needed(self, parameter_name: str, recent_performance: List[float]) -> bool:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦å›æ»¾"""
        if len(recent_performance) < 20:
            return False
        
        # è¨ˆç®—æœ€è¿‘æ€§èƒ½è¶¨å‹¢
        recent_avg = np.mean(recent_performance[-10:])
        baseline_avg = np.mean(recent_performance[-20:-10])
        
        if baseline_avg > 0:
            performance_change = (recent_avg - baseline_avg) / baseline_avg
            
            if performance_change < -self.resolution_config['rollback_threshold']:
                logger.warning(f"ğŸ”„ åƒæ•¸ {parameter_name} è§¸ç™¼å›æ»¾: æ€§èƒ½ä¸‹é™ {performance_change:.2%}")
                return True
        
        return False
    
    def get_conflict_statistics(self) -> ConflictResolutionStats:
        """ç²å–è¡çªè§£æ±ºçµ±è¨ˆ"""
        if not self.conflict_history:
            return ConflictResolutionStats(
                total_conflicts=0,
                resolution_success_rate=0.0,
                most_conflicted_parameters=[],
                avg_resolution_time=0.0,
                rollback_rate=0.0,
                performance_improvement=0.0
            )
        
        # çµ±è¨ˆè¡çªåƒæ•¸
        parameter_counts = {}
        successful_resolutions = 0
        rollbacks = 0
        
        for conflict in self.conflict_history:
            param = conflict.parameter_name
            parameter_counts[param] = parameter_counts.get(param, 0) + 1
            
            if conflict.resolution_success:
                successful_resolutions += 1
            
            if conflict.rollback_triggered:
                rollbacks += 1
        
        # æ’åºæœ€å¸¸è¡çªçš„åƒæ•¸
        most_conflicted = sorted(parameter_counts.items(), key=lambda x: x[1], reverse=True)
        most_conflicted_params = [param for param, count in most_conflicted[:5]]
        
        return ConflictResolutionStats(
            total_conflicts=len(self.conflict_history),
            resolution_success_rate=successful_resolutions / len(self.conflict_history),
            most_conflicted_parameters=most_conflicted_params,
            avg_resolution_time=0.0,  # ç°¡åŒ–å¯¦ç¾
            rollback_rate=rollbacks / len(self.conflict_history),
            performance_improvement=0.0  # éœ€è¦æ›´å¤šæ•¸æ“šè¨ˆç®—
        )
    
    def save_conflict_history(self):
        """ä¿å­˜è¡çªæ­·å²"""
        try:
            data = []
            for conflict in self.conflict_history:
                conflict_dict = asdict(conflict)
                conflict_dict['conflict_timestamp'] = conflict.conflict_timestamp.isoformat()
                conflict_dict['severity'] = conflict.severity.value
                conflict_dict['resolution_method'] = conflict.resolution_method.value
                data.append(conflict_dict)
            
            with open(self.conflicts_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"âŒ è¡çªæ­·å²ä¿å­˜å¤±æ•—: {e}")
    
    def load_conflict_history(self):
        """è¼‰å…¥è¡çªæ­·å²"""
        try:
            if self.conflicts_file.exists():
                with open(self.conflicts_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.conflict_history = []
                for item in data:
                    # è½‰æ›å›æšèˆ‰å’Œæ—¥æœŸé¡å‹
                    item['conflict_timestamp'] = datetime.fromisoformat(item['conflict_timestamp'])
                    item['severity'] = ConflictSeverity(item['severity'])
                    item['resolution_method'] = ConflictResolutionMethod(item['resolution_method'])
                    
                    conflict = ParameterConflict(**item)
                    self.conflict_history.append(conflict)
                
                logger.info(f"ğŸ“š è¼‰å…¥è¡çªæ­·å²: {len(self.conflict_history)} æ¢è¨˜éŒ„")
                
        except Exception as e:
            logger.error(f"âŒ è¡çªæ­·å²è¼‰å…¥å¤±æ•—: {e}")
            self.conflict_history = []

# å…¨å±€å¯¦ä¾‹
conflict_manager = ParameterConflictManager()

async def main():
    """æ¸¬è©¦å‡½æ•¸"""
    print("âš–ï¸ Parameter Conflict Manager æ¸¬è©¦")
    
    # æ¨¡æ“¬è¡çªæª¢æ¸¬
    conflict = conflict_manager.detect_conflict(
        'signal_threshold',
        0.65,  # Phase2 å€¼
        0.55,  # Phase5 å€¼
        {
            'market_conditions': {'volatility': 0.3},
            'signal_count': 150,
            'recent_performance': 0.7
        }
    )
    
    print(f"è¡çªæª¢æ¸¬: {conflict.severity.value}, æœ€çµ‚å€¼: {conflict.final_value:.4f}")
    
    # ç²å–çµ±è¨ˆ
    stats = conflict_manager.get_conflict_statistics()
    print(f"è¡çªçµ±è¨ˆ: {stats}")
    
    print("âœ… æ¸¬è©¦å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())
