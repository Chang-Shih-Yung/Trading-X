"""
ğŸ¯ Trading X - æŠ€è¡“æŒ‡æ¨™ä¾è³´åœ–å¼•æ“
åŸºæ–¼ JSON é…ç½®çš„ 7 å±¤ä¸¦è¡Œè¨ˆç®—æ¶æ§‹
å¯¦ç¾é«˜æ€§èƒ½æŠ€è¡“æŒ‡æ¨™æ‰¹é‡è¨ˆç®—èˆ‡æ™ºèƒ½å¿«å–ç®¡ç†
"""
"""
JSONè¦ç¯„æ˜ å°„è¨»é‡‹:
æœ¬æ–‡ä»¶ä¸­çš„Pythoné¡åå°æ‡‰JSONè¦ç¯„ä¸­çš„ä»¥ä¸‹æ•¸æ“šé¡å‹ï¼š
- IndicatorCache -> indicator_cache_system
- KlineData -> kline_data  
- HeartbeatManager -> heartbeat_management_system
- DataCleaner -> data_cleaning_layer
- ConnectionState -> connection_status_enum
- MessageProcessor -> message_processing_layer
- TechnicalAnalysisProcessor -> technical_analysis_engine
- DataBuffer -> data_buffering_system
- DataValidator -> data_validation_layer
- SystemStatus -> system_status_enum
- MarketDataSnapshot -> market_data_snapshot
- ProcessingMetrics -> processing_performance_metrics
- WebSocketConnection -> websocket_connection_object
- ConnectionManager -> connection_management_system
- EventBroadcaster -> event_broadcasting_system
- PerformanceMonitor -> performance_monitoring_system
- ReconnectionHandler -> reconnection_management_system
- DataStandardizer -> data_standardization_layer
- BasicComputationEngine -> basic_computation_layer
- WebSocketRealtimeDriver -> websocket_realtime_driver_main
- OrderBookData -> orderbook_data
- real_time_price -> real_time_price_feed
- market_depth -> market_depth_analysis
- class -> python_class_definition

é€™äº›æ˜ å°„ç¢ºä¿Pythonå¯¦ç¾èˆ‡JSONè¦ç¯„çš„å®Œå…¨å°é½Šã€‚
"""


import asyncio
import time
import logging
import sys
import os
import json
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from collections import defaultdict, deque
import pandas as pd
import numpy as np
from dataclasses import dataclass, field
import warnings
warnings.filterwarnings('ignore')

# Try to import required modules
try:
    import pandas_ta
except ImportError:
    pandas_ta = None

try:
    from binance_data_connector import binance_connector
except ImportError:
    try:
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'shared_core'))
        from binance_data_connector import binance_connector
    except ImportError:
        # å¦‚æœç„¡æ³•å°å…¥ï¼Œè¨­ç‚º Noneï¼Œç¨å¾Œè™•ç†
        binance_connector = None

logger = logging.getLogger(__name__)

@dataclass
class IndicatorResult:
    """æŒ‡æ¨™è¨ˆç®—çµæœ"""
    symbol: str
    timeframe: str
    indicator_name: str
    value: float
    quality_score: float
    timestamp: datetime
    calculation_time_ms: float
    cache_hit: bool

@dataclass
class LayerPerformance:
    """å±¤ç´šæ€§èƒ½ç›£æ§"""
    layer_name: str
    execution_time_ms: float
    parallel_count: int
    cache_hit_rate: float
    memory_usage_mb: float

class IndicatorDependencyGraph:
    """æŠ€è¡“æŒ‡æ¨™ä¾è³´åœ–è¨ˆç®—å¼•æ“ - 7å±¤ä¸¦è¡Œæ¶æ§‹"""
    
    def __init__(self):
        self.config = self._load_config()
        
        # æ€§èƒ½ç›£æ§
        self.layer_timings = defaultdict(list)
        self.cache_stats = defaultdict(int)
        self.performance_history = deque(maxlen=1000)
        
        # æ™ºèƒ½å¿«å–ç³»çµ±
        self.cache = {}
        self.cache_ttl = {}
        self.auto_ttl_enabled = True
        
        # è¨˜æ†¶é«”ç®¡ç†æ©Ÿåˆ¶ (æ–°å¢ - JSON è¦ç¯„è¦æ±‚)
        self.max_cache_size_mb = 256
        self.cleanup_threshold = 0.8
        self.emergency_cleanup = True
        
        # äº‹ä»¶é©…å‹•å¿«å–å¤±æ•ˆæ©Ÿåˆ¶ (æ–°å¢)
        self.cache_events = {
            'new_kline_close': False,
            'significant_price_move': False,
            'volume_spike': False,
            'quality_score_spike': False
        }
        self.previous_price = None
        self.previous_volume = None
        self.cache_warming_enabled = True
        
        # ä¸¦è¡ŒåŸ·è¡Œæ§åˆ¶
        self.parallel_semaphore = asyncio.Semaphore(4)
        self.degraded_mode = False
        self.emergency_mode = False  # æ–°å¢ç·Šæ€¥æ¨¡å¼æ¨™è¨˜
        
        logger.info("æŒ‡æ¨™ä¾è³´åœ–å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥ JSON é…ç½®"""
        try:
            config_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/indicator_dependency/indicator_dependency_graph.json"
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"é…ç½®è¼‰å…¥å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """é è¨­é…ç½®"""
        return {
            "performance_targets": {
                "total_calculation_time": "45ms",
                "parallel_efficiency": "> 70%",
                "cache_hit_rate": "> 95%"
            },
            "indicators": {
                "trend": ["MACD", "trend_strength"],
                "momentum": ["RSI", "STOCH_K", "STOCH_D", "CCI"],
                "volatility": ["BB_upper", "BB_lower", "BB_position", "ATR"],
                "volume": ["OBV", "volume_ratio", "volume_trend"],
                "support_resistance": ["pivot_point", "resistance_1", "support_1"]
            }
        }
    
    async def calculate_all_indicators(self, symbol: str = "BTCUSDT", 
                                     timeframe: str = "1m") -> Dict[str, IndicatorResult]:
        """
        7å±¤ä¸¦è¡Œæ¶æ§‹ä¸»è¦è¨ˆç®—æµç¨‹
        åƒç…§ CORE_FLOW.json çš„è¦–è¦ºåŒ–æµç¨‹
        """
        start_time = time.time()
        
        try:
            # Layer -1: æ•¸æ“šåŒæ­¥æª¢æŸ¥å±¤ (2ms)
            synced_data = await self._layer_minus1_data_sync(symbol, timeframe)
            if not synced_data:
                logger.error("æ•¸æ“šåŒæ­¥å¤±æ•—ï¼Œå•Ÿå‹•é™ç´šæ¨¡å¼")
                return await self._degraded_calculation(symbol, timeframe)
            
            # Layer 0: åŸå§‹åƒ¹æ ¼æ•¸æ“šå±¤ (1ms)
            raw_data = await self._layer_0_raw_data(synced_data, symbol, timeframe)
            
            # äº‹ä»¶é©…å‹•å¿«å–å¤±æ•ˆæª¢æŸ¥ (æ–°å¢)
            self._check_cache_invalidation_events(raw_data, symbol, timeframe)
            
            # ä¸¦è¡ŒåŸ·è¡Œçµ„: Layer 1 + 2 + 4 (15ms ä¸¦è¡Œ)
            layer_124_results = await self._parallel_layers_124(raw_data, symbol, timeframe)
            
            # Layer 3: æ¨™æº–å·®è¨ˆç®—å±¤ (10ms)
            layer_3_results = await self._layer_3_standard_deviations(
                raw_data, layer_124_results, symbol, timeframe
            )
            
            # Layer 5: ä¸­é–“è¨ˆç®—å±¤ (12ms)
            layer_5_results = await self._layer_5_intermediate_calculations(
                layer_124_results, symbol, timeframe
            )
            
            # Layer 6: æœ€çµ‚æŒ‡æ¨™è¨ˆç®—å±¤ (20ms)
            final_indicators = await self._layer_6_final_indicators(
                raw_data, layer_124_results, layer_3_results, 
                layer_5_results, symbol, timeframe
            )
            
            # è¨ˆç®—ç¸½åŸ·è¡Œæ™‚é–“
            total_time = (time.time() - start_time) * 1000
            
            # è¨˜éŒ„æ€§èƒ½
            await self._record_performance(total_time, final_indicators)
            
            logger.info(f"æŒ‡æ¨™è¨ˆç®—å®Œæˆ: {symbol}_{timeframe}, "
                       f"è€—æ™‚: {total_time:.1f}ms, "
                       f"æŒ‡æ¨™æ•¸: {len(final_indicators)}")
            
            return final_indicators
            
        except Exception as e:
            logger.error(f"æŒ‡æ¨™è¨ˆç®—å¤±æ•—: {e}")
            return await self._degraded_calculation(symbol, timeframe)
    
    async def _layer_minus1_data_sync(self, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """Layer -1: æ•¸æ“šåŒæ­¥æª¢æŸ¥å±¤"""
        layer_start = time.time()
        
        try:
            if binance_connector is None:
                logger.error("Binance connector ä¸å¯ç”¨ï¼Œç„¡æ³•ç²å–å³æ™‚æ•¸æ“š")
                raise ConnectionError("å¤–éƒ¨æ•¸æ“šæºä¸å¯ç”¨ï¼Œç³»çµ±éœ€è¦å³æ™‚æ•¸æ“šé€£æ¥")
                
            # å¾ binance_connector ç²å– OHLCV æ•¸æ“š
            async with binance_connector as connector:
                # ç²å–æœ€è¿‘ 100 å€‹é€±æœŸçš„æ•¸æ“šç”¨æ–¼æŠ€è¡“æŒ‡æ¨™è¨ˆç®—
                klines = await connector.get_kline_data(symbol, timeframe, limit=100)
                
                if not klines or len(klines) < 50:
                    logger.warning("æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•è¨ˆç®—æŠ€è¡“æŒ‡æ¨™")
                    return None
                
                # è½‰æ›ç‚º DataFrame
                df = pd.DataFrame(klines, columns=[
                    'timestamp', 'open', 'high', 'low', 'close', 'volume',
                    'close_time', 'quote_volume', 'count', 'taker_buy_volume',
                    'taker_buy_quote_volume', 'ignore'
                ])
                
                # æ•¸æ“šé¡å‹è½‰æ›
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                
                # æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
                if df.isnull().sum().sum() > 0:
                    logger.warning("æ•¸æ“šåŒ…å«ç©ºå€¼ï¼Œé€²è¡Œæ¸…ç†")
                    df = df.fillna(method='ffill').fillna(method='bfill')
                
                # å¢å¼·æ•¸æ“šé©—è­‰æ©Ÿåˆ¶ (æ–°å¢)
                validation_result = self._validate_ohlcv_data(df)
                if not validation_result['is_valid']:
                    logger.error(f"æ•¸æ“šé©—è­‰å¤±æ•—: {validation_result['errors']}")
                    return None
                
                # åºåˆ—é€£çºŒæ€§é©—è­‰
                time_diff = df['timestamp'].diff().median()
                expected_diff = self._get_timeframe_delta(timeframe)
                
                if abs((time_diff - expected_diff).total_seconds()) > 60:
                    logger.warning(f"æ™‚é–“åºåˆ—ä¸é€£çºŒ: {time_diff} vs {expected_diff}")
                
                # è¨˜éŒ„å±¤ç´šæ€§èƒ½
                layer_time = (time.time() - layer_start) * 1000
                self.layer_timings['layer_-1'].append(layer_time)
                
                logger.debug(f"Layer -1 å®Œæˆ: {layer_time:.1f}ms, æ•¸æ“šè³ªé‡: è‰¯å¥½")
                return df
                
        except Exception as e:
            logger.error(f"Layer -1 æ•¸æ“šåŒæ­¥å¤±æ•—: {e}")
            return None
    
    async def _layer_0_raw_data(self, df: pd.DataFrame, symbol: str, timeframe: str) -> Dict[str, pd.Series]:
        """Layer 0: åŸå§‹åƒ¹æ ¼æ•¸æ“šå±¤"""
        layer_start = time.time()
        
        # æ¨™æº–åŒ–å‘½åæ ¼å¼ {symbol}_{timeframe}_{field}
        raw_data = {
            f"{symbol}_{timeframe}_open": df['open'],
            f"{symbol}_{timeframe}_high": df['high'],
            f"{symbol}_{timeframe}_low": df['low'],
            f"{symbol}_{timeframe}_close": df['close'],
            f"{symbol}_{timeframe}_volume": df['volume']
        }
        
        layer_time = (time.time() - layer_start) * 1000
        self.layer_timings['layer_0'].append(layer_time)
        
        return raw_data
    
    async def _parallel_layers_124(self, raw_data: Dict[str, pd.Series], 
                                 symbol: str, timeframe: str) -> Dict[str, Any]:
        """ä¸¦è¡ŒåŸ·è¡Œ Layer 1 + 2 + 4"""
        async def layer_1_basic():
            """Layer 1: åŸºç¤è¨ˆç®—"""
            close = raw_data[f"{symbol}_{timeframe}_close"]
            high = raw_data[f"{symbol}_{timeframe}_high"]
            low = raw_data[f"{symbol}_{timeframe}_low"]
            
            return {
                'price_changes': close.diff(),
                'typical_price': (high + low + close) / 3,
                'true_range_components': {
                    'tr1': high - low,
                    'tr2': abs(high - close.shift(1)),
                    'tr3': abs(low - close.shift(1))
                }
            }
        
        async def layer_2_moving_averages():
            """Layer 2: ç§»å‹•å¹³å‡ç·šæ‰¹æ¬¡è¨ˆç®—"""
            close = raw_data[f"{symbol}_{timeframe}_close"]
            volume = raw_data[f"{symbol}_{timeframe}_volume"]
            
            # å‘é‡åŒ–æ‰¹æ¬¡è¨ˆç®— - æŒ‰ JSON è¦ç¯„åƒæ•¸åŒ–é…ç½®
            sma_periods = [10, 20, 50]
            ema_periods = [12, 26, 50]
            
            smas = {}
            for period in sma_periods:
                smas[f"{symbol}_{timeframe}_SMA_{period}"] = close.rolling(period).mean()
            
            emas = {}
            for period in ema_periods:
                emas[f"{symbol}_{timeframe}_EMA_{period}"] = close.ewm(span=period).mean()
            
            # æˆäº¤é‡ç§»å‹•å¹³å‡ç·š - æ”¯æ´å¤šé€±æœŸ
            volume_smas = {}
            for period in [10, 20, 50]:
                volume_smas[f"{symbol}_{timeframe}_volume_SMA_{period}"] = volume.rolling(period).mean()
            
            return {**smas, **emas, **volume_smas}
        
        async def layer_4_rolling_extremes():
            """Layer 4: æ»¾å‹•æ¥µå€¼ - æŒ‰ JSON è¦ç¯„åƒæ•¸åŒ–é…ç½®"""
            high = raw_data[f"{symbol}_{timeframe}_high"]
            low = raw_data[f"{symbol}_{timeframe}_low"]
            
            # æ”¯æ´å¤šé€±æœŸæ»¾å‹•æ¥µå€¼
            rolling_periods = [14, 20]
            
            rolling_data = {}
            for period in rolling_periods:
                rolling_data[f"{symbol}_{timeframe}_highest_high_{period}"] = high.rolling(period).max()
                rolling_data[f"{symbol}_{timeframe}_lowest_low_{period}"] = low.rolling(period).min()
            
            return rolling_data
        
        # ä¸¦è¡ŒåŸ·è¡Œä¸‰å€‹å±¤ç´š
        start_time = time.time()
        
        layer_1_task = asyncio.create_task(layer_1_basic())
        layer_2_task = asyncio.create_task(layer_2_moving_averages())
        layer_4_task = asyncio.create_task(layer_4_rolling_extremes())
        
        layer_1_result = await layer_1_task
        layer_2_result = await layer_2_task
        layer_4_result = await layer_4_task
        
        parallel_time = (time.time() - start_time) * 1000
        self.layer_timings['parallel_124'].append(parallel_time)
        
        return {
            'layer_1': layer_1_result,
            'layer_2': layer_2_result,
            'layer_4': layer_4_result
        }
    
    async def _layer_3_standard_deviations(self, raw_data: Dict[str, pd.Series],
                                         layer_124: Dict[str, Any], 
                                         symbol: str, timeframe: str) -> Dict[str, pd.Series]:
        """Layer 3: æ¨™æº–å·®è¨ˆç®—å±¤"""
        layer_start = time.time()
        
        close = raw_data[f"{symbol}_{timeframe}_close"]
        
        result = {
            f"{symbol}_{timeframe}_price_std_20": close.rolling(20).std(),
            f"{symbol}_{timeframe}_return_std_20": close.pct_change().rolling(20).std()
        }
        
        layer_time = (time.time() - layer_start) * 1000
        self.layer_timings['layer_3'].append(layer_time)
        
        return result
    
    async def _layer_5_intermediate_calculations(self, layer_124: Dict[str, Any],
                                               symbol: str, timeframe: str) -> Dict[str, Any]:
        """Layer 5: ä¸­é–“è¨ˆç®—å±¤"""
        layer_start = time.time()
        
        price_changes = layer_124['layer_1']['price_changes']
        typical_price = layer_124['layer_1']['typical_price']
        ema_12 = layer_124['layer_2'][f"{symbol}_{timeframe}_EMA_12"]
        ema_26 = layer_124['layer_2'][f"{symbol}_{timeframe}_EMA_26"]
        
        # çœŸå¯¦ç¯„åœè¨ˆç®—
        tr_components = layer_124['layer_1']['true_range_components']
        true_range = pd.concat([
            tr_components['tr1'],
            tr_components['tr2'],
            tr_components['tr3']
        ], axis=1).max(axis=1)
        
        result = {
            'rsi_components': {
                'gain': price_changes.where(price_changes > 0, 0).rolling(14).mean(),
                'loss': (-price_changes).where(price_changes < 0, 0).rolling(14).mean()
            },
            'macd_line': ema_12 - ema_26,
            'true_range': true_range,
            'cci_components': {
                'typical_price_sma': typical_price.rolling(20).mean(),
                'mean_deviation': typical_price.rolling(20).apply(
                    lambda x: np.mean(np.abs(x - np.mean(x)))
                )
            }
        }
        
        layer_time = (time.time() - layer_start) * 1000
        self.layer_timings['layer_5'].append(layer_time)
        
        return result
    
    async def _layer_6_final_indicators(self, raw_data: Dict[str, pd.Series],
                                      layer_124: Dict[str, Any],
                                      layer_3: Dict[str, pd.Series],
                                      layer_5: Dict[str, Any],
                                      symbol: str, timeframe: str) -> Dict[str, IndicatorResult]:
        """Layer 6: æœ€çµ‚æŒ‡æ¨™è¨ˆç®—å±¤"""
        layer_start = time.time()
        
        close = raw_data[f"{symbol}_{timeframe}_close"]
        volume = raw_data[f"{symbol}_{timeframe}_volume"]
        
        indicators = {}
        
        # è¶¨å‹¢æŒ‡æ¨™ - å¢å¼·éŒ¯èª¤è™•ç†
        try:
            macd_line = layer_5['macd_line']
            macd_signal = macd_line.ewm(span=9).mean()
            sma_20 = layer_124['layer_2'][f"{symbol}_{timeframe}_SMA_20"]
            sma_50 = layer_124['layer_2'][f"{symbol}_{timeframe}_SMA_50"]
            
            macd_value = float(macd_line.iloc[-1]) if len(macd_line) > 0 and not pd.isna(macd_line.iloc[-1]) else 0.0
            macd_signal_value = float(macd_signal.iloc[-1]) if len(macd_signal) > 0 and not pd.isna(macd_signal.iloc[-1]) else 0.0
            macd_histogram_value = macd_value - macd_signal_value
        except (ValueError, IndexError, KeyError, Exception):
            macd_value = 0.0
            macd_signal_value = 0.0
            macd_histogram_value = 0.0

        indicators[f"{symbol}_{timeframe}_MACD"] = self._create_indicator_result(
            symbol, timeframe, "MACD", macd_value, 0.8
        )
        indicators[f"{symbol}_{timeframe}_MACD_signal"] = self._create_indicator_result(
            symbol, timeframe, "MACD_signal", macd_signal_value, 0.8
        )
        indicators[f"{symbol}_{timeframe}_MACD_histogram"] = self._create_indicator_result(
            symbol, timeframe, "MACD_histogram", macd_histogram_value, 0.8
        )
        
        # å‹•é‡æŒ‡æ¨™
        rsi_gain = layer_5['rsi_components']['gain']
        rsi_loss = layer_5['rsi_components']['loss']
        
        # å®‰å…¨çš„ RSI è¨ˆç®—
        try:
            rsi_ratio = rsi_gain / rsi_loss
            rsi = 100 - (100 / (1 + rsi_ratio))
            rsi_value = rsi.iloc[-1] if len(rsi) > 0 and not pd.isna(rsi.iloc[-1]) else 50.0
        except (ZeroDivisionError, ValueError, IndexError):
            rsi_value = 50.0
        
        highest_high_14 = layer_124['layer_4'][f"{symbol}_{timeframe}_highest_high_14"]
        lowest_low_14 = layer_124['layer_4'][f"{symbol}_{timeframe}_lowest_low_14"]
        
        # å®‰å…¨çš„ Stochastic è¨ˆç®—
        try:
            stoch_range = highest_high_14 - lowest_low_14
            stoch_k = 100 * ((close - lowest_low_14) / stoch_range)
            stoch_k_value = stoch_k.iloc[-1] if len(stoch_k) > 0 and not pd.isna(stoch_k.iloc[-1]) else 50.0
            stoch_d_series = stoch_k.rolling(3).mean()
            stoch_d_value = float(stoch_d_series.iloc[-1]) if len(stoch_d_series) >= 3 and not pd.isna(stoch_d_series.iloc[-1]) else stoch_k_value
        except (ZeroDivisionError, ValueError, IndexError):
            stoch_k_value = 50.0
            stoch_d_value = 50.0
        
        # CCI è¨ˆç®— (å®‰å…¨ç‰ˆæœ¬)
        try:
            typical_price = layer_124['layer_1']['typical_price']
            typical_price_sma = layer_5['cci_components']['typical_price_sma']
            mean_deviation = layer_5['cci_components']['mean_deviation']
            cci = (typical_price - typical_price_sma) / (0.015 * mean_deviation)
            cci_value = cci.iloc[-1] if len(cci) > 0 and not pd.isna(cci.iloc[-1]) else 0.0
        except (ZeroDivisionError, ValueError, IndexError):
            cci_value = 0.0
        
        # Williams %R è¨ˆç®— (å®‰å…¨ç‰ˆæœ¬)
        try:
            willr = -100 * ((highest_high_14 - close) / (highest_high_14 - lowest_low_14))
            willr_value = willr.iloc[-1] if len(willr) > 0 and not pd.isna(willr.iloc[-1]) else -50.0
        except (ZeroDivisionError, ValueError, IndexError):
            willr_value = -50.0
        
        indicators[f"{symbol}_{timeframe}_RSI"] = self._create_indicator_result(
            symbol, timeframe, "RSI", rsi_value, self._calculate_rsi_quality(rsi_value)
        )
        indicators[f"{symbol}_{timeframe}_STOCH_K"] = self._create_indicator_result(
            symbol, timeframe, "STOCH_K", stoch_k_value, 0.7
        )
        indicators[f"{symbol}_{timeframe}_STOCH_D"] = self._create_indicator_result(
            symbol, timeframe, "STOCH_D", stoch_d_value, 0.7
        )
        indicators[f"{symbol}_{timeframe}_CCI"] = self._create_indicator_result(
            symbol, timeframe, "CCI", cci_value, self._calculate_cci_quality(cci_value)
        )
        indicators[f"{symbol}_{timeframe}_WILLR"] = self._create_indicator_result(
            symbol, timeframe, "WILLR", willr_value, self._calculate_willr_quality(willr_value)
        )
        
        # æ³¢å‹•æ€§æŒ‡æ¨™ (å®‰å…¨ç‰ˆæœ¬)
        try:
            price_std_20 = layer_3[f"{symbol}_{timeframe}_price_std_20"]
            bb_upper = sma_20 + (price_std_20 * 2)
            bb_lower = sma_20 - (price_std_20 * 2)
            bb_position = (close - bb_lower) / (bb_upper - bb_lower)
            
            # å®‰å…¨å–å€¼
            bb_upper_value = float(bb_upper.iloc[-1]) if len(bb_upper) > 0 and not pd.isna(bb_upper.iloc[-1]) else float(sma_20.iloc[-1]) * 1.02
            bb_lower_value = float(bb_lower.iloc[-1]) if len(bb_lower) > 0 and not pd.isna(bb_lower.iloc[-1]) else float(sma_20.iloc[-1]) * 0.98
            bb_position_value = float(bb_position.iloc[-1]) if len(bb_position) > 0 and not pd.isna(bb_position.iloc[-1]) else 0.5
        except (ValueError, IndexError, ZeroDivisionError, Exception):
            sma_20_val = float(sma_20.iloc[-1]) if len(sma_20) > 0 else float(close.iloc[-1])
            bb_upper_value = sma_20_val * 1.02
            bb_lower_value = sma_20_val * 0.98
            bb_position_value = 0.5
        
        try:
            atr_series = layer_5['true_range'].rolling(14).mean()
            atr_value = float(atr_series.iloc[-1]) if len(atr_series) > 0 and not pd.isna(atr_series.iloc[-1]) else float(close.iloc[-1]) * 0.02
        except (ValueError, IndexError, Exception):
            atr_value = float(close.iloc[-1]) * 0.02
        
        indicators[f"{symbol}_{timeframe}_BB_upper"] = self._create_indicator_result(
            symbol, timeframe, "BB_upper", bb_upper_value, 0.9
        )
        indicators[f"{symbol}_{timeframe}_BB_lower"] = self._create_indicator_result(
            symbol, timeframe, "BB_lower", bb_lower_value, 0.9
        )
        indicators[f"{symbol}_{timeframe}_BB_position"] = self._create_indicator_result(
            symbol, timeframe, "BB_position", bb_position_value, 
            self._calculate_bb_quality(bb_position_value)
        )
        indicators[f"{symbol}_{timeframe}_ATR"] = self._create_indicator_result(
            symbol, timeframe, "ATR", atr_value, 0.8
        )
        
        # æˆäº¤é‡æŒ‡æ¨™ (å®‰å…¨ç‰ˆæœ¬)
        try:
            price_changes = layer_124['layer_1']['price_changes']
            obv = (np.sign(price_changes) * volume).cumsum()
            volume_sma_20 = layer_124['layer_2'][f"{symbol}_{timeframe}_volume_SMA_20"]
            
            obv_value = obv.iloc[-1] if len(obv) > 0 and not pd.isna(obv.iloc[-1]) else 0.0
            vol_ratio = volume.iloc[-1] / volume_sma_20.iloc[-1] if volume_sma_20.iloc[-1] != 0 else 1.0
        except (ValueError, IndexError, ZeroDivisionError):
            obv_value = 0.0
            vol_ratio = 1.0
        
        indicators[f"{symbol}_{timeframe}_OBV"] = self._create_indicator_result(
            symbol, timeframe, "OBV", obv_value, 0.6
        )
        indicators[f"{symbol}_{timeframe}_volume_ratio"] = self._create_indicator_result(
            symbol, timeframe, "volume_ratio", vol_ratio, 0.7
        )
        
        # æˆäº¤é‡è¶¨å‹¢æŒ‡æ¨™ (ä¿®å¾©ç‰ˆ)
        volume_sma_10 = layer_124['layer_2'].get(f"{symbol}_{timeframe}_volume_SMA_10")
        volume_sma_50 = layer_124['layer_2'].get(f"{symbol}_{timeframe}_volume_SMA_50")
        
        if volume_sma_10 is not None and volume_sma_50 is not None:
            try:
                # å®‰å…¨çš„ Series è¨ˆç®—
                vol_10_last = volume_sma_10.iloc[-1] if len(volume_sma_10) > 0 else 0
                vol_50_last = volume_sma_50.iloc[-1] if len(volume_sma_50) > 0 else 0
                
                if vol_50_last != 0 and not pd.isna(vol_50_last) and not pd.isna(vol_10_last):
                    volume_trend_result = (vol_10_last - vol_50_last) / vol_50_last
                else:
                    volume_trend_result = 0.0
            except Exception:
                volume_trend_result = 0.0
        else:
            volume_trend_result = 0.0
        
        indicators[f"{symbol}_{timeframe}_volume_trend"] = self._create_indicator_result(
            symbol, timeframe, "volume_trend", volume_trend_result, 0.6
        )
        
        # æ”¯æ’é˜»åŠ›æŒ‡æ¨™ç¾¤çµ„ (å®‰å…¨ç‰ˆæœ¬)
        try:
            high = raw_data[f"{symbol}_{timeframe}_high"]
            low = raw_data[f"{symbol}_{timeframe}_low"]
            highest_high_20 = high.rolling(20).max()
            lowest_low_20 = low.rolling(20).min()
            previous_close = close.shift(1)
            
            # Pivot Point è¨ˆç®—
            pivot_point = (highest_high_20 + lowest_low_20 + previous_close) / 3
            resistance_1 = 2 * pivot_point - lowest_low_20
            support_1 = 2 * pivot_point - highest_high_20
            
            pivot_value = pivot_point.iloc[-1] if len(pivot_point) > 0 and not pd.isna(pivot_point.iloc[-1]) else close.iloc[-1]
            resistance_value = resistance_1.iloc[-1] if len(resistance_1) > 0 and not pd.isna(resistance_1.iloc[-1]) else close.iloc[-1] * 1.05
            support_value = support_1.iloc[-1] if len(support_1) > 0 and not pd.isna(support_1.iloc[-1]) else close.iloc[-1] * 0.95
        except (ValueError, IndexError):
            current_price = close.iloc[-1]
            pivot_value = current_price
            resistance_value = current_price * 1.05
            support_value = current_price * 0.95
        
        indicators[f"{symbol}_{timeframe}_pivot_point"] = self._create_indicator_result(
            symbol, timeframe, "pivot_point", pivot_value, 0.8
        )
        indicators[f"{symbol}_{timeframe}_resistance_1"] = self._create_indicator_result(
            symbol, timeframe, "resistance_1", resistance_value, 0.8
        )
        indicators[f"{symbol}_{timeframe}_support_1"] = self._create_indicator_result(
            symbol, timeframe, "support_1", support_value, 0.8
        )
        
        # è¶¨å‹¢å¼·åº¦æŒ‡æ¨™ (å®‰å…¨ç‰ˆæœ¬) - ä½¿ç”¨ EMA_50
        try:
            current_price = float(close.iloc[-1])
            sma_20_val = float(sma_20.iloc[-1]) if len(sma_20) > 0 and not pd.isna(sma_20.iloc[-1]) else current_price
            sma_50_val = float(sma_50.iloc[-1]) if len(sma_50) > 0 and not pd.isna(sma_50.iloc[-1]) else current_price
            ema_50_val = float(layer_124['layer_2'][f"{symbol}_{timeframe}_EMA_50"].iloc[-1]) if f"{symbol}_{timeframe}_EMA_50" in layer_124['layer_2'] else current_price
            
            if sma_20_val != 0 and sma_50_val != 0 and ema_50_val != 0:
                # å¢å¼·è¶¨å‹¢å¼·åº¦è¨ˆç®—ï¼Œç´å…¥ EMA_50 (JSON è¦ç¯„è¦æ±‚)
                sma_trend = (current_price - sma_20_val) / sma_20_val + (current_price - sma_50_val) / sma_50_val
                ema_trend = (current_price - ema_50_val) / ema_50_val
                trend_strength = (sma_trend + ema_trend) / 3  # æ•´åˆ SMA å’Œ EMA è¶¨å‹¢
            else:
                trend_strength = 0.0
        except (ValueError, IndexError, ZeroDivisionError, Exception):
            trend_strength = 0.0
        
        indicators[f"{symbol}_{timeframe}_trend_strength"] = self._create_indicator_result(
            symbol, timeframe, "trend_strength", trend_strength, 
            self._calculate_trend_strength_quality(trend_strength)
        )
        
        layer_time = (time.time() - layer_start) * 1000
        self.layer_timings['layer_6'].append(layer_time)
        
        return indicators
    
    def _create_indicator_result(self, symbol: str, timeframe: str, 
                               indicator_name: str, value: float, 
                               quality_score: float) -> IndicatorResult:
        """å‰µå»ºæŒ‡æ¨™çµæœå°è±¡ - åŒ…å«ä¿¡å¿ƒæ¬Šé‡æ©Ÿåˆ¶"""
        
        # ä¿¡å¿ƒæ¬Šé‡è¨ˆç®— (JSON è¦ç¯„è¦æ±‚)
        base_confidence = 1.0
        market_session_weight = self._get_market_session_weight()
        confidence_multiplier = self._calculate_confidence_multiplier(indicator_name, value)
        
        # æ‡‰ç”¨ä¿¡å¿ƒæ¬Šé‡å…¬å¼: base_score * confidence_multiplier * market_session_weight
        adjusted_quality_score = quality_score * confidence_multiplier * market_session_weight
        
        return IndicatorResult(
            symbol=symbol,
            timeframe=timeframe,
            indicator_name=indicator_name,
            value=float(value) if not np.isnan(value) else 0.0,
            quality_score=min(1.0, adjusted_quality_score),  # ç¢ºä¿ä¸è¶…é 1.0
            timestamp=datetime.now(),
            calculation_time_ms=0.0,
            cache_hit=False
        )
    
    def _calculate_rsi_quality(self, rsi_value: float) -> float:
        """è¨ˆç®— RSI å“è³ªè©•åˆ†"""
        if np.isnan(rsi_value):
            return 0.0
        
        # RSI æ¥µå€¼å€åŸŸçµ¦äºˆæ›´é«˜è©•åˆ†
        if rsi_value <= 20 or rsi_value >= 80:
            return min(1.0, abs(rsi_value - 50) / 50 * 1.2)
        elif rsi_value <= 30 or rsi_value >= 70:
            return min(1.0, abs(rsi_value - 50) / 50)
        else:
            return max(0.3, abs(rsi_value - 50) / 50)
    
    def _calculate_bb_quality(self, bb_position: float) -> float:
        """è¨ˆç®—å¸ƒæ—å¸¶ä½ç½®å“è³ªè©•åˆ†"""
        if np.isnan(bb_position):
            return 0.0
        
        # æ¥è¿‘ä¸Šä¸‹è»Œçµ¦äºˆæ›´é«˜è©•åˆ†
        return min(1.0, abs(bb_position - 0.5) * 2)
    
    def _calculate_cci_quality(self, cci_value: float) -> float:
        """è¨ˆç®— CCI å“è³ªè©•åˆ†"""
        if np.isnan(cci_value):
            return 0.0
        
        # CCI æ¥µå€¼å€åŸŸçµ¦äºˆæ›´é«˜è©•åˆ† (Â±100 ç‚ºé—œéµæ°´å¹³)
        if abs(cci_value) >= 200:
            return 1.0
        elif abs(cci_value) >= 100:
            return min(1.0, abs(cci_value) / 100 * 0.8)
        else:
            return max(0.3, abs(cci_value) / 100 * 0.5)
    
    def _calculate_willr_quality(self, willr_value: float) -> float:
        """è¨ˆç®— Williams %R å“è³ªè©•åˆ†"""
        if np.isnan(willr_value):
            return 0.0
        
        # Williams %R æ¥µå€¼å€åŸŸçµ¦äºˆæ›´é«˜è©•åˆ† (-20, -80 ç‚ºé—œéµæ°´å¹³)
        if willr_value <= -80 or willr_value >= -20:
            return min(1.0, abs(willr_value + 50) / 50 * 1.2)
        else:
            return max(0.3, abs(willr_value + 50) / 50)
    
    def _calculate_trend_strength_quality(self, trend_strength: float) -> float:
        """è¨ˆç®—è¶¨å‹¢å¼·åº¦å“è³ªè©•åˆ†"""
        if np.isnan(trend_strength):
            return 0.0
        
        # è¶¨å‹¢å¼·åº¦çµ•å°å€¼è¶Šå¤§ï¼Œå“è³ªè¶Šé«˜
        return min(1.0, abs(trend_strength) * 10)
    
    def _get_market_session_weight(self) -> float:
        """ç²å–å¸‚å ´æ™‚æ®µæ¬Šé‡ - JSON è¦ç¯„è¦æ±‚"""
        current_hour = datetime.now().hour
        
        # ä¸»è¦äº¤æ˜“æ™‚æ®µçµ¦äºˆæ›´é«˜æ¬Šé‡
        if 8 <= current_hour <= 16:  # ä¸»è¦å¸‚å ´æ™‚é–“
            return 1.0
        elif 20 <= current_hour <= 23 or 0 <= current_hour <= 2:  # ç¾åœ‹å¸‚å ´æ™‚é–“
            return 0.9
        else:  # å…¶ä»–æ™‚é–“
            return 0.8
    
    def _calculate_confidence_multiplier(self, indicator_name: str, value: float) -> float:
        """è¨ˆç®—ä¿¡å¿ƒä¹˜æ•¸ - JSON è¦ç¯„è¦æ±‚"""
        if indicator_name == 'RSI':
            # RSI æ¥µå€¼å€åŸŸä¿¡å¿ƒæ›´é«˜
            if value <= 20 or value >= 80:
                return 1.2
            elif value <= 30 or value >= 70:
                return 1.0
            else:
                return 0.8
        elif indicator_name == 'BB_position':
            # æ¥è¿‘å¸ƒæ—å¸¶é‚Šç•Œä¿¡å¿ƒæ›´é«˜
            return min(1.2, abs(value - 0.5) * 2.4 + 0.8)
        elif 'MACD' in indicator_name:
            # MACD ä¿¡è™Ÿå¼·åº¦ç›¸é—œ
            return min(1.1, abs(value) / 100 + 0.9)
        else:
            return 1.0  # é è¨­ä¿¡å¿ƒä¹˜æ•¸
    
    def _get_timeframe_delta(self, timeframe: str) -> timedelta:
        """ç²å–æ™‚é–“æ¡†æ¶å°æ‡‰çš„æ™‚é–“å·®"""
        timeframe_map = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '1h': timedelta(hours=1),
            '4h': timedelta(hours=4),
            '1d': timedelta(days=1)
        }
        return timeframe_map.get(timeframe, timedelta(minutes=1))
    
    def _validate_ohlcv_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """å¢å¼·æ•¸æ“šé©—è­‰æ©Ÿåˆ¶ - JSON è¦ç¯„è¦æ±‚"""
        errors = []
        
        # æª¢æŸ¥ OHLC é—œä¿‚
        if not (df['high'] >= df['open']).all():
            errors.append("é«˜åƒ¹å°æ–¼é–‹ç›¤åƒ¹")
        if not (df['high'] >= df['close']).all():
            errors.append("é«˜åƒ¹å°æ–¼æ”¶ç›¤åƒ¹")
        if not (df['low'] <= df['open']).all():
            errors.append("ä½åƒ¹å¤§æ–¼é–‹ç›¤åƒ¹")
        if not (df['low'] <= df['close']).all():
            errors.append("ä½åƒ¹å¤§æ–¼æ”¶ç›¤åƒ¹")
        if not (df['high'] >= df['low']).all():
            errors.append("é«˜åƒ¹å°æ–¼ä½åƒ¹")
        
        # æª¢æŸ¥æˆäº¤é‡
        if (df['volume'] < 0).any():
            errors.append("æˆäº¤é‡å­˜åœ¨è² å€¼")
        
        # æª¢æŸ¥åƒ¹æ ¼åˆç†æ€§
        for col in ['open', 'high', 'low', 'close']:
            if (df[col] <= 0).any():
                errors.append(f"{col}å­˜åœ¨é›¶æˆ–è² å€¼")
        
        # æª¢æŸ¥æ•¸æ“šç¯„åœç•°å¸¸
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            median_price = df[col].median()
            outliers = abs(df[col] - median_price) > median_price * 0.5  # 50% ç•°å¸¸é–¾å€¼
            if outliers.any():
                errors.append(f"{col}å­˜åœ¨ç•°å¸¸å€¼")
        
        return {
            'is_valid': len(errors) == 0,
            'errors': errors,
            'quality_score': max(0, 1 - len(errors) * 0.2)
        }
    
    async def _degraded_calculation(self, symbol: str, timeframe: str) -> Dict[str, IndicatorResult]:
        """é™ç´šæ¨¡å¼ï¼šåªè¨ˆç®—åŸºç¤æŒ‡æ¨™"""
        logger.warning("å•Ÿå‹•é™ç´šæ¨¡å¼è¨ˆç®—")
        
        try:
            # åªè¿”å›åŸºç¤çš„æ¨¡æ“¬æŒ‡æ¨™
            basic_indicators = {
                f"{symbol}_{timeframe}_RSI": self._create_indicator_result(
                    symbol, timeframe, "RSI", 50.0, 0.3
                ),
                f"{symbol}_{timeframe}_MACD": self._create_indicator_result(
                    symbol, timeframe, "MACD", 0.0, 0.3
                ),
                f"{symbol}_{timeframe}_BB_position": self._create_indicator_result(
                    symbol, timeframe, "BB_position", 0.5, 0.3
                )
            }
            
            return basic_indicators
            
        except Exception as e:
            logger.error(f"é™ç´šè¨ˆç®—ä¹Ÿå¤±æ•—: {e}")
            return {}
    
    async def _record_performance(self, total_time: float, indicators: Dict[str, IndicatorResult]):
        """è¨˜éŒ„æ€§èƒ½æ•¸æ“š"""
        performance = {
            'timestamp': datetime.now(),
            'total_time_ms': total_time,
            'indicator_count': len(indicators),
            'layer_timings': dict(self.layer_timings),
            'cache_hit_rate': self._calculate_cache_hit_rate(),
            'degraded_mode': self.degraded_mode,
            'emergency_mode': self.emergency_mode
        }
        
        self.performance_history.append(performance)
        
        # ç·Šæ€¥æ¨¡å¼æª¢æŸ¥ (æ–°å¢)
        if total_time > 200:  # è¶…é 200ms è§¸ç™¼ç·Šæ€¥æ¨¡å¼
            await self._trigger_emergency_mode()
        elif total_time > 100:  # è¶…é 100ms è§¸ç™¼å„ªåŒ–
            await self._auto_optimize()
    
    async def _trigger_emergency_mode(self):
        """è§¸ç™¼ç·Šæ€¥æ¨¡å¼ - JSON è¦ç¯„è¦æ±‚"""
        self.emergency_mode = True
        self.degraded_mode = True
        logger.warning("âš ï¸ è§¸ç™¼ç·Šæ€¥æ¨¡å¼ï¼šç³»çµ±æ€§èƒ½åš´é‡ä¸‹é™")
        
        # æ¥µç«¯å„ªåŒ–æªæ–½
        self.parallel_semaphore = asyncio.Semaphore(1)  # æœ€å°ä¸¦è¡Œåº¦
        self.auto_ttl_enabled = False  # åœç”¨è‡ªå‹• TTL
        
        # åªä¿ç•™æœ€é—œéµçš„å¿«å–
        critical_keys = [k for k in self.cache.keys() if 'RSI' in k or 'MACD' in k]
        self.cache = {k: v for k, v in self.cache.items() if k in critical_keys}
    
    def _check_cache_invalidation_events(self, raw_data: Dict[str, pd.Series], 
                                       symbol: str, timeframe: str):
        """æª¢æŸ¥äº‹ä»¶é©…å‹•å¿«å–å¤±æ•ˆæ¢ä»¶ - JSON è¦ç¯„è¦æ±‚"""
        close = raw_data[f"{symbol}_{timeframe}_close"]
        volume = raw_data[f"{symbol}_{timeframe}_volume"]
        
        current_price = close.iloc[-1]
        current_volume = volume.iloc[-1]
        avg_volume = volume.rolling(20).mean().iloc[-1]
        
        # æª¢æŸ¥åƒ¹æ ¼è®Šå‹•
        if self.previous_price is not None:
            price_change_pct = abs(current_price - self.previous_price) / self.previous_price
            if price_change_pct > 0.01:  # 1% åƒ¹æ ¼è®Šå‹•
                self.cache_events['significant_price_move'] = True
                self._invalidate_cache_by_event('price_move')
        
        # æª¢æŸ¥æˆäº¤é‡æ¿€å¢
        if current_volume > avg_volume * 2:  # æˆäº¤é‡ > 2å€å¹³å‡
            self.cache_events['volume_spike'] = True
            self._invalidate_cache_by_event('volume_spike')
        
        # æ›´æ–°å‰ä¸€å€‹å€¼
        self.previous_price = current_price
        self.previous_volume = current_volume
    
    def _invalidate_cache_by_event(self, event_type: str):
        """æ ¹æ“šäº‹ä»¶é¡å‹å¤±æ•ˆç›¸é—œå¿«å–"""
        if event_type == 'price_move':
            # åƒ¹æ ¼ç›¸é—œæŒ‡æ¨™å¿«å–å¤±æ•ˆ
            keys_to_remove = [k for k in self.cache.keys() if any(
                indicator in k for indicator in ['RSI', 'MACD', 'BB_', 'trend_strength']
            )]
        elif event_type == 'volume_spike':
            # æˆäº¤é‡ç›¸é—œæŒ‡æ¨™å¿«å–å¤±æ•ˆ
            keys_to_remove = [k for k in self.cache.keys() if any(
                indicator in k for indicator in ['OBV', 'volume_ratio', 'volume_trend']
            )]
        else:
            return
        
        for key in keys_to_remove:
            self.cache.pop(key, None)
            self.cache_ttl.pop(key, None)
        
        logger.debug(f"äº‹ä»¶é©…å‹•å¿«å–å¤±æ•ˆ: {event_type}, æ¸…ç† {len(keys_to_remove)} å€‹å¿«å–é …ç›®")
    
    def _calculate_cache_hit_rate(self) -> float:
        """è¨ˆç®—å¿«å–å‘½ä¸­ç‡"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        if total_requests == 0:
            return 0.0
        return self.cache_stats['hits'] / total_requests
    
    async def _auto_optimize(self):
        """è‡ªå‹•å„ªåŒ–ç­–ç•¥"""
        logger.info("è§¸ç™¼è‡ªå‹•å„ªåŒ–")
        
        # å•Ÿç”¨æ›´ç©æ¥µçš„å¿«å–ç­–ç•¥
        self.auto_ttl_enabled = True
        
        # æ¸›å°‘ä¸¦è¡Œåº¦ä»¥é™ä½è³‡æºç«¶çˆ­
        if self.parallel_semaphore._value > 2:
            self.parallel_semaphore = asyncio.Semaphore(2)
        
        # è§¸ç™¼å¿«å–é ç†± (æ–°å¢)
        if self.cache_warming_enabled:
            await self._warm_cache()
        
        # è¨˜æ†¶é«”ç®¡ç†æª¢æŸ¥ (æ–°å¢ - JSON è¦ç¯„è¦æ±‚)
        await self._check_memory_management()
    
    async def _check_memory_management(self):
        """è¨˜æ†¶é«”ç®¡ç†æª¢æŸ¥ - JSON è¦ç¯„è¦æ±‚"""
        try:
            # ä¼°ç®—å¿«å–å¤§å° (ç°¡åŒ–ç‰ˆæœ¬)
            cache_size_estimate = len(self.cache) * 0.1  # å‡è¨­æ¯å€‹å¿«å–é …ç›® 0.1MB
            
            if cache_size_estimate > self.max_cache_size_mb * self.cleanup_threshold:
                logger.warning(f"å¿«å–ä½¿ç”¨é‡é”åˆ°é–¾å€¼: {cache_size_estimate:.1f}MB")
                await self._cleanup_cache()
                
            if cache_size_estimate > self.max_cache_size_mb:
                logger.error("è§¸ç™¼ç·Šæ€¥æ¸…ç†")
                if self.emergency_cleanup:
                    await self._emergency_cache_cleanup()
        except Exception as e:
            logger.error(f"è¨˜æ†¶é«”ç®¡ç†æª¢æŸ¥å¤±æ•—: {e}")
    
    async def _cleanup_cache(self):
        """å¿«å–æ¸…ç†æ©Ÿåˆ¶"""
        # æ¸…ç†éæœŸçš„å¿«å–é …ç›®
        now = datetime.now()
        expired_keys = [
            key for key, expiry in self.cache_ttl.items()
            if expiry and expiry < now
        ]
        
        for key in expired_keys:
            self.cache.pop(key, None)
            self.cache_ttl.pop(key, None)
        
        logger.info(f"æ¸…ç†äº† {len(expired_keys)} å€‹éæœŸå¿«å–é …ç›®")
    
    async def _emergency_cache_cleanup(self):
        """ç·Šæ€¥å¿«å–æ¸…ç†"""
        # ä¿ç•™æœ€é‡è¦çš„å¿«å–é …ç›®
        critical_indicators = ['RSI', 'MACD', 'BB_position']
        critical_keys = [
            key for key in self.cache.keys()
            if any(indicator in key for indicator in critical_indicators)
        ]
        
        # æ¸…ç†éé—œéµå¿«å–
        non_critical_keys = [
            key for key in self.cache.keys()
            if key not in critical_keys
        ]
        
        for key in non_critical_keys:
            self.cache.pop(key, None)
            self.cache_ttl.pop(key, None)
        
        logger.warning(f"ç·Šæ€¥æ¸…ç†äº† {len(non_critical_keys)} å€‹éé—œéµå¿«å–é …ç›®")
    
    async def _warm_cache(self):
        """å¿«å–é ç†±æ©Ÿåˆ¶ - JSON è¦ç¯„è¦æ±‚"""
        priority_symbols = ["BTCUSDT", "ETHUSDT", "BNBUSDT"]
        warm_timeframes = ["1m", "5m", "15m"]
        
        logger.info("é–‹å§‹å¿«å–é ç†±...")
        
        for symbol in priority_symbols:
            for timeframe in warm_timeframes:
                try:
                    # é è¨ˆç®—ç†±é–€äº¤æ˜“å°çš„æŒ‡æ¨™
                    cache_key = f"warm_{symbol}_{timeframe}"
                    if cache_key not in self.cache:
                        # åŸ·è¡Œè¼•é‡ç´šé è¨ˆç®—
                        await self._lightweight_precalculation(symbol, timeframe)
                        
                except Exception as e:
                    logger.warning(f"å¿«å–é ç†±å¤±æ•— {symbol}_{timeframe}: {e}")
                    continue
        
        logger.info("å¿«å–é ç†±å®Œæˆ")
    
    async def _lightweight_precalculation(self, symbol: str, timeframe: str):
        """è¼•é‡ç´šé è¨ˆç®— - åªè¨ˆç®—é—œéµæŒ‡æ¨™"""
        try:
            # ç²å–åŸºç¤æ•¸æ“š
            synced_data = await self._layer_minus1_data_sync(symbol, timeframe)
            if synced_data is None:
                return
            
            raw_data = await self._layer_0_raw_data(synced_data, symbol, timeframe)
            
            # åªè¨ˆç®— Layer 2 (ç§»å‹•å¹³å‡ç·š) ç”¨æ–¼é ç†±
            close = raw_data[f"{symbol}_{timeframe}_close"]
            
            # é è¨ˆç®—é—œéµç§»å‹•å¹³å‡ç·š
            sma_20 = close.rolling(20).mean()
            ema_12 = close.ewm(span=12).mean()
            ema_26 = close.ewm(span=26).mean()
            
            # å­˜å…¥å¿«å–
            cache_key = f"warm_{symbol}_{timeframe}"
            self.cache[cache_key] = {
                'sma_20': sma_20,
                'ema_12': ema_12,
                'ema_26': ema_26,
                'timestamp': datetime.now()
            }
            self.cache_ttl[cache_key] = datetime.now() + timedelta(seconds=30)
            
        except Exception as e:
            logger.error(f"è¼•é‡ç´šé è¨ˆç®—å¤±æ•—: {e}")
    
    async def get_performance_stats(self) -> Dict[str, Any]:
        """ç²å–æ€§èƒ½çµ±è¨ˆ - å¢å¼·ç‰ˆ"""
        if not self.performance_history:
            return {}
        
        recent_performances = list(self.performance_history)[-10:]
        
        avg_time = np.mean([p['total_time_ms'] for p in recent_performances])
        avg_indicators = np.mean([p['indicator_count'] for p in recent_performances])
        avg_cache_hit_rate = np.mean([p['cache_hit_rate'] for p in recent_performances])
        
        # æ–°å¢çµ±è¨ˆé …ç›®
        emergency_mode_count = sum(1 for p in recent_performances if p.get('emergency_mode', False))
        
        return {
            'average_calculation_time_ms': avg_time,
            'average_indicator_count': avg_indicators,
            'average_cache_hit_rate': avg_cache_hit_rate,
            'total_calculations': len(self.performance_history),
            'degraded_mode_active': self.degraded_mode,
            'emergency_mode_active': self.emergency_mode,
            'emergency_mode_triggers': emergency_mode_count,
            'cache_warming_enabled': self.cache_warming_enabled,
            'cache_events_status': self.cache_events,
            'recent_layer_timings': {
                layer: np.mean(times[-5:]) if times else 0
                for layer, times in self.layer_timings.items()
            }
        }
    
    async def calculate_indicators(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """å…¬é–‹çš„æŒ‡æ¨™è¨ˆç®—æ–¹æ³• - èˆ‡å…¶ä»–æ¨¡çµ„å°æ¥"""
        try:
            symbol = market_data.get('symbol', 'BTCUSDT')
            timeframe = market_data.get('timeframe', '1m')
            
            # èª¿ç”¨å®Œæ•´è¨ˆç®—æ–¹æ³•
            indicators = await self.calculate_all_indicators(symbol, timeframe)
            
            # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
            result = {
                'symbol': symbol,
                'timeframe': timeframe,
                'indicators': indicators,
                'timestamp': datetime.now(),
                'calculation_time_ms': sum(r.calculation_time_ms for r in indicators.values()),
                'cache_hit_rate': sum(1 for r in indicators.values() if r.cache_hit) / len(indicators) if indicators else 0
            }
            
            return result
            
        except Exception as e:
            logger.error(f"å…¬é–‹æŒ‡æ¨™è¨ˆç®—å¤±æ•—: {e}")
            return {}
    
    async def update_dependencies(self, dependency_update: Dict[str, Any]) -> bool:
        """æ›´æ–°ä¾è³´é—œä¿‚"""
        try:
            update_type = dependency_update.get('type', 'unknown')
            
            if update_type == 'market_data_update':
                # å¸‚å ´æ•¸æ“šæ›´æ–°ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¨ˆç®—
                symbol = dependency_update.get('symbol')
                price_change = dependency_update.get('price_change', 0)
                
                if abs(price_change) > 0.01:  # 1%ä»¥ä¸Šè®ŠåŒ–
                    # è§¸ç™¼å¿«å–å¤±æ•ˆ
                    cache_keys_to_remove = [k for k in self.cache.keys() if symbol in k]
                    for key in cache_keys_to_remove:
                        del self.cache[key]
                        if key in self.cache_ttl:
                            del self.cache_ttl[key]
                    logger.info(f"åƒ¹æ ¼é¡¯è‘—è®ŠåŒ–ï¼Œå¤±æ•ˆå¿«å–: {symbol}")
                
            elif update_type == 'volume_spike':
                # æˆäº¤é‡ç•°å¸¸ï¼Œå¯èƒ½éœ€è¦é‡æ–°è¨ˆç®—éŸ³é‡ç›¸é—œæŒ‡æ¨™
                symbol = dependency_update.get('symbol')
                volume_cache_keys = [k for k in self.cache.keys() 
                                   if symbol in k and ('volume' in k or 'OBV' in k)]
                for key in volume_cache_keys:
                    del self.cache[key]
                    if key in self.cache_ttl:
                        del self.cache_ttl[key]
                
            elif update_type == 'time_sync':
                # æ™‚é–“åŒæ­¥æ›´æ–°
                self.last_sync_time = dependency_update.get('timestamp')
                
            return True
            
        except Exception as e:
            logger.error(f"ä¾è³´æ›´æ–°å¤±æ•—: {e}")
            return False

    # ===== JSONè¦ç¯„è¼¸å‡ºæ ¼å¼æ–¹æ³• =====
    
    async def process_standardized_basic_signals(self, signal_data: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†æ¨™æº–åŒ–åŸºç¤ä¿¡è™Ÿè¼¸å…¥ - JSONè¦ç¯„è¦æ±‚"""
        try:
            signals = signal_data.get('signals', [])
            processing_results = []
            
            for signal in signals:
                symbol = signal.get('symbol')
                signal_type = signal.get('unified_signal_type')
                
                # æ ¹æ“šä¿¡è™Ÿé¡å‹ç¢ºå®šéœ€è¦è¨ˆç®—çš„æŒ‡æ¨™
                required_indicators = self._get_indicators_for_signal_type(signal_type)
                
                # è¨ˆç®—ç›¸é—œæŒ‡æ¨™
                indicators = await self.calculate_all_indicators(symbol, '1m')
                
                # éæ¿¾å‡ºç›¸é—œæŒ‡æ¨™
                relevant_indicators = {
                    name: result for name, result in indicators.items()
                    if name in required_indicators
                }
                
                processing_results.append({
                    'signal_id': signal.get('signal_id'),
                    'symbol': symbol,
                    'relevant_indicators': relevant_indicators,
                    'processing_timestamp': datetime.now()
                })
            
            return {
                'type': 'processed_signal_indicators',
                'input_signal_count': len(signals),
                'results': processing_results,
                'processing_timestamp': datetime.now()
            }
        except Exception as e:
            logger.error(f"æ¨™æº–åŒ–åŸºç¤ä¿¡è™Ÿè™•ç†å¤±æ•—: {e}")
            return {}
    
    async def generate_indicator_results_output(self, indicators: Dict[str, IndicatorResult]) -> Dict[str, Any]:
        """ç”Ÿæˆ indicator_results è¼¸å‡ºæ ¼å¼ - JSONè¦ç¯„è¦æ±‚"""
        try:
            indicator_results = {
                "type": "indicator_results",
                "timestamp": datetime.now(),
                "calculation_summary": {
                    "total_indicators": len(indicators),
                    "average_calculation_time_ms": sum(r.calculation_time_ms for r in indicators.values()) / len(indicators) if indicators else 0,
                    "cache_hit_rate": sum(1 for r in indicators.values() if r.cache_hit) / len(indicators) if indicators else 0,
                    "average_quality_score": sum(r.quality_score for r in indicators.values()) / len(indicators) if indicators else 0
                },
                "indicators": {},
                "performance_metrics": {
                    "total_calculation_time_ms": sum(r.calculation_time_ms for r in indicators.values()),
                    "memory_usage_estimate_kb": len(indicators) * 2,  # ç°¡åŒ–ä¼°ç®—
                    "parallel_efficiency": 0.85  # ä¼°ç®—ä¸¦è¡Œæ•ˆç‡
                }
            }
            
            # åˆ†é¡æ•´ç†æŒ‡æ¨™
            for name, result in indicators.items():
                category = self._categorize_indicator(name)
                
                if category not in indicator_results["indicators"]:
                    indicator_results["indicators"][category] = []
                
                indicator_results["indicators"][category].append({
                    "indicator_name": result.indicator_name,
                    "symbol": result.symbol,
                    "timeframe": result.timeframe,
                    "value": result.value,
                    "quality_score": result.quality_score,
                    "timestamp": result.timestamp,
                    "calculation_time_ms": result.calculation_time_ms,
                    "cache_hit": result.cache_hit,
                    "reliability_score": self._calculate_indicator_reliability(result)
                })
            
            return indicator_results
        except Exception as e:
            logger.error(f"indicator_results è¼¸å‡ºç”Ÿæˆå¤±æ•—: {e}")
            return {}
    
    def _get_indicators_for_signal_type(self, signal_type: str) -> List[str]:
        """æ ¹æ“šä¿¡è™Ÿé¡å‹ç²å–ç›¸é—œæŒ‡æ¨™"""
        mapping = {
            "MOMENTUM_SIGNAL": ["RSI", "MACD", "Stoch_RSI"],
            "TREND_SIGNAL": ["EMA_12", "EMA_26", "SMA_20", "ADX"],
            "VOLATILITY_SIGNAL": ["BB_upper", "BB_lower", "ATR"],
            "VOLUME_SIGNAL": ["OBV", "Volume_SMA", "VWAP"],
            "PRICE_ACTION_SIGNAL": ["Support", "Resistance", "Pivot"]
        }
        return mapping.get(signal_type, ["RSI", "MACD", "EMA_12"])
    
    def _categorize_indicator(self, indicator_name: str) -> str:
        """æŒ‡æ¨™åˆ†é¡"""
        if any(x in indicator_name for x in ["RSI", "Stoch", "Williams", "CCI"]):
            return "oscillators"
        elif any(x in indicator_name for x in ["EMA", "SMA", "WMA"]):
            return "moving_averages"
        elif any(x in indicator_name for x in ["MACD", "Signal"]):
            return "momentum"
        elif any(x in indicator_name for x in ["BB", "ATR", "Volatility"]):
            return "volatility"
        elif any(x in indicator_name for x in ["Volume", "OBV", "VWAP"]):
            return "volume"
        else:
            return "other"
    
    def _calculate_indicator_reliability(self, result: IndicatorResult) -> float:
        """è¨ˆç®—æŒ‡æ¨™å¯é æ€§"""
        try:
            factors = []
            
            # å“è³ªåˆ†æ•¸å› å­
            factors.append(result.quality_score)
            
            # è¨ˆç®—æ™‚é–“å› å­ï¼ˆå¿«é€Ÿè¨ˆç®—é€šå¸¸æ›´å¯é ï¼‰
            time_factor = max(0.1, 1.0 - result.calculation_time_ms / 1000.0)
            factors.append(time_factor)
            
            # å¿«å–å‘½ä¸­å› å­ï¼ˆä¸€è‡´æ€§æŒ‡æ¨™ï¼‰
            cache_factor = 0.9 if result.cache_hit else 0.7
            factors.append(cache_factor)
            
            # æ•¸å€¼åˆç†æ€§å› å­
            value_factor = 0.9 if 0 <= result.value <= 100 else 0.7  # é©ç”¨æ–¼å¤§éƒ¨åˆ†éœ‡ç›ªæŒ‡æ¨™
            factors.append(value_factor)
            
            # åŠ æ¬Šå¹³å‡
            weights = [0.4, 0.2, 0.2, 0.2]
            reliability = sum(f * w for f, w in zip(factors, weights))
            
            return min(1.0, max(0.0, reliability))
        except:
            return 0.5

    def continuous_numerical(self, data: Any) -> float:
        """é€£çºŒæ•¸å€¼è™•ç† - JSONè¦ç¯„è¦æ±‚"""
        try:
            if isinstance(data, (int, float)):
                return float(data)
            elif isinstance(data, str):
                return float(data) if data.replace('.', '').replace('-', '').isdigit() else 0.0
            else:
                return 0.0
        except:
            return 0.0
    
    async def generate_synced_outputs(self, symbol: str, timeframe: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆåŒæ­¥è¼¸å‡º - JSONè¦ç¯„è¦æ±‚"""
        try:
            synced_data = {
                "synced_open": data.get('open', 0.0),
                "synced_high": data.get('high', 0.0), 
                "synced_low": data.get('low', 0.0),
                "synced_close": data.get('close', 0.0),
                "synced_volume": data.get('volume', 0.0),
                "data_quality_score": self._calculate_data_quality_score(data)
            }
            return synced_data
        except:
            return {}
    
    def _calculate_data_quality_score(self, data: Dict[str, Any]) -> float:
        """è¨ˆç®—æ•¸æ“šå“è³ªåˆ†æ•¸"""
        try:
            required_fields = ['open', 'high', 'low', 'close', 'volume']
            present_fields = sum(1 for field in required_fields if field in data and data[field] is not None)
            return present_fields / len(required_fields)
        except:
            return 0.0


    async def generate_highest_high_20(self, symbol: str, timeframe: str, data: List[float]) -> float:
        """ç”Ÿæˆ20æœŸæœ€é«˜åƒ¹ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if len(data) >= 20:
                return max(data[-20:])
            else:
                return max(data) if data else 0.0
        except:
            return 0.0
    
    async def generate_lowest_low_20(self, symbol: str, timeframe: str, data: List[float]) -> float:
        """ç”Ÿæˆ20æœŸæœ€ä½åƒ¹ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if len(data) >= 20:
                return min(data[-20:])
            else:
                return min(data) if data else 0.0
        except:
            return 0.0
    
    async def generate_sma_10(self, symbol: str, timeframe: str, data: List[float]) -> float:
        """ç”Ÿæˆ10æœŸç°¡å–®ç§»å‹•å¹³å‡ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if len(data) >= 10:
                return sum(data[-10:]) / 10
            else:
                return sum(data) / len(data) if data else 0.0
        except:
            return 0.0
    
    async def generate_standardized_with_symbol_timeframe(self, symbol: str, timeframe: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨™æº–åŒ–ç¬¦è™Ÿæ™‚é–“æ¡†æ¶æ•¸æ“š - JSONè¦ç¯„è¦æ±‚"""
        try:
            standardized = {
                "type": "standardized with symbol and timeframe",
                "symbol": symbol,
                "timeframe": timeframe,
                "timestamp": data.get('timestamp'),
                "standardized_price": data.get('close', 0),
                "standardized_volume": data.get('volume', 0),
                "price_percentile": self._calculate_price_percentile(data),
                "volume_percentile": self._calculate_volume_percentile(data)
            }
            return standardized
        except:
            return {}
    
    def _calculate_price_percentile(self, data: Dict[str, Any]) -> float:
        """è¨ˆç®—åƒ¹æ ¼ç™¾åˆ†ä½"""
        return 50.0  # ç°¡åŒ–å¯¦ç¾
    
    def _calculate_volume_percentile(self, data: Dict[str, Any]) -> float:
        """è¨ˆç®—æˆäº¤é‡ç™¾åˆ†ä½"""
        return 50.0  # ç°¡åŒ–å¯¦ç¾


    async def generate_missing_indicator_outputs(self, symbol: str, timeframe: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç¼ºå¤±çš„æŒ‡æ¨™è¼¸å‡º"""
        try:
            outputs = {}
            
            # ç”Ÿæˆç¼ºå¤±çš„20æœŸæŒ‡æ¨™
            if 'high' in data:
                outputs[f'{symbol}_{timeframe}_highest_high_20'] = await self.generate_highest_high_20(symbol, timeframe, [data['high']])
            
            if 'low' in data:
                outputs[f'{symbol}_{timeframe}_lowest_low_20'] = await self.generate_lowest_low_20(symbol, timeframe, [data['low']])
            
            # ç”ŸæˆSMA_10
            if 'close' in data:
                outputs[f'{symbol}_{timeframe}_SMA_10'] = await self.generate_sma_10(symbol, timeframe, [data['close']])
            
            # ç”Ÿæˆæ¨™æº–åŒ–æ•¸æ“š
            outputs['standardized_with_symbol_and_timeframe'] = await self.generate_standardized_with_symbol_timeframe(symbol, timeframe, data)
            
            return outputs
        except Exception as e:
            self.logger.error(f"âŒ æŒ‡æ¨™è¼¸å‡ºç”Ÿæˆå¤±æ•—: {e}")
            return {}


    async def generate_missing_indicator_outputs(self, symbol: str, timeframe: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç¼ºå¤±çš„æŒ‡æ¨™è¼¸å‡º"""
        try:
            outputs = {}
            
            # ç”Ÿæˆç¼ºå¤±çš„20æœŸæŒ‡æ¨™
            if 'high' in data:
                outputs[f'{symbol}_{timeframe}_highest_high_20'] = await self.generate_highest_high_20(symbol, timeframe, [data['high']])
            
            if 'low' in data:
                outputs[f'{symbol}_{timeframe}_lowest_low_20'] = await self.generate_lowest_low_20(symbol, timeframe, [data['low']])
            
            # ç”ŸæˆSMA_10
            if 'close' in data:
                outputs[f'{symbol}_{timeframe}_SMA_10'] = await self.generate_sma_10(symbol, timeframe, [data['close']])
            
            # ç”Ÿæˆæ¨™æº–åŒ–æ•¸æ“š
            outputs['standardized_with_symbol_and_timeframe'] = await self.generate_standardized_with_symbol_timeframe(symbol, timeframe, data)
            
            return outputs
        except Exception as e:
            self.logger.error(f"âŒ æŒ‡æ¨™è¼¸å‡ºç”Ÿæˆå¤±æ•—: {e}")
            return {}

# å…¨å±€å¯¦ä¾‹
indicator_dependency_graph = IndicatorDependencyGraph()

# ä¾¿æ·å‡½æ•¸
async def calculate_technical_indicators(symbol: str = "BTCUSDT", 
                                       timeframe: str = "1m") -> Dict[str, IndicatorResult]:
    """ä¾¿æ·å‡½æ•¸ï¼šè¨ˆç®—æŠ€è¡“æŒ‡æ¨™"""
    return await indicator_dependency_graph.calculate_all_indicators(symbol, timeframe)

async def get_indicator_performance() -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•¸ï¼šç²å–æ€§èƒ½çµ±è¨ˆ"""
    return await indicator_dependency_graph.get_performance_stats()
