#!/usr/bin/env python3
"""
ğŸ¯ Trading X - ç¬¬ä¸€éšæ®µå›æ¸¬æ•´åˆæ¸¬è©¦
åŸ·è¡Œå®Œæ•´çš„åŸºç¤å›æ¸¬æ•´åˆåŠŸèƒ½æ¸¬è©¦
æ¸¬è©¦å®Œæˆå¾Œè‡ªå‹•æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
"""

import asyncio
import logging
import sys
import json
from pathlib import Path
from datetime import datetime

# è¨­ç½®é …ç›®è·¯å¾‘
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# å°å…¥æ¸¬è©¦æ¨¡çµ„
from historical_data_extension import test_historical_data_extension
from multiframe_backtest_engine import test_multiframe_backtest
from phase5_integrated_validator import test_phase5_integration

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(project_root / 'phase1_backtest_integration_test.log')
    ]
)

logger = logging.getLogger(__name__)

class Phase1BacktestIntegrationTester:
    """ç¬¬ä¸€éšæ®µå›æ¸¬æ•´åˆæ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.temp_files = []
        self.start_time = datetime.now()
    
    async def run_complete_test_suite(self):
        """é‹è¡Œå®Œæ•´çš„æ¸¬è©¦å¥—ä»¶"""
        logger.info("ğŸš€ é–‹å§‹ç¬¬ä¸€éšæ®µå›æ¸¬æ•´åˆæ¸¬è©¦")
        logger.info("=" * 80)
        
        try:
            # æ¸¬è©¦1: æ­·å²æ•¸æ“šæ“´å±•åŠŸèƒ½
            logger.info("ğŸ“Š æ¸¬è©¦1: æ­·å²æ•¸æ“šæ“´å±•åŠŸèƒ½")
            await test_historical_data_extension()
            self.test_results['historical_data_extension'] = {
                'status': 'success',
                'message': 'æ­·å²æ•¸æ“šæ“´å±•åŠŸèƒ½æ¸¬è©¦é€šé'
            }
            logger.info("âœ… æ¸¬è©¦1å®Œæˆ\n")
            
            # æ¸¬è©¦2: å¤šæ™‚é–“æ¡†æ¶å›æ¸¬å¼•æ“
            logger.info("ğŸ“ˆ æ¸¬è©¦2: å¤šæ™‚é–“æ¡†æ¶å›æ¸¬å¼•æ“")
            backtest_results = await test_multiframe_backtest()
            self.test_results['multiframe_backtest'] = {
                'status': 'success',
                'message': 'å¤šæ™‚é–“æ¡†æ¶å›æ¸¬å¼•æ“æ¸¬è©¦é€šé',
                'data': backtest_results
            }
            
            # è¨˜éŒ„è‡¨æ™‚æª”æ¡ˆ
            temp_file = project_root / "backtest_results_temp.json"
            if temp_file.exists():
                self.temp_files.append(temp_file)
            
            logger.info("âœ… æ¸¬è©¦2å®Œæˆ\n")
            
            # æ¸¬è©¦3: Phase5æ•´åˆé©—è­‰
            logger.info("ğŸ” æ¸¬è©¦3: Phase5æ•´åˆé©—è­‰")
            phase5_results = await test_phase5_integration()
            self.test_results['phase5_integration'] = {
                'status': 'success',
                'message': 'Phase5æ•´åˆé©—è­‰æ¸¬è©¦é€šé',
                'data': phase5_results
            }
            
            # è¨˜éŒ„è‡¨æ™‚æª”æ¡ˆ
            temp_file = project_root / "phase5_integration_results_temp.json"
            if temp_file.exists():
                self.temp_files.append(temp_file)
            
            logger.info("âœ… æ¸¬è©¦3å®Œæˆ\n")
            
            # ç”Ÿæˆæœ€çµ‚å ±å‘Š
            await self._generate_final_report()
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            await self._cleanup_temp_files()
            
            logger.info("ğŸ‰ ç¬¬ä¸€éšæ®µå›æ¸¬æ•´åˆæ¸¬è©¦å…¨éƒ¨å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            self.test_results['error'] = str(e)
            
            # å³ä½¿ç™¼ç”ŸéŒ¯èª¤ä¹Ÿè¦æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            await self._cleanup_temp_files()
            raise
    
    async def _generate_final_report(self):
        """ç”Ÿæˆæœ€çµ‚æ¸¬è©¦å ±å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæœ€çµ‚æ¸¬è©¦å ±å‘Š")
        
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        report = {
            "test_suite": "Phase1 å›æ¸¬æ•´åˆæ¸¬è©¦",
            "version": "1.0.0",
            "test_time": {
                "start": self.start_time.isoformat(),
                "end": end_time.isoformat(),
                "duration_seconds": duration.total_seconds()
            },
            "test_results": self.test_results,
            "summary": self._generate_test_summary(),
            "next_steps": self._generate_next_steps()
        }
        
        # ä¿å­˜æœ€çµ‚å ±å‘Š
        report_file = project_root / f"Phase1_Backtest_Integration_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“ æœ€çµ‚å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # è¼¸å‡ºæ‘˜è¦åˆ°æ§åˆ¶å°
        self._print_test_summary(report)
    
    def _generate_test_summary(self):
        """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦"""
        successful_tests = sum(1 for result in self.test_results.values() 
                             if isinstance(result, dict) and result.get('status') == 'success')
        total_tests = len([k for k in self.test_results.keys() if k != 'error'])
        
        summary = {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": total_tests - successful_tests,
            "success_rate": successful_tests / total_tests if total_tests > 0 else 0,
            "overall_status": "PASS" if successful_tests == total_tests else "FAIL"
        }
        
        # æå–é—œéµæŒ‡æ¨™
        if 'multiframe_backtest' in self.test_results and self.test_results['multiframe_backtest'].get('data'):
            backtest_data = self.test_results['multiframe_backtest']['data']
            if 'backtest_summary' in backtest_data:
                backtest_summary = backtest_data['backtest_summary']
                summary['backtest_metrics'] = {
                    "total_backtests": backtest_summary.get('total_backtests', 0),
                    "successful_backtests": backtest_summary.get('successful_backtests', 0),
                    "overall_performance": backtest_summary.get('overall_performance', {})
                }
        
        if 'phase5_integration' in self.test_results and self.test_results['phase5_integration'].get('data'):
            phase5_data = self.test_results['phase5_integration']['data']
            if 'results' in phase5_data and 'phase5_validation' in phase5_data['results']:
                phase5_validation = phase5_data['results']['phase5_validation']
                excellent_count = sum(1 for p in phase5_validation.get('performance_classification', {}).values() 
                                    if p.get('classification') == 'excellent')
                total_count = len(phase5_validation.get('performance_classification', {}))
                summary['phase5_metrics'] = {
                    "total_validations": total_count,
                    "excellent_performances": excellent_count,
                    "excellence_rate": excellent_count / total_count if total_count > 0 else 0
                }
        
        return summary
    
    def _generate_next_steps(self):
        """ç”Ÿæˆä¸‹ä¸€æ­¥å»ºè­°"""
        next_steps = []
        
        # åŸºæ–¼æ¸¬è©¦çµæœç”Ÿæˆå»ºè­°
        if self.test_results.get('historical_data_extension', {}).get('status') == 'success':
            next_steps.append("âœ… æ­·å²æ•¸æ“šæ“´å±•åŠŸèƒ½å·²å°±ç·’ï¼Œå¯é€²å…¥ç¬¬äºŒéšæ®µé–‹ç™¼")
        
        if self.test_results.get('multiframe_backtest', {}).get('status') == 'success':
            next_steps.append("âœ… å¤šæ™‚é–“æ¡†æ¶å›æ¸¬å¼•æ“é‹è¡Œæ­£å¸¸ï¼Œå¯é–‹å§‹æ™ºèƒ½åƒæ•¸èª¿æ•´é–‹ç™¼")
        
        if self.test_results.get('phase5_integration', {}).get('status') == 'success':
            next_steps.append("âœ… Phase5æ•´åˆæˆåŠŸï¼Œå¯é–‹å§‹TradingViewé¢¨æ ¼å ±å‘Šé–‹ç™¼")
        
        # ç¬¬äºŒéšæ®µå»ºè­°
        next_steps.extend([
            "ğŸ”§ ç¬¬äºŒéšæ®µ: å¯¦ç¾æ™ºèƒ½åƒæ•¸èª¿æ•´",
            "ğŸ“Š ç¬¬äºŒéšæ®µ: å¯¦ç¾TradingViewé¢¨æ ¼å ±å‘Šç”Ÿæˆ",
            "ğŸ¯ ç¬¬äºŒéšæ®µ: å¯¦ç¾æœˆåº¦è‡ªå‹•åƒæ•¸å„ªåŒ–",
            "ğŸ“ˆ ç¬¬äºŒéšæ®µ: å¯¦ç¾å¸‚å ´åˆ¶åº¦è‡ªé©æ‡‰èª¿æ•´"
        ])
        
        return next_steps
    
    def _print_test_summary(self, report):
        """è¼¸å‡ºæ¸¬è©¦æ‘˜è¦åˆ°æ§åˆ¶å°"""
        summary = report['summary']
        
        logger.info("=" * 80)
        logger.info("ğŸ“‹ ç¬¬ä¸€éšæ®µå›æ¸¬æ•´åˆæ¸¬è©¦ç¸½çµ")
        logger.info("=" * 80)
        logger.info(f"ğŸ• æ¸¬è©¦æ™‚é–“: {report['test_time']['duration_seconds']:.1f} ç§’")
        logger.info(f"ğŸ“Š æ¸¬è©¦çµæœ: {summary['successful_tests']}/{summary['total_tests']} é€šé")
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {summary['success_rate']:.1%}")
        logger.info(f"ğŸ† æ•´é«”ç‹€æ…‹: {summary['overall_status']}")
        
        if 'backtest_metrics' in summary:
            metrics = summary['backtest_metrics']
            logger.info(f"ğŸ“ˆ å›æ¸¬æˆåŠŸ: {metrics['successful_backtests']}/{metrics['total_backtests']}")
            
            if 'overall_performance' in metrics and metrics['overall_performance']:
                perf = metrics['overall_performance']
                logger.info(f"ğŸ“Š å¹³å‡å‹ç‡: {perf.get('avg_win_rate', 0):.1%}")
                logger.info(f"ğŸ’° å¹³å‡æ”¶ç›Š: {perf.get('avg_return', 0):.3%}")
        
        if 'phase5_metrics' in summary:
            metrics = summary['phase5_metrics']
            logger.info(f"ğŸ† å„ªç§€è¡¨ç¾: {metrics['excellent_performances']}/{metrics['total_validations']}")
            logger.info(f"âœ¨ å„ªç§€ç‡: {metrics['excellence_rate']:.1%}")
        
        logger.info("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè­°:")
        for i, step in enumerate(report['next_steps'][:5], 1):
            logger.info(f"   {i}. {step}")
        
        logger.info("=" * 80)
    
    async def _cleanup_temp_files(self):
        """æ¸…ç†è‡¨æ™‚æª”æ¡ˆ"""
        logger.info("ğŸ§¹ æ¸…ç†è‡¨æ™‚æª”æ¡ˆ")
        
        cleaned_count = 0
        for temp_file in self.temp_files:
            try:
                if temp_file.exists():
                    temp_file.unlink()
                    cleaned_count += 1
                    logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤: {temp_file.name}")
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤ {temp_file.name}: {e}")
        
        # æ¸…ç†æ—¥èªŒæª”æ¡ˆ (å¯é¸)
        log_file = project_root / 'phase1_backtest_integration_test.log'
        try:
            if log_file.exists():
                log_file.unlink()
                cleaned_count += 1
                logger.info(f"ğŸ—‘ï¸ å·²åˆªé™¤: {log_file.name}")
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤æ—¥èªŒæª”æ¡ˆ: {e}")
        
        logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œå…±åˆªé™¤ {cleaned_count} å€‹è‡¨æ™‚æª”æ¡ˆ")


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    tester = Phase1BacktestIntegrationTester()
    await tester.run_complete_test_suite()


if __name__ == "__main__":
    asyncio.run(main())
