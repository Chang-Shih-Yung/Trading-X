#!/usr/bin/env python3
"""
ğŸ¯ Trading X - æ­·å²æ•¸æ“šæ“´å±•æ¨¡çµ„
æ“´å±•Phase1Aæ­·å²æ•¸æ“šç²å–èƒ½åŠ›ï¼Œæ”¯æ´å¤šæ™‚é–“æ¡†æ¶å›æ¸¬
ä¿æŒåŸæœ‰JSON Schemaä¸è®Šï¼Œç´”æ•¸æ“šæ“´å±•
"""

import asyncio
import aiohttp
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import json
from pathlib import Path
import pandas as pd

logger = logging.getLogger(__name__)

class HistoricalDataExtension:
    """æ­·å²æ•¸æ“šæ“´å±•å™¨ - æ”¯æ´å›æ¸¬ç”¨æ­·å²æ•¸æ“šç²å–"""
    
    def __init__(self):
        self.base_url = "https://api.binance.com/api/v3/klines"
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _interval_to_minutes(self, interval: str) -> int:
        """æ™‚é–“é–“éš”è½‰æ›ç‚ºåˆ†é˜æ•¸"""
        interval_map = {
            "1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30,
            "1h": 60, "2h": 120, "4h": 240, "6h": 360, "8h": 480, "12h": 720,
            "1d": 1440, "3d": 4320, "1w": 10080, "1M": 43200
        }
        return interval_map.get(interval, 1)
    
    async def fetch_extended_historical_data(self, 
                                           symbol: str, 
                                           interval: str = "1m", 
                                           days_back: int = 30) -> List[Dict[str, Any]]:
        """
        æ“´å±•æ­·å²æ•¸æ“šç²å– - æ”¯æ´å¤šå¤©å›æ¸¬æ•¸æ“š
        
        Args:
            symbol: äº¤æ˜“å°
            interval: æ™‚é–“é–“éš”
            days_back: å›æº¯å¤©æ•¸
            
        Returns:
            æ­·å²Kç·šæ•¸æ“šåˆ—è¡¨
        """
        try:
            if not self.session:
                raise RuntimeError("Session not initialized. Use async context manager.")
            
            # è¨ˆç®—éœ€è¦çš„Kç·šæ•¸é‡
            minutes_per_day = 1440
            interval_minutes = self._interval_to_minutes(interval)
            klines_per_day = minutes_per_day // interval_minutes
            total_limit = min(1000, days_back * klines_per_day)  # Binanceé™åˆ¶1000æ ¹
            
            logger.info(f"ğŸ”„ ç²å– {symbol} {interval} æ­·å²æ•¸æ“š: {days_back}å¤© ({total_limit}æ ¹Kç·š)")
            
            params = {
                'symbol': symbol,
                'interval': interval,
                'limit': total_limit
            }
            
            async with self.session.get(self.base_url, params=params) as response:
                if response.status == 200:
                    raw_data = await response.json()
                    
                    # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼ (ä¿æŒèˆ‡åŸæœ‰æ ¼å¼ä¸€è‡´)
                    formatted_data = []
                    for kline in raw_data:
                        formatted_kline = {
                            'open_time': int(kline[0]),
                            'open': float(kline[1]),
                            'high': float(kline[2]),
                            'low': float(kline[3]),
                            'close': float(kline[4]),
                            'volume': float(kline[5]),
                            'close_time': int(kline[6]),
                            'quote_asset_volume': float(kline[7]),
                            'number_of_trades': int(kline[8]),
                            'taker_buy_base_volume': float(kline[9]),
                            'taker_buy_quote_volume': float(kline[10]),
                            'symbol': symbol,
                            'interval': interval,
                            'timestamp': datetime.fromtimestamp(kline[0] / 1000)
                        }
                        formatted_data.append(formatted_kline)
                    
                    logger.info(f"âœ… æˆåŠŸç²å– {len(formatted_data)} æ ¹ {symbol} Kç·šæ•¸æ“š")
                    return formatted_data
                    
                else:
                    error_text = await response.text()
                    logger.error(f"âŒ Binance APIéŒ¯èª¤: {response.status} - {error_text}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ ç²å–æ­·å²æ•¸æ“šå¤±æ•—: {e}")
            return []
    
    async def fetch_multiple_symbols_data(self, 
                                        symbols: List[str], 
                                        interval: str = "1m", 
                                        days_back: int = 30) -> Dict[str, List[Dict[str, Any]]]:
        """
        æ‰¹é‡ç²å–å¤šå€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š
        
        Args:
            symbols: äº¤æ˜“å°åˆ—è¡¨
            interval: æ™‚é–“é–“éš”
            days_back: å›æº¯å¤©æ•¸
            
        Returns:
            {symbol: æ­·å²æ•¸æ“š} çš„å­—å…¸
        """
        logger.info(f"ğŸ”„ æ‰¹é‡ç²å– {len(symbols)} å€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š")
        
        results = {}
        
        # ä¸¦è¡Œç²å–æ•¸æ“š (é™åˆ¶ä¸¦ç™¼æ•¸é¿å…APIé™åˆ¶)
        semaphore = asyncio.Semaphore(5)  # é™åˆ¶ä¸¦ç™¼æ•¸ç‚º5
        
        async def fetch_single_symbol(symbol):
            async with semaphore:
                data = await self.fetch_extended_historical_data(symbol, interval, days_back)
                return symbol, data
        
        # å‰µå»ºä»»å‹™
        tasks = [fetch_single_symbol(symbol) for symbol in symbols]
        
        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è™•ç†çµæœ
        for result in completed_tasks:
            if isinstance(result, Exception):
                logger.error(f"âŒ ç²å–æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {result}")
                continue
                
            symbol, data = result
            results[symbol] = data
        
        logger.info(f"âœ… æˆåŠŸç²å– {len(results)} å€‹äº¤æ˜“å°çš„æ­·å²æ•¸æ“š")
        return results
    
    def convert_to_dataframe(self, kline_data: List[Dict[str, Any]]) -> pd.DataFrame:
        """
        è½‰æ›Kç·šæ•¸æ“šç‚ºDataFrameæ ¼å¼ (ç”¨æ–¼æŠ€è¡“åˆ†æ)
        
        Args:
            kline_data: Kç·šæ•¸æ“šåˆ—è¡¨
            
        Returns:
            pandas DataFrame
        """
        if not kline_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(kline_data)
        
        # è¨­ç½®æ™‚é–“ç´¢å¼•
        df['datetime'] = pd.to_datetime(df['open_time'], unit='ms')
        df.set_index('datetime', inplace=True)
        
        # ç¢ºä¿æ•¸å€¼åˆ—ç‚ºfloaté¡å‹
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                          'quote_asset_volume', 'taker_buy_base_volume', 'taker_buy_quote_volume']
        
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # æŒ‰æ™‚é–“æ’åº
        df.sort_index(inplace=True)
        
        logger.info(f"âœ… è½‰æ›ç‚ºDataFrame: {len(df)} è¡Œæ•¸æ“š")
        return df
    
    async def validate_data_quality(self, kline_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        é©—è­‰æ­·å²æ•¸æ“šå“è³ª
        
        Args:
            kline_data: Kç·šæ•¸æ“šåˆ—è¡¨
            
        Returns:
            æ•¸æ“šå“è³ªå ±å‘Š
        """
        if not kline_data:
            return {"status": "empty", "message": "ç„¡æ•¸æ“š"}
        
        df = self.convert_to_dataframe(kline_data)
        
        # æ•¸æ“šå“è³ªæª¢æŸ¥
        quality_report = {
            "total_records": len(df),
            "time_range": {
                "start": df.index.min().isoformat() if len(df) > 0 else None,
                "end": df.index.max().isoformat() if len(df) > 0 else None
            },
            "missing_values": {
                "open": int(df['open'].isna().sum()),
                "high": int(df['high'].isna().sum()), 
                "low": int(df['low'].isna().sum()),
                "close": int(df['close'].isna().sum()),
                "volume": int(df['volume'].isna().sum())
            },
            "data_gaps": [],
            "anomalies": {
                "zero_volume_count": int((df['volume'] == 0).sum()),
                "extreme_price_changes": []
            }
        }
        
        # æª¢æŸ¥æ™‚é–“é–“éš”æ˜¯å¦é€£çºŒ
        if len(df) > 1:
            time_diffs = df.index.to_series().diff()
            expected_interval = time_diffs.mode()[0] if len(time_diffs.mode()) > 0 else None
            
            if expected_interval:
                gaps = time_diffs[time_diffs > expected_interval * 1.5]
                quality_report["data_gaps"] = [
                    {
                        "timestamp": gap_time.isoformat(),
                        "duration_minutes": float(gap_duration.total_seconds() / 60)
                    }
                    for gap_time, gap_duration in gaps.items()
                ]
        
        # æª¢æŸ¥æ¥µç«¯åƒ¹æ ¼è®ŠåŒ–
        if len(df) > 1:
            price_changes = df['close'].pct_change()
            extreme_changes = price_changes[abs(price_changes) > 0.1]  # 10%ä»¥ä¸Šè®ŠåŒ–
            
            quality_report["anomalies"]["extreme_price_changes"] = [
                {
                    "timestamp": change_time.isoformat(),
                    "change_percent": float(change_value * 100)
                }
                for change_time, change_value in extreme_changes.items()
            ]
        
        # æ•´é«”å“è³ªè©•åˆ†
        issues_count = (
            sum(quality_report["missing_values"].values()) +
            len(quality_report["data_gaps"]) +
            quality_report["anomalies"]["zero_volume_count"] +
            len(quality_report["anomalies"]["extreme_price_changes"])
        )
        
        if issues_count == 0:
            quality_report["quality_score"] = "excellent"
        elif issues_count < 5:
            quality_report["quality_score"] = "good"
        elif issues_count < 20:
            quality_report["quality_score"] = "fair"
        else:
            quality_report["quality_score"] = "poor"
        
        logger.info(f"âœ… æ•¸æ“šå“è³ªè©•åˆ†: {quality_report['quality_score']}")
        return quality_report


# æ¸¬è©¦å‡½æ•¸
async def test_historical_data_extension():
    """æ¸¬è©¦æ­·å²æ•¸æ“šæ“´å±•åŠŸèƒ½"""
    logger.info("ğŸ§ª é–‹å§‹æ¸¬è©¦æ­·å²æ•¸æ“šæ“´å±•åŠŸèƒ½")
    
    async with HistoricalDataExtension() as data_ext:
        # æ¸¬è©¦å–®ä¸€äº¤æ˜“å°
        test_symbol = "BTCUSDT"
        logger.info(f"ğŸ“Š æ¸¬è©¦ç²å– {test_symbol} 7å¤©1åˆ†é˜æ•¸æ“š")
        
        data = await data_ext.fetch_extended_historical_data(
            symbol=test_symbol,
            interval="1m", 
            days_back=7
        )
        
        if data:
            logger.info(f"âœ… ç²å–åˆ° {len(data)} æ ¹Kç·š")
            
            # é©—è­‰æ•¸æ“šå“è³ª
            quality_report = await data_ext.validate_data_quality(data)
            # è½‰æ›numpyé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
            quality_report_json = json.loads(json.dumps(quality_report, default=str))
            logger.info(f"ğŸ“‹ æ•¸æ“šå“è³ªå ±å‘Š: {json.dumps(quality_report_json, indent=2, ensure_ascii=False)}")
            
            # æ¸¬è©¦DataFrameè½‰æ›
            df = data_ext.convert_to_dataframe(data)
            logger.info(f"ğŸ“ˆ DataFrameå½¢ç‹€: {df.shape}")
            logger.info(f"ğŸ“ˆ æœ€æ–°åƒ¹æ ¼: {df['close'].iloc[-1]:.2f}")
            
        else:
            logger.error("âŒ æœªç²å–åˆ°æ•¸æ“š")
    
    logger.info("ğŸ‰ æ­·å²æ•¸æ“šæ“´å±•åŠŸèƒ½æ¸¬è©¦å®Œæˆ")


if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(test_historical_data_extension())
