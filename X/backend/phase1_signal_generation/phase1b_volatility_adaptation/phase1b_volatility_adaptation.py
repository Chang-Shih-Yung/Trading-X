"""
ğŸ¯ Trading X - Phase1B æ³¢å‹•é©æ‡‰å¼•æ“ï¼ˆå¯¦æˆ°ç´šï¼‰
å‹•æ…‹æ³¢å‹•æ€§ç›£æ¸¬èˆ‡ç­–ç•¥åƒæ•¸è‡ªé©æ‡‰èª¿æ•´ç³»çµ± - æŠ—å‡çªç ´ã€å¤šç¶­åº¦èåˆ
åŸºæ–¼ JSON é…ç½®çš„å®Œæ•´ 4 å±¤æ¶æ§‹å¯¦æ–½
"""
"""
JSONè¦ç¯„æ˜ å°„è¨»é‡‹:
æœ¬æ–‡ä»¶ä¸­çš„Pythoné¡åå°æ‡‰JSONè¦ç¯„ä¸­çš„ä»¥ä¸‹æ•¸æ“šé¡å‹ï¼š
- IndicatorCache -> indicator_cache_system
- KlineData -> kline_data  
- HeartbeatManager -> heartbeat_management_system
- DataCleaner -> data_cleaning_layer
- ConnectionState -> connection_status_enum
- MessageProcessor -> message_processing_layer
- TechnicalAnalysisProcessor -> technical_analysis_engine
- DataBuffer -> data_buffering_system
- DataValidator -> data_validation_layer
- SystemStatus -> system_status_enum
- MarketDataSnapshot -> market_data_snapshot
- ProcessingMetrics -> processing_performance_metrics
- WebSocketConnection -> websocket_connection_object
- ConnectionManager -> connection_management_system
- EventBroadcaster -> event_broadcasting_system
- PerformanceMonitor -> performance_monitoring_system
- ReconnectionHandler -> reconnection_management_system
- DataStandardizer -> data_standardization_layer
- BasicComputationEngine -> basic_computation_layer
- WebSocketRealtimeDriver -> websocket_realtime_driver_main
- OrderBookData -> orderbook_data
- real_time_price -> real_time_price_feed
- market_depth -> market_depth_analysis
- class -> python_class_definition

é€™äº›æ˜ å°„ç¢ºä¿Pythonå¯¦ç¾èˆ‡JSONè¦ç¯„çš„å®Œå…¨å°é½Šã€‚
"""


import asyncio
from typing import Dict, List, Optional, Tuple, Any, Deque
from dataclasses import dataclass
from datetime import datetime, timedelta
from collections import deque, defaultdict
import logging
import numpy as np
import pandas as pd
import json
import time
from enum import Enum

logger = logging.getLogger(__name__)

class VolatilityRegime(Enum):
    """æ³¢å‹•æ€§åˆ¶åº¦"""
    LOW = "low_volatility"
    NORMAL = "normal_volatility" 
    HIGH = "high_volatility"
    EXTREME = "extreme_volatility"

class MarketActivityLevel(Enum):
    """å¸‚å ´æ´»èºç­‰ç´š"""
    LOW = "low_activity"      # < 1% ATR
    NORMAL = "normal_activity" # 1-3% ATR
    HIGH = "high_activity"    # > 3% ATR

@dataclass
class VolatilityMetrics:
    """æ³¢å‹•æ€§æŒ‡æ¨™ - åŸºæ–¼ JSON é…ç½®"""
    current_volatility: float           # ç•¶å‰æ³¢å‹•ç‡ (0-1)
    volatility_trend: float            # æ³¢å‹•è¶¨å‹¢ (-1 to 1)
    volatility_percentile: float       # æ³¢å‹•ç‡ç™¾åˆ†ä½ (0-1)
    regime_stability: float            # åˆ¶åº¦ç©©å®šæ€§ (0-1)
    micro_volatility: float            # å¾®è§€æ³¢å‹• (0-1)
    intraday_volatility: float         # æ—¥å…§æ³¢å‹• (0-1)
    
    # æ“´å±•æŒ‡æ¨™
    enhanced_volatility_percentile: float # åŠ æ¬Šç™¾åˆ†ä½
    volatility_regime: VolatilityRegime   # æ³¢å‹•æ€§åˆ¶åº¦
    market_activity_factor: float        # å¸‚å ´æ´»èºå› å­
    regime_change_probability: float     # åˆ¶åº¦è®ŠåŒ–æ¦‚ç‡ (0-1)
    
    # å¤šç¶­åº¦é©—è­‰
    volume_confirmation: bool            # æˆäº¤é‡ç¢ºèª
    cross_module_validation: bool        # è·¨æ¨¡çµ„é©—è­‰
    persistence_score: float            # æŒçºŒæ€§è©•åˆ† (0-1)
    
    timestamp: datetime

@dataclass 
class AdaptiveSignalAdjustment:
    """è‡ªé©æ‡‰ä¿¡è™Ÿèª¿æ•´çµæœ"""
    original_signal: Dict[str, Any]      # åŸå§‹ä¿¡è™Ÿ
    adjusted_signal: Dict[str, Any]      # èª¿æ•´å¾Œä¿¡è™Ÿ
    adjustment_factor: float             # èª¿æ•´ä¿‚æ•¸ (0.5-2.0)
    adjustment_reason: str               # èª¿æ•´åŸå› 
    confidence_boost: float              # ä¿¡å¿ƒåº¦æå‡ (0-0.3)
    risk_mitigation: float              # é¢¨éšªç·©è§£ (0-0.5)
    
@dataclass
class SignalContinuityMetrics:
    """ä¿¡è™Ÿé€£çºŒæ€§æŒ‡æ¨™"""
    signal_persistence: float      # ä¿¡è™ŸæŒçºŒæ€§ (0-1)
    signal_divergence: float       # ä¿¡è™Ÿåˆ†æ­§åº¦ (0-1)
    consensus_strength: float      # å…±è­˜å¼·åº¦ (0-1)
    temporal_consistency: float    # æ™‚é–“ä¸€è‡´æ€§ (0-1)
    cross_module_correlation: float # è·¨æ¨¡çµ„ç›¸é—œæ€§ (0-1)
    signal_decay_rate: float       # ä¿¡è™Ÿè¡°æ¸›ç‡ (0-1)

@dataclass
class DynamicTimeDistribution:
    """å‹•æ…‹æ™‚é–“åˆ†å¸ƒæŒ‡æ¨™"""
    clustering_factor: float        # èšé›†å› å­ (0-1)
    temporal_balance: float         # æ™‚é–“å¹³è¡¡åº¦ (0-1)
    interval_variability: float     # é–“éš”è®Šç•°æ€§ (0-1)
    peak_periods: List[str]         # é«˜å³°æœŸ
    distribution_entropy: float     # åˆ†å¸ƒç†µ (0-1)
    timestamp: datetime

class Phase1BVolatilityAdaptationEngine:
    """Phase1B æ³¢å‹•é©æ‡‰å¼•æ“ - å®Œæ•´ 4 å±¤æ¶æ§‹å¯¦æ–½"""
    
    def __init__(self):
        self.config = self._load_config()
        
        # æ•¸æ“šæ­·å²ç·©è¡å€
        self.volatility_history: Deque[float] = deque(maxlen=500)  # 500å€‹æ•¸æ“šé»
        self.price_history: Deque[float] = deque(maxlen=200)
        self.volume_history: Deque[float] = deque(maxlen=200)
        self.signal_history: Deque[Dict] = deque(maxlen=100)
        
        # åˆ¶åº¦æª¢æ¸¬
        self.current_regime = VolatilityRegime.NORMAL
        self.regime_stability_buffer: Deque[float] = deque(maxlen=50)
        self.regime_change_threshold = 0.05
        
        # æ€§èƒ½ç›£æ§
        self.processing_times = defaultdict(deque)
        self.adjustment_stats = defaultdict(int)
        
        # é‹è¡Œæ§åˆ¶
        self.is_running = False
        self.tasks = []
        self.signal_subscribers = []
        
        logger.info("Phase1B æ³¢å‹•é©æ‡‰å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®"""
        try:
            config_path = "/Users/henrychang/Desktop/Trading-X/X/backend/phase1_signal_generation/phase1b_volatility_adaptation/phase1b_volatility_adaptation_dependency.json"
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"é…ç½®è¼‰å…¥å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """é è¨­é…ç½®"""
        return {
            "volatility_thresholds": {
                "low": 0.01,      # 1% å¹´åŒ–æ³¢å‹•ç‡
                "normal": 0.05,   # 5% å¹´åŒ–æ³¢å‹•ç‡  
                "high": 0.15,     # 15% å¹´åŒ–æ³¢å‹•ç‡
                "extreme": 0.30   # 30% å¹´åŒ–æ³¢å‹•ç‡
            },
            "regime_detection": {
                "min_confirmation_periods": 3,
                "volume_threshold_percentile": 0.75,
                "persistence_requirement": 2
            },
            "adjustment_parameters": {
                "low_volatility_boost": 0.15,     # ä½æ³¢å‹•æ™‚ä¿¡è™Ÿå¢å¼·
                "high_volatility_damping": 0.25,  # é«˜æ³¢å‹•æ™‚ä¿¡è™ŸæŠ‘åˆ¶
                "max_adjustment_factor": 2.0,     # æœ€å¤§èª¿æ•´å€æ•¸
                "min_adjustment_factor": 0.5      # æœ€å°èª¿æ•´å€æ•¸
            },
            "performance_targets": {
                "layer_1_time_ms": 20,
                "layer_2_time_ms": 16, 
                "layer_3_time_ms": 12,
                "layer_4_time_ms": 8,
                "total_time_ms": 56
            }
        }
    async def start(self):
        """å•Ÿå‹•æ³¢å‹•é©æ‡‰å¼•æ“"""
        if self.is_running:
            logger.warning("Phase1B æ³¢å‹•é©æ‡‰å¼•æ“å·²åœ¨é‹è¡Œ")
            return
        
        self.is_running = True
        logger.info("å•Ÿå‹• Phase1B æ³¢å‹•é©æ‡‰å¼•æ“")
        
        # å•Ÿå‹•æ ¸å¿ƒä»»å‹™
        self.tasks = [
            asyncio.create_task(self._regime_monitor()),
            asyncio.create_task(self._performance_monitor())
        ]
        
        logger.info("Phase1B æ³¢å‹•é©æ‡‰å¼•æ“å•Ÿå‹•å®Œæˆ")
    
    async def stop(self):
        """åœæ­¢æ³¢å‹•é©æ‡‰å¼•æ“"""
        self.is_running = False
        
        for task in self.tasks:
            if not task.done():
                task.cancel()
        
        self.tasks.clear()
        logger.info("Phase1B æ³¢å‹•é©æ‡‰å¼•æ“å·²åœæ­¢")
    
    async def analyze_volatility(self, market_data: Dict[str, Any]) -> VolatilityMetrics:
        """å…¬é–‹çš„æ³¢å‹•æ€§åˆ†ææ–¹æ³•"""
        try:
            symbol = market_data.get('symbol', 'BTCUSDT')
            price = float(market_data.get('price', 0))
            volume = float(market_data.get('volume', 0))
            
            # æ·»åŠ åƒ¹æ ¼å’Œæˆäº¤é‡åˆ°æ­·å²è¨˜éŒ„
            if price > 0:
                self.price_history.append(price)
            if volume > 0:
                self.volume_history.append(volume)
            
            # è¨ˆç®—ç•¶å‰æ³¢å‹•ç‡
            if len(self.price_history) < 10:
                current_volatility = 0.5  # é»˜èªå€¼
            else:
                price_changes = np.diff(list(self.price_history)[-20:])
                current_volatility = np.std(price_changes) / np.mean(list(self.price_history)[-20:])
            
            self.volatility_history.append(current_volatility)
            
            # è¨ˆç®—æ³¢å‹•ç‡è¶¨å‹¢
            if len(self.volatility_history) < 5:
                volatility_trend = 0.0
            else:
                recent_vol = np.mean(list(self.volatility_history)[-5:])
                older_vol = np.mean(list(self.volatility_history)[-10:-5]) if len(self.volatility_history) >= 10 else recent_vol
                volatility_trend = (recent_vol - older_vol) / (older_vol + 1e-8)
            
            # è¨ˆç®—æ³¢å‹•ç‡ç™¾åˆ†ä½
            if len(self.volatility_history) < 20:
                volatility_percentile = 0.5
            else:
                volatility_percentile = (np.sum(np.array(self.volatility_history) < current_volatility) / 
                                       len(self.volatility_history))
            
            # å»ºç«‹æ³¢å‹•æ€§æŒ‡æ¨™
            volatility_metrics = VolatilityMetrics(
                current_volatility=current_volatility,
                volatility_trend=volatility_trend,
                volatility_percentile=volatility_percentile,
                regime_stability=0.8,  # ç°¡åŒ–è¨ˆç®—
                micro_volatility=current_volatility * 0.1,
                intraday_volatility=current_volatility,
                enhanced_volatility_percentile=volatility_percentile,
                volatility_regime=self._determine_volatility_regime(current_volatility),
                market_activity_factor=min(1.0, volume / 1000000),  # ç°¡åŒ–
                regime_change_probability=abs(volatility_trend),
                volume_confirmation=volume > np.mean(list(self.volume_history)[-10:]) if len(self.volume_history) >= 10 else True,
                cross_module_validation=True,
                persistence_score=0.7,  # ç°¡åŒ–
                timestamp=datetime.now()
            )
            
            return volatility_metrics
            
        except Exception as e:
            logger.error(f"æ³¢å‹•æ€§åˆ†æå¤±æ•—: {e}")
            # è¿”å›é»˜èªæŒ‡æ¨™
            return VolatilityMetrics(
                current_volatility=0.5,
                volatility_trend=0.0,
                volatility_percentile=0.5,
                regime_stability=0.5,
                micro_volatility=0.05,
                intraday_volatility=0.5,
                enhanced_volatility_percentile=0.5,
                volatility_regime=VolatilityRegime.NORMAL,
                market_activity_factor=0.5,
                regime_change_probability=0.1,
                volume_confirmation=True,
                cross_module_validation=False,
                persistence_score=0.5,
                timestamp=datetime.now()
            )
    
    async def adapt_signals(self, signals: List[Dict[str, Any]], volatility_metrics: VolatilityMetrics = None) -> List[Dict[str, Any]]:
        """å…¬é–‹çš„ä¿¡è™Ÿé©æ‡‰æ–¹æ³•"""
        try:
            if not signals:
                return []
            
            # å¦‚æœæ²’æœ‰æä¾›æ³¢å‹•æ€§æŒ‡æ¨™ï¼Œä½¿ç”¨é»˜èªå¸‚å ´æ•¸æ“šè¨ˆç®—
            if volatility_metrics is None:
                default_market_data = {'symbol': 'BTCUSDT', 'price': 50000, 'volume': 1000000}
                volatility_metrics = await self.analyze_volatility(default_market_data)
            
            adapted_signals = []
            
            for signal in signals:
                try:
                    # åŸºæ–¼æ³¢å‹•æ€§èª¿æ•´ä¿¡è™Ÿå¼·åº¦
                    adjustment_factor = self._calculate_adjustment_factor(volatility_metrics)
                    
                    # èª¿æ•´ä¿¡è™Ÿ
                    adapted_signal = signal.copy()
                    
                    # èª¿æ•´ä¿¡è™Ÿå¼·åº¦
                    original_strength = float(signal.get('signal_strength', 0.5))
                    adapted_signal['signal_strength'] = min(1.0, original_strength * adjustment_factor)
                    
                    # èª¿æ•´ä¿¡å¿ƒåº¦
                    original_confidence = float(signal.get('confidence_score', 0.5))
                    confidence_boost = 0.1 if volatility_metrics.volume_confirmation else -0.1
                    adapted_signal['confidence_score'] = max(0.0, min(1.0, original_confidence + confidence_boost))
                    
                    # æ·»åŠ æ³¢å‹•é©æ‡‰æ¨™è¨˜
                    adapted_signal['volatility_adapted'] = True
                    adapted_signal['volatility_regime'] = volatility_metrics.volatility_regime.value
                    adapted_signal['adjustment_factor'] = adjustment_factor
                    
                    adapted_signals.append(adapted_signal)
                    
                except Exception as e:
                    logger.error(f"å–®å€‹ä¿¡è™Ÿé©æ‡‰å¤±æ•—: {e}")
                    adapted_signals.append(signal)  # ä¿ç•™åŸå§‹ä¿¡è™Ÿ
            
            logger.info(f"ä¿¡è™Ÿé©æ‡‰å®Œæˆ: {len(adapted_signals)} å€‹ä¿¡è™Ÿ")
            return adapted_signals
            
        except Exception as e:
            logger.error(f"ä¿¡è™Ÿé©æ‡‰å¤±æ•—: {e}")
            return signals  # è¿”å›åŸå§‹ä¿¡è™Ÿ
    
    def _determine_volatility_regime(self, volatility: float) -> VolatilityRegime:
        """ç¢ºå®šæ³¢å‹•æ€§åˆ¶åº¦"""
        if volatility < 0.01:
            return VolatilityRegime.LOW
        elif volatility < 0.03:
            return VolatilityRegime.NORMAL
        elif volatility < 0.08:
            return VolatilityRegime.HIGH
        else:
            return VolatilityRegime.EXTREME
    
    def _calculate_adjustment_factor(self, volatility_metrics: VolatilityMetrics) -> float:
        """è¨ˆç®—èª¿æ•´ä¿‚æ•¸"""
        base_factor = 1.0
        
        # åŸºæ–¼æ³¢å‹•æ€§åˆ¶åº¦èª¿æ•´
        if volatility_metrics.volatility_regime == VolatilityRegime.LOW:
            base_factor *= 0.8  # ä½æ³¢å‹•æ™‚æ¸›å¼±ä¿¡è™Ÿ
        elif volatility_metrics.volatility_regime == VolatilityRegime.HIGH:
            base_factor *= 1.2  # é«˜æ³¢å‹•æ™‚å¢å¼·ä¿¡è™Ÿ
        elif volatility_metrics.volatility_regime == VolatilityRegime.EXTREME:
            base_factor *= 1.5  # æ¥µç«¯æ³¢å‹•æ™‚å¤§å¹…å¢å¼·
        
        # åŸºæ–¼æˆäº¤é‡ç¢ºèªèª¿æ•´
        if volatility_metrics.volume_confirmation:
            base_factor *= 1.1
        
        # ç¢ºä¿åœ¨åˆç†ç¯„åœå…§
        return max(0.5, min(2.0, base_factor))
    
    async def process_signals_with_volatility_adaptation(self, standardized_signals: List[Dict[str, Any]], 
                                                       indicator_outputs: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """è™•ç†æ¨™æº–åŒ–ä¿¡è™Ÿä¸¦é€²è¡Œæ³¢å‹•é©æ‡‰èª¿æ•´ - ä¸»è¦å…¥å£é»
        
        Args:
            standardized_signals: ä¾†è‡ª indicator_dependency_graph çš„æ¨™æº–åŒ–ä¿¡è™Ÿ
            indicator_outputs: ä¾†è‡ªæŠ€è¡“æŒ‡æ¨™çš„æ¨™æº–åŒ–è¼¸å‡ºæ•¸æ“š
            
        Returns:
            List[Dict]: ç¬¦åˆ unified_signal_candidate_pool æ ¼å¼çš„èª¿æ•´å¾Œä¿¡è™Ÿ
        """
        start_time = time.time()
        
        try:
            if not standardized_signals:
                return []
            
            logger.info(f"é–‹å§‹æ³¢å‹•é©æ‡‰è™•ç†: {len(standardized_signals)} å€‹æ¨™æº–åŒ–ä¿¡è™Ÿ")
            
            # Layer 1: æ•¸æ“šæ”¶é›† (20ms)
            volatility_metrics = await self._layer_1_data_collection(indicator_outputs)
            
            # Layer 2: æ³¢å‹•æ€§æŒ‡æ¨™è¨ˆç®— (16ms)
            enhanced_metrics = await self._layer_2_volatility_metrics(volatility_metrics, indicator_outputs)
            
            # Layer 3: è‡ªé©æ‡‰åƒæ•¸èª¿æ•´ (12ms)
            adaptive_parameters = await self._layer_3_adaptive_parameters(standardized_signals, enhanced_metrics)
            
            # Layer 4: æ³¢å‹•é©æ‡‰ä¿¡è™Ÿç”Ÿæˆ (8ms)
            volatility_adapted_signals = await self._layer_4_strategy_signals(adaptive_parameters, enhanced_metrics)
            
            # è¨˜éŒ„æ€§èƒ½
            end_time = time.time()
            processing_time = (end_time - start_time) * 1000
            self.processing_times['total'].append(processing_time)
            
            logger.info(f"æ³¢å‹•é©æ‡‰è™•ç†å®Œæˆ: {len(volatility_adapted_signals)} å€‹ä¿¡è™Ÿ, è€—æ™‚ {processing_time:.1f}ms")
            
            return volatility_adapted_signals
            
        except Exception as e:
            logger.error(f"æ³¢å‹•é©æ‡‰è™•ç†å¤±æ•—: {e}")
            return []
    
    async def _layer_1_data_collection(self, indicator_outputs: Dict[str, Any] = None) -> VolatilityMetrics:
        """Layer 1: æ³¢å‹•æ€§æ•¸æ“šæ”¶é›†å±¤ (20ms) - ç¬¦åˆ JSON è¦ç¯„"""
        start_time = time.time()
        
        try:
            # 1. æ­·å²æ³¢å‹•æ€§è¨ˆç®— (5ms) - JSON è¦ç¯„è¦æ±‚
            if indicator_outputs and 'OHLCV' in indicator_outputs:
                ohlcv_data = indicator_outputs['OHLCV']
                historical_volatility = self._calculate_historical_volatility_from_ohlcv(ohlcv_data)
            else:
                # å‚™ç”¨è¨ˆç®—æ–¹å¼
                historical_volatility = self._calculate_historical_volatility()
            
            # 2. å¯¦ç¾æ³¢å‹•æ€§è¨ˆç®— (3ms) - JSON è¦ç¯„è¦æ±‚
            if indicator_outputs and 'high_frequency_prices' in indicator_outputs:
                hf_prices = indicator_outputs['high_frequency_prices']
                realized_volatility = self._calculate_realized_volatility_from_hf(hf_prices)
            else:
                # å‚™ç”¨è¨ˆç®—æ–¹å¼
                realized_volatility = self._calculate_realized_volatility()
            
            # 3. æ³¢å‹•æ€§åˆ¶åº¦æª¢æ¸¬ (12ms) - JSON è¦ç¯„è¦æ±‚ï¼ŒåŒ…å«å¤šé‡ç¢ºèª
            volume_data = indicator_outputs.get('volume_data', []) if indicator_outputs else []
            phase3_liquidity = indicator_outputs.get('phase3_liquidity_regime', {}) if indicator_outputs else {}
            
            volatility_regime, regime_stability = await self._detect_volatility_regime_enhanced(
                volume_data=volume_data, 
                phase3_confirmation=phase3_liquidity
            )
            
            # çµ„è£çµæœ - åŒ…å«æ‰€æœ‰ JSON è¦ç¯„è¦æ±‚çš„è¼¸å‡º
            result = VolatilityMetrics(
                current_volatility=realized_volatility,
                volatility_trend=0.0,  # å°‡åœ¨ Layer 2 è¨ˆç®—
                volatility_percentile=0.5,  # å°‡åœ¨ Layer 2 è¨ˆç®—
                regime_stability=regime_stability,
                micro_volatility=realized_volatility * 0.8,
                intraday_volatility=realized_volatility * 1.2,
                enhanced_volatility_percentile=0.5,
                volatility_regime=volatility_regime,
                market_activity_factor=0.5,
                regime_change_probability=0.1,
                volume_confirmation=len(volume_data) > 0,
                cross_module_validation=bool(phase3_liquidity),
                persistence_score=0.7,
                timestamp=datetime.now()
            )
            
            # è¨˜éŒ„è™•ç†æ™‚é–“
            processing_time = (time.time() - start_time) * 1000
            self.processing_times['layer_1'].append(processing_time)
            
            logger.debug(f"Layer 1 å®Œæˆ: {processing_time:.1f}ms")
            return result
            
        except Exception as e:
            logger.error(f"Layer 1 æ•¸æ“šæ”¶é›†å¤±æ•—: {e}")
            return self._get_minimal_volatility_metrics()
    
    async def _layer_2_volatility_metrics(self, base_metrics: VolatilityMetrics, 
                                        market_data: Dict[str, Any] = None) -> VolatilityMetrics:
        """Layer 2: æ³¢å‹•æ€§æŒ‡æ¨™è¨ˆç®—å±¤ (16ms)"""
        start_time = time.time()
        
        try:
            # 1. åŠ æ¬Šç™¾åˆ†ä½è¨ˆç®— (4ms)
            enhanced_percentile = self._calculate_enhanced_percentile(base_metrics.current_volatility)
            
            # 2. æ³¢å‹•æ€§è¶¨å‹¢è¨ˆç®— (4ms)
            volatility_trend = self._calculate_volatility_trend()
            
            # 3. åˆ¶åº¦ç©©å®šæ€§è©•ä¼° (3ms)
            regime_stability = self._assess_regime_stability()
            
            # 4. å¸‚å ´æ´»èºå› å­è¨ˆç®— (2ms)
            activity_factor = self._calculate_market_activity_factor()
            
            # 5. åˆ¶åº¦è®ŠåŒ–æ¦‚ç‡ (3ms)
            change_probability = self._calculate_regime_change_probability()
            
            # æ›´æ–°æŒ‡æ¨™
            enhanced_metrics = VolatilityMetrics(
                current_volatility=base_metrics.current_volatility,
                volatility_trend=volatility_trend,
                volatility_percentile=enhanced_percentile,
                regime_stability=regime_stability,
                micro_volatility=base_metrics.micro_volatility,
                intraday_volatility=base_metrics.intraday_volatility,
                enhanced_volatility_percentile=enhanced_percentile,
                volatility_regime=base_metrics.volatility_regime,
                market_activity_factor=activity_factor,
                regime_change_probability=change_probability,
                volume_confirmation=base_metrics.volume_confirmation,
                cross_module_validation=base_metrics.cross_module_validation,
                persistence_score=base_metrics.persistence_score,
                timestamp=datetime.now()
            )
            
            # è¨˜éŒ„è™•ç†æ™‚é–“
            processing_time = (time.time() - start_time) * 1000
            self.processing_times['layer_2'].append(processing_time)
            
            logger.debug(f"Layer 2 å®Œæˆ: {processing_time:.1f}ms")
            return enhanced_metrics
            
        except Exception as e:
            logger.error(f"Layer 2 æŒ‡æ¨™è¨ˆç®—å¤±æ•—: {e}")
            return base_metrics
    
    async def _layer_3_adaptive_parameters(self, standardized_signals: List[Dict[str, Any]], 
                                          volatility_metrics: VolatilityMetrics) -> Dict[str, Any]:
        """Layer 3: è‡ªé©æ‡‰åƒæ•¸èª¿æ•´å±¤ (12ms) - ç¬¦åˆ JSON è¦ç¯„"""
        start_time = time.time()
        
        try:
            adaptive_params = {}
            
            # 1. ä¿¡è™Ÿé–¾å€¼è‡ªé©æ‡‰ (1ms) - JSON è¦ç¯„è¦æ±‚
            adaptive_threshold = self._calculate_signal_threshold_adaptation(volatility_metrics)
            adaptive_params['adaptive_signal_threshold'] = adaptive_threshold
            
            # 2. å€‰ä½å¤§å°ç¸®æ”¾ (1ms) - JSON è¦ç¯„è¦æ±‚
            base_position_size = 1.0  # åŸºç¤å€‰ä½å¤§å°
            adaptive_position_size = self._calculate_position_size_scaling(
                volatility_metrics, base_position_size
            )
            adaptive_params['adaptive_position_size'] = adaptive_position_size
            
            # 3. æ™‚é–“æ¡†æ¶å„ªåŒ– (4ms) - JSON è¦ç¯„è¦æ±‚
            optimal_timeframe_plan = self._calculate_timeframe_optimization(volatility_metrics)
            adaptive_params['optimal_timeframe_with_transition_plan'] = optimal_timeframe_plan
            
            # 4. å¸‚å ´æƒ…ç·’æ•´åˆ (3ms) - JSON è¦ç¯„è¦æ±‚
            sentiment_adjustment = self._calculate_market_sentiment_integration(volatility_metrics)
            adaptive_params['sentiment_weighted_adjustment'] = sentiment_adjustment
            
            # 5. è™•ç†æ¨™æº–åŒ–ä¿¡è™Ÿä»¥æ‡‰ç”¨è‡ªé©æ‡‰åƒæ•¸
            processed_signals = []
            for signal in standardized_signals:
                processed_signal = signal.copy()
                
                # æ‡‰ç”¨è‡ªé©æ‡‰é–¾å€¼
                if 'strength' in processed_signal:
                    if processed_signal['strength'] < adaptive_threshold:
                        processed_signal['filtered'] = True
                        processed_signal['filter_reason'] = 'below_adaptive_threshold'
                
                # æ‡‰ç”¨å€‰ä½å¤§å°èª¿æ•´
                processed_signal['adaptive_position_multiplier'] = adaptive_position_size
                
                # æ‡‰ç”¨æƒ…ç·’æ¬Šé‡
                if 'strength' in processed_signal:
                    processed_signal['strength'] *= sentiment_adjustment
                    processed_signal['strength'] = min(1.0, processed_signal['strength'])
                
                processed_signals.append(processed_signal)
            
            adaptive_params['processed_signals'] = processed_signals
            
            # è¨˜éŒ„è™•ç†æ™‚é–“
            processing_time = (time.time() - start_time) * 1000
            self.processing_times['layer_3'].append(processing_time)
            
            logger.debug(f"Layer 3 å®Œæˆ: {processing_time:.1f}ms")
            return adaptive_params
            
        except Exception as e:
            logger.error(f"Layer 3 è‡ªé©æ‡‰åƒæ•¸èª¿æ•´å¤±æ•—: {e}")
            return {'processed_signals': standardized_signals}
    
    async def _layer_4_strategy_signals(self, adaptive_parameters: Dict[str, Any],
                                       volatility_metrics: VolatilityMetrics) -> List[Dict[str, Any]]:
        """Layer 4: æ³¢å‹•é©æ‡‰ä¿¡è™Ÿç”Ÿæˆå±¤ (8ms) - ç¬¦åˆ JSON è¦ç¯„"""
        start_time = time.time()
        
        try:
            generated_signals = []
            processed_signals = adaptive_parameters.get('processed_signals', [])
            
            # 1. æ³¢å‹•çªç ´ä¿¡è™Ÿ (4ms) - JSON è¦ç¯„è¦æ±‚
            breakout_signals = self._generate_volatility_breakout_signals(volatility_metrics, adaptive_parameters)
            generated_signals.extend(breakout_signals)
            
            # 2. æ³¢å‹•å‡å€¼å›æ­¸ä¿¡è™Ÿ (4ms) - JSON è¦ç¯„è¦æ±‚
            mean_reversion_signals = self._generate_volatility_mean_reversion_signals(volatility_metrics, adaptive_parameters)
            generated_signals.extend(mean_reversion_signals)
            
            # 3. æ³¢å‹•åˆ¶åº¦è®ŠåŒ–ä¿¡è™Ÿ (5ms) - JSON è¦ç¯„è¦æ±‚
            regime_change_signals = self._generate_volatility_regime_change_signals(volatility_metrics, adaptive_parameters)
            generated_signals.extend(regime_change_signals)
            
            # 4. æ•´åˆè™•ç†éçš„ä¿¡è™Ÿ
            for signal in processed_signals:
                if not signal.get('filtered', False):
                    # è½‰æ›ç‚ºçµ±ä¸€ä¿¡è™Ÿæ ¼å¼
                    unified_signal = self._convert_to_unified_signal_format(signal, volatility_metrics, adaptive_parameters)
                    generated_signals.append(unified_signal)
            
            # è¨˜éŒ„è™•ç†æ™‚é–“
            processing_time = (time.time() - start_time) * 1000
            self.processing_times['layer_4'].append(processing_time)
            
            logger.debug(f"Layer 4 å®Œæˆ: {processing_time:.1f}ms, ç”Ÿæˆ {len(generated_signals)} å€‹ä¿¡è™Ÿ")
            return generated_signals
            
        except Exception as e:
            logger.error(f"Layer 4 ç­–ç•¥ä¿¡è™Ÿç”Ÿæˆå¤±æ•—: {e}")
            return []
    
    def _generate_sample_price_data(self) -> List[float]:
        """ç”Ÿæˆæ¨£æœ¬åƒ¹æ ¼æ•¸æ“š"""
        base_price = 50000.0
        prices = []
        
        for i in range(50):
            noise = np.random.normal(0, base_price * 0.01)  # 1% å™ªéŸ³
            price = base_price + noise
            prices.append(price)
            base_price = price * 0.9999  # è¼•å¾®è¶¨å‹¢
        
        return prices
    
    def _generate_sample_volume_data(self) -> List[float]:
        """ç”Ÿæˆæ¨£æœ¬æˆäº¤é‡æ•¸æ“š"""
        base_volume = 1000000.0
        volumes = []
        
        for i in range(50):
            noise = np.random.normal(0, base_volume * 0.2)  # 20% å™ªéŸ³
            volume = max(0, base_volume + noise)
            volumes.append(volume)
        
        return volumes
    
    
    def _calculate_historical_volatility(self) -> float:
        """è¨ˆç®—æ­·å²æ³¢å‹•æ€§"""
        if len(self.price_history) < 21:
            return 0.02  # é è¨­ 2%
        
        prices = list(self.price_history)
        returns = np.diff(np.log(prices))
        volatility = np.std(returns[-21:]) * np.sqrt(252)  # å¹´åŒ–
        
        # æ¨™æº–åŒ–åˆ° 0-1 ç¯„åœ
        return min(1.0, volatility / 2.0)
    
    def _calculate_realized_volatility(self) -> float:
        """è¨ˆç®—å¯¦ç¾æ³¢å‹•æ€§"""
        if len(self.price_history) < 10:
            return 0.02
        
        prices = list(self.price_history)
        returns = np.diff(np.log(prices))
        realized_vol = np.sqrt(np.sum(returns[-10:]**2) * 252 / 10)
        
        return min(1.0, realized_vol / 2.0)
    
    async def _detect_volatility_regime_enhanced(self, volume_data: List[float] = None, 
                                                phase3_confirmation: Dict[str, Any] = None) -> Tuple[VolatilityRegime, float]:
        """å¢å¼·çš„æ³¢å‹•æ€§åˆ¶åº¦æª¢æ¸¬ - åŒ…å«å¤šé‡ç¢ºèªæ©Ÿåˆ¶"""
        try:
            current_vol = self._calculate_realized_volatility()
            thresholds = self.config["volatility_thresholds"]
            
            # åŸºç¤åˆ¶åº¦åˆ†é¡
            if current_vol < thresholds["low"]:
                base_regime = VolatilityRegime.LOW
            elif current_vol < thresholds["normal"]:
                base_regime = VolatilityRegime.NORMAL
            elif current_vol < thresholds["high"]:
                base_regime = VolatilityRegime.HIGH
            else:
                base_regime = VolatilityRegime.EXTREME
            
            # å¤šé‡ç¢ºèªæ©Ÿåˆ¶
            confirmation_score = 0.0
            total_confirmations = 3
            
            # 1. æˆäº¤é‡ç¢ºèª
            if volume_data and len(volume_data) > 0:
                volume_percentile = np.percentile(volume_data, 75) if len(volume_data) > 1 else volume_data[0]
                current_volume = volume_data[-1] if volume_data else 0
                if current_volume > volume_percentile:
                    confirmation_score += 1.0
                logger.debug(f"æˆäº¤é‡ç¢ºèª: {current_volume > volume_percentile}")
            
            # 2. æŒçºŒæœŸé–“ç¢ºèª
            self.regime_stability_buffer.append(current_vol)
            if len(self.regime_stability_buffer) >= 3:
                recent_regimes = []
                for vol in list(self.regime_stability_buffer)[-3:]:
                    if vol < thresholds["low"]:
                        recent_regimes.append(VolatilityRegime.LOW)
                    elif vol < thresholds["normal"]:
                        recent_regimes.append(VolatilityRegime.NORMAL)
                    elif vol < thresholds["high"]:
                        recent_regimes.append(VolatilityRegime.HIGH)
                    else:
                        recent_regimes.append(VolatilityRegime.EXTREME)
                
                # æª¢æŸ¥åˆ¶åº¦ä¸€è‡´æ€§
                if recent_regimes.count(base_regime) >= 2:
                    confirmation_score += 1.0
                logger.debug(f"æŒçºŒæœŸé–“ç¢ºèª: {recent_regimes.count(base_regime) >= 2}")
            
            # 3. Phase3 è·¨æ¨¡çµ„é©—è­‰
            if phase3_confirmation and 'liquidity_regime' in phase3_confirmation:
                phase3_regime = phase3_confirmation['liquidity_regime']
                # ç°¡åŒ–çš„å°æ‡‰é—œä¿‚æª¢æŸ¥
                if (base_regime in [VolatilityRegime.HIGH, VolatilityRegime.EXTREME] and 
                    phase3_regime in ['high_volatility', 'stressed']):
                    confirmation_score += 1.0
                elif (base_regime in [VolatilityRegime.LOW, VolatilityRegime.NORMAL] and 
                      phase3_regime in ['normal', 'stable']):
                    confirmation_score += 1.0
                logger.debug(f"Phase3 ç¢ºèª: ç›¸å®¹æ€§åŒ¹é…")
            
            # è¨ˆç®—åˆ¶åº¦ç©©å®šæ€§
            confirmation_ratio = confirmation_score / total_confirmations
            stability = max(0.3, min(1.0, confirmation_ratio))
            
            self.current_regime = base_regime
            logger.debug(f"åˆ¶åº¦æª¢æ¸¬: {base_regime.value}, ç©©å®šæ€§: {stability:.2f}")
            
            return base_regime, stability
            
        except Exception as e:
            logger.error(f"å¢å¼·åˆ¶åº¦æª¢æ¸¬å¤±æ•—: {e}")
            return VolatilityRegime.NORMAL, 0.5
    
    async def _detect_volatility_regime_with_multi_confirmation(self, volume_data: List[float] = None):
        """å¤šé‡ç¢ºèªçš„åˆ¶åº¦æª¢æ¸¬ - JSON æŒ‡å®šåç¨±"""
        return await self._detect_volatility_regime_enhanced(volume_data)
    
    def _calculate_enhanced_percentile(self, current_vol: float) -> float:
        """è¨ˆç®—åŠ æ¬Šç™¾åˆ†ä½"""
        self.volatility_history.append(current_vol)
        
        if len(self.volatility_history) < 20:
            return 0.5
        
        # æŒ‡æ•¸è¡°æ¸›æ¬Šé‡
        weights = np.exp(-np.arange(len(self.volatility_history)) * 0.1)
        weights = weights[::-1]  # åå‘ï¼Œæœ€æ–°çš„æ¬Šé‡æœ€å¤§
        
        sorted_indices = np.argsort(list(self.volatility_history))
        weighted_rank = sum(weights[i] for i in sorted_indices if list(self.volatility_history)[i] <= current_vol)
        total_weight = sum(weights)
        
        return weighted_rank / total_weight
    
    def _calculate_enhanced_volatility_percentile(self, current_vol: float) -> float:
        """è¨ˆç®—å¢å¼·æ³¢å‹•ç‡ç™¾åˆ†ä½ - JSON æŒ‡å®šåç¨±"""
        return self._calculate_enhanced_percentile(current_vol)
    
    def _adapt_signal_threshold(self, base_threshold: float, volatility_metrics: VolatilityMetrics) -> float:
        """é©æ‡‰æ€§ä¿¡è™Ÿé–¾å€¼èª¿æ•´"""
        # æ ¹æ“šæ³¢å‹•æ€§èª¿æ•´é–¾å€¼
        volatility_factor = volatility_metrics.current_volatility
        regime_factor = {
            VolatilityRegime.LOW: 1.2,      # ä½æ³¢å‹•æ™‚æé«˜é–¾å€¼
            VolatilityRegime.NORMAL: 1.0,   # æ­£å¸¸æ³¢å‹•ä¿æŒ
            VolatilityRegime.HIGH: 0.8,     # é«˜æ³¢å‹•æ™‚é™ä½é–¾å€¼
            VolatilityRegime.EXTREME: 0.6   # æ¥µç«¯æ³¢å‹•æ™‚å¤§å¹…é™ä½
        }.get(volatility_metrics.volatility_regime, 1.0)
        
        # å¸‚å ´æ´»èºåº¦èª¿æ•´
        activity_factor = 1.0 + (volatility_metrics.market_activity_factor - 0.5) * 0.2
        
        adapted_threshold = base_threshold * regime_factor * activity_factor
        return max(0.1, min(0.9, adapted_threshold))
    
    def _scale_position_size(self, base_position: float, volatility_metrics: VolatilityMetrics) -> float:
        """å€‰ä½è¦æ¨¡ç¸®æ”¾"""
        # æ³¢å‹•æ€§é€†å‘ç¸®æ”¾
        volatility_scale = max(0.5, 1.0 - volatility_metrics.current_volatility * 1.5)
        
        # åˆ¶åº¦ç©©å®šæ€§èª¿æ•´
        stability_scale = 0.8 + volatility_metrics.regime_stability * 0.4
        
        # ç¶œåˆç¸®æ”¾
        position_scale = volatility_scale * stability_scale
        scaled_position = base_position * position_scale
        
        return max(0.1, min(2.0, scaled_position))
    
    def _optimize_timeframe(self, base_timeframe: str, volatility_metrics: VolatilityMetrics) -> Dict[str, Any]:
        """æ™‚é–“æ¡†æ¶å„ªåŒ–"""
        timeframe_mapping = {
            '1m': 1, '5m': 5, '15m': 15, '30m': 30, '1h': 60, '4h': 240, '1d': 1440
        }
        
        current_minutes = timeframe_mapping.get(base_timeframe, 15)
        
        # æ ¹æ“šæ³¢å‹•æ€§èª¿æ•´æ™‚é–“æ¡†æ¶
        if volatility_metrics.volatility_regime == VolatilityRegime.EXTREME:
            # æ¥µç«¯æ³¢å‹•æ™‚ä½¿ç”¨è¼ƒçŸ­æ™‚é–“æ¡†æ¶
            optimal_minutes = max(1, current_minutes // 2)
        elif volatility_metrics.volatility_regime == VolatilityRegime.LOW:
            # ä½æ³¢å‹•æ™‚ä½¿ç”¨è¼ƒé•·æ™‚é–“æ¡†æ¶
            optimal_minutes = min(240, current_minutes * 2)
        else:
            optimal_minutes = current_minutes
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ™‚é–“æ¡†æ¶
        optimal_timeframe = base_timeframe
        min_diff = float('inf')
        for tf, minutes in timeframe_mapping.items():
            diff = abs(minutes - optimal_minutes)
            if diff < min_diff:
                min_diff = diff
                optimal_timeframe = tf
        
        return {
            'optimal_timeframe': optimal_timeframe,
            'confidence': volatility_metrics.regime_stability,
            'transition_plan': {
                'from': base_timeframe,
                'to': optimal_timeframe,
                'reason': f"volatility_regime_{volatility_metrics.volatility_regime.value}"
            }
        }
    
    def _calculate_volatility_trend(self) -> float:
        """è¨ˆç®—æ³¢å‹•æ€§è¶¨å‹¢"""
        if len(self.volatility_history) < 10:
            return 0.0
        
        recent_vols = list(self.volatility_history)[-10:]
        x = np.arange(len(recent_vols))
        slope = np.polyfit(x, recent_vols, 1)[0]
        
        # æ¨™æº–åŒ–åˆ° -1 åˆ° 1
        return max(-1, min(1, slope * 100))
    
    def _assess_regime_stability(self) -> float:
        """è©•ä¼°åˆ¶åº¦ç©©å®šæ€§"""
        if len(self.regime_stability_buffer) < 5:
            return 0.7
        
        recent_vols = list(self.regime_stability_buffer)[-5:]
        stability = 1.0 - (np.std(recent_vols) / (np.mean(recent_vols) + 1e-8))
        return max(0, min(1, stability))
    
    def _calculate_market_activity_factor(self) -> float:
        """è¨ˆç®—å¸‚å ´æ´»èºå› å­"""
        if len(self.price_history) < 10 or len(self.volume_history) < 10:
            return 0.5
        
        # ATR è¨ˆç®—ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        prices = list(self.price_history)[-10:]
        high_low_range = (max(prices) - min(prices)) / min(prices)
        
        # æˆäº¤é‡æ´»èºåº¦
        volumes = list(self.volume_history)[-10:]
        current_vol = volumes[-1]
        avg_vol = np.mean(volumes[:-1])
        volume_ratio = current_vol / (avg_vol + 1e-8)
        
        # ç¶œåˆæ´»èºå› å­
        activity = (high_low_range * 10 + min(volume_ratio, 3.0) / 3.0) / 2
        return min(1.0, activity)
    
    def _calculate_signal_threshold_adaptation(self, volatility_metrics: VolatilityMetrics) -> float:
        """è¨ˆç®—ä¿¡è™Ÿé–¾å€¼è‡ªé©æ‡‰ - JSON è¦ç¯„è¦æ±‚"""
        base_threshold = 0.5
        
        if volatility_metrics.volatility_percentile > 0.8:
            # é«˜æ³¢å‹•æ™‚é™ä½é–¾å€¼ 20%
            return base_threshold * 0.8
        elif volatility_metrics.volatility_percentile < 0.2:
            # ä½æ³¢å‹•æ™‚æé«˜é–¾å€¼ 10%
            return base_threshold * 1.1
        else:
            return base_threshold
    
    def _calculate_position_size_scaling(self, volatility_metrics: VolatilityMetrics, base_size: float) -> float:
        """è¨ˆç®—å€‰ä½å¤§å°ç¸®æ”¾ - JSON è¦ç¯„è¦æ±‚"""
        # base_size * (1 / sqrt(current_volatility))
        scaling_factor = 1.0 / np.sqrt(volatility_metrics.current_volatility + 0.01)  # é¿å…é™¤é›¶
        return base_size * min(2.0, max(0.5, scaling_factor))  # é™åˆ¶åœ¨ 0.5-2.0 ç¯„åœ
    
    def _calculate_timeframe_optimization(self, volatility_metrics: VolatilityMetrics) -> Dict[str, Any]:
        """è¨ˆç®—æ™‚é–“æ¡†æ¶å„ªåŒ– - JSON è¦ç¯„è¦æ±‚"""
        current_timeframe = "15m"  # é»˜èªæ™‚é–“æ¡†æ¶
        
        # æ™ºèƒ½åˆ‡æ›é‚è¼¯
        if (volatility_metrics.volatility_regime in [VolatilityRegime.HIGH, VolatilityRegime.EXTREME] and 
            volatility_metrics.regime_stability < 0.5):
            optimal_timeframe = "5m"  # é«˜æ³¢å‹•ä½ç©©å®šæ€§ â†’ çŸ­æ™‚é–“æ¡†æ¶
            transition_needed = True
        elif (volatility_metrics.volatility_regime == VolatilityRegime.LOW and 
              volatility_metrics.regime_stability > 0.8):
            optimal_timeframe = "30m"  # ä½æ³¢å‹•é«˜ç©©å®šæ€§ â†’ é•·æ™‚é–“æ¡†æ¶
            transition_needed = True
        else:
            optimal_timeframe = current_timeframe
            transition_needed = False
        
        return {
            "current_timeframe": current_timeframe,
            "optimal_timeframe": optimal_timeframe,
            "transition_needed": transition_needed,
            "min_hold_duration_minutes": 15,
            "parallel_calculation_required": transition_needed,
            "benefit_threshold": 0.15
        }
    
    def _calculate_historical_volatility_from_ohlcv(self, ohlcv_data: List[Dict]) -> float:
        """å¾ OHLCV æ•¸æ“šè¨ˆç®—æ­·å²æ³¢å‹•æ€§ - JSON è¦ç¯„æ–¹æ³•"""
        try:
            if len(ohlcv_data) < 21:
                return self._calculate_historical_volatility()  # å‚™ç”¨æ–¹æ³•
            
            closes = [candle.get('close', 0) for candle in ohlcv_data[-21:]]
            if not closes or any(c <= 0 for c in closes):
                return self._calculate_historical_volatility()  # å‚™ç”¨æ–¹æ³•
            
            # close.pct_change().rolling(window).std() * sqrt(252)
            returns = []
            for i in range(1, len(closes)):
                pct_change = (closes[i] - closes[i-1]) / closes[i-1]
                returns.append(pct_change)
            
            if len(returns) > 0:
                volatility = np.std(returns) * np.sqrt(252)
                return min(1.0, volatility / 2.0)  # æ¨™æº–åŒ–
            else:
                return 0.02  # é è¨­å€¼
                
        except Exception as e:
            logger.error(f"OHLCV æ³¢å‹•æ€§è¨ˆç®—å¤±æ•—: {e}")
            return self._calculate_historical_volatility()  # å‚™ç”¨æ–¹æ³•
    
    def _calculate_realized_volatility_from_hf(self, hf_prices: List[float]) -> float:
        """å¾é«˜é »åƒ¹æ ¼æ•¸æ“šè¨ˆç®—å¯¦ç¾æ³¢å‹•æ€§ - JSON è¦ç¯„æ–¹æ³•"""
        try:
            if len(hf_prices) < 10:
                return self._calculate_realized_volatility()  # å‚™ç”¨æ–¹æ³•
            
            # sqrt(sum(log_returns^2) * 252/n)
            log_returns = []
            for i in range(1, len(hf_prices)):
                if hf_prices[i] > 0 and hf_prices[i-1] > 0:
                    log_return = np.log(hf_prices[i] / hf_prices[i-1])
                    log_returns.append(log_return)
            
            if len(log_returns) > 0:
                realized_vol = np.sqrt(np.sum(np.array(log_returns)**2) * 252 / len(log_returns))
                return min(1.0, realized_vol / 2.0)  # æ¨™æº–åŒ–
            else:
                return 0.02  # é è¨­å€¼
                
        except Exception as e:
            logger.error(f"é«˜é »æ³¢å‹•æ€§è¨ˆç®—å¤±æ•—: {e}")
            return self._calculate_realized_volatility()  # å‚™ç”¨æ–¹æ³•
    
    def _calculate_regime_change_probability(self) -> float:
        """è¨ˆç®—åˆ¶åº¦è®ŠåŒ–æ¦‚ç‡"""
        if len(self.volatility_history) < 20:
            return 0.1
        
        recent_vols = list(self.volatility_history)[-5:]
        historical_mean = np.mean(list(self.volatility_history)[:-5])
        
        current_deviation = abs(np.mean(recent_vols) - historical_mean)
        change_prob = min(1.0, current_deviation / self.regime_change_threshold)
        
        return change_prob
    
    def _generate_volatility_breakout_signals(self, volatility_metrics: VolatilityMetrics, 
                                             adaptive_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ³¢å‹•çªç ´ä¿¡è™Ÿ - JSON è¦ç¯„è¦æ±‚"""
        signals = []
        
        # çªç ´æ¢ä»¶æª¢æŸ¥
        condition_met = (
            volatility_metrics.enhanced_volatility_percentile > 0.9 and
            volatility_metrics.volatility_trend > 0.5 and
            volatility_metrics.volume_confirmation
        )
        
        if condition_met:
            # å¸‚å ´ç‹€æ…‹èª¿æ•´
            base_strength = 0.8
            market_adjustment = volatility_metrics.market_activity_factor
            sentiment_adjustment = adaptive_params.get('sentiment_weighted_adjustment', 1.0)
            
            signal_strength = min(1.0, base_strength * market_adjustment * sentiment_adjustment)
            
            # åŸ·è¡Œå„ªå…ˆç´š
            if (volatility_metrics.enhanced_volatility_percentile > 0.95 and 
                volatility_metrics.market_activity_factor > 2.0):
                priority = "HIGH"
            elif (volatility_metrics.enhanced_volatility_percentile > 0.9 and 
                  volatility_metrics.regime_stability > 0.7):
                priority = "MEDIUM"
            else:
                priority = "LOW"
            
            signal = {
                "signal_type": "VOLATILITY_BREAKOUT",
                "signal_strength": signal_strength,
                "signal_confidence": min(1.0, volatility_metrics.persistence_score),
                "execution_priority": priority,
                "timestamp": datetime.now().isoformat(),
                "source": "phase1b_volatility_adaptation_v2",
                "market_context": {
                    "current_volatility": volatility_metrics.current_volatility,
                    "volatility_percentile": volatility_metrics.enhanced_volatility_percentile,
                    "volatility_trend": volatility_metrics.volatility_trend,
                    "market_activity_level": "HIGH" if volatility_metrics.market_activity_factor > 0.7 else "NORMAL"
                }
            }
            signals.append(signal)
        
        return signals
    
    def _generate_volatility_mean_reversion_signals(self, volatility_metrics: VolatilityMetrics,
                                                   adaptive_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ³¢å‹•å‡å€¼å›æ­¸ä¿¡è™Ÿ - JSON è¦ç¯„è¦æ±‚"""
        signals = []
        
        # å‡å€¼å›æ­¸æ¢ä»¶
        condition_met = (
            volatility_metrics.enhanced_volatility_percentile > 0.8 and
            volatility_metrics.regime_stability > 0.7 and
            volatility_metrics.volume_confirmation
        )
        
        if condition_met:
            base_strength = 0.7  # è¼ƒä¿å®ˆçš„å¼·åº¦
            signal_strength = min(0.85, base_strength * volatility_metrics.regime_stability)
            
            # æŠ—å‡çªç ´èª¿æ•´
            if volatility_metrics.regime_change_probability > 0.3:
                signal_strength *= 0.8  # é™ä½å¼·åº¦
            
            signal = {
                "signal_type": "VOLATILITY_MEAN_REVERSION", 
                "signal_strength": signal_strength,
                "signal_confidence": min(1.0, volatility_metrics.regime_stability),
                "execution_priority": "MEDIUM",
                "timestamp": datetime.now().isoformat(),
                "source": "phase1b_volatility_adaptation_v2",
                "anti_false_signal": {
                    "consecutive_confirmation": True,
                    "volume_filter": volatility_metrics.volume_confirmation,
                    "regime_stability_check": volatility_metrics.regime_stability > 0.7
                }
            }
            signals.append(signal)
        
        return signals
    
    def _generate_volatility_regime_change_signals(self, volatility_metrics: VolatilityMetrics,
                                                  adaptive_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ³¢å‹•åˆ¶åº¦è®ŠåŒ–ä¿¡è™Ÿ - JSON è¦ç¯„è¦æ±‚"""
        signals = []
        
        # åˆ¶åº¦è®ŠåŒ–æ¢ä»¶
        condition_met = (
            volatility_metrics.regime_change_probability > 0.7 and
            volatility_metrics.regime_stability < 0.3 and
            volatility_metrics.cross_module_validation
        )
        
        if condition_met:
            base_strength = 0.9  # é«˜å¼·åº¦ä¿¡è™Ÿ
            confidence_boost = 0.2 if volatility_metrics.cross_module_validation else 0.0
            
            signal = {
                "signal_type": "VOLATILITY_REGIME_CHANGE",
                "signal_strength": min(1.0, base_strength + confidence_boost),
                "signal_confidence": min(1.0, volatility_metrics.regime_change_probability),
                "execution_priority": "HIGH" if volatility_metrics.cross_module_validation else "MEDIUM",
                "timestamp": datetime.now().isoformat(),
                "source": "phase1b_volatility_adaptation_v2",
                "enhanced_validation": {
                    "phase3_cross_confirmation": volatility_metrics.cross_module_validation,
                    "volume_spike_validation": volatility_metrics.volume_confirmation,
                    "multi_timeframe_check": True
                },
                "note": "å°ˆæ³¨æ–¼æ³¢å‹•æ€§åˆ¶åº¦è®ŠåŒ–ï¼Œèˆ‡phase3çš„æµå‹•æ€§åˆ¶åº¦åˆ†æäº’è£œ"
            }
            signals.append(signal)
        
        return signals
    
    def _convert_to_unified_signal_format(self, signal: Dict[str, Any], 
                                        volatility_metrics: VolatilityMetrics,
                                        adaptive_params: Dict[str, Any]) -> Dict[str, Any]:
        """è½‰æ›ç‚ºçµ±ä¸€ä¿¡è™Ÿæ ¼å¼"""
        return {
            "signal_type": signal.get("type", "ADAPTED_SIGNAL"),
            "signal_strength": signal.get("strength", 0.5),
            "signal_confidence": signal.get("confidence", 0.5),
            "execution_priority": "MEDIUM",
            "timestamp": datetime.now().isoformat(),
            "source": "phase1b_volatility_adaptation_v2",
            "adaptive_parameters": {
                "signal_threshold": adaptive_params.get('adaptive_signal_threshold', 0.5),
                "position_size_multiplier": adaptive_params.get('adaptive_position_size', 1.0),
                "optimal_timeframe": adaptive_params.get('optimal_timeframe_with_transition_plan', {}).get('optimal_timeframe', '15m'),
                "risk_adjustment": max(0, 1 - volatility_metrics.current_volatility) * 0.5
            },
            "market_context": {
                "current_volatility": volatility_metrics.current_volatility,
                "volatility_percentile": volatility_metrics.enhanced_volatility_percentile,
                "volatility_regime_stability": volatility_metrics.regime_stability,
                "market_activity_level": "HIGH" if volatility_metrics.market_activity_factor > 0.7 else "NORMAL"
            }
        }
    
    def _calculate_regime_adjustment(self, signal: Dict[str, Any], 
                                   volatility_metrics: VolatilityMetrics) -> float:
        """è¨ˆç®—åˆ¶åº¦æ„ŸçŸ¥èª¿æ•´ä¿‚æ•¸"""
        stability = volatility_metrics.regime_stability
        change_prob = volatility_metrics.regime_change_probability
        
        # åˆ¶åº¦ç©©å®šæ™‚å¢å¼·ï¼Œä¸ç©©å®šæ™‚æŠ‘åˆ¶
        regime_factor = stability * (1 - change_prob * 0.5)
        return max(0.7, min(1.3, regime_factor + 0.5))
    
    def _calculate_activity_adjustment(self, signal: Dict[str, Any], 
                                     volatility_metrics: VolatilityMetrics) -> float:
        """è¨ˆç®—å¸‚å ´æ´»èºåº¦èª¿æ•´ä¿‚æ•¸"""
        activity = volatility_metrics.market_activity_factor
        
        if activity > 0.7:  # é«˜æ´»èº
            return 1.2  # å¢å¼·ä¿¡è™Ÿ
        elif activity < 0.3:  # ä½æ´»èº
            return 0.85  # æŠ‘åˆ¶ä¿¡è™Ÿ
        else:
            return 1.0  # æ­£å¸¸æ´»èº
    
    def _resolve_signal_conflicts(self, adjustments: List[AdaptiveSignalAdjustment]) -> List[AdaptiveSignalAdjustment]:
        """è§£æ±ºä¿¡è™Ÿè¡çª"""
        if len(adjustments) <= 1:
            return adjustments
        
        # æŒ‰èª¿æ•´ä¿‚æ•¸æ’åºï¼Œä¿ç•™æœ€å¼·çš„ä¿¡è™Ÿ
        sorted_adjustments = sorted(adjustments, 
                                  key=lambda x: x.adjustment_factor * x.adjusted_signal.get('strength', 0), 
                                  reverse=True)
        
        # ä¿ç•™å‰70%çš„ä¿¡è™Ÿ
        keep_count = max(1, int(len(sorted_adjustments) * 0.7))
        return sorted_adjustments[:keep_count]
    
    def _optimize_signal_portfolio(self, adjustments: List[AdaptiveSignalAdjustment],
                                 volatility_metrics: VolatilityMetrics) -> List[AdaptiveSignalAdjustment]:
        """å„ªåŒ–ä¿¡è™Ÿçµ„åˆ"""
        # æ ¹æ“šæ³¢å‹•æ€§èª¿æ•´æ•´é«”ä¿¡è™Ÿçµ„åˆ
        regime = volatility_metrics.volatility_regime
        
        if regime == VolatilityRegime.EXTREME:
            # æ¥µç«¯æ³¢å‹•æ™‚ï¼Œåªä¿ç•™æœ€å¼·ä¿¡è™Ÿ
            if adjustments:
                best_adjustment = max(adjustments, key=lambda x: x.adjustment_factor)
                return [best_adjustment]
        
        return adjustments
    
    def _apply_risk_adjustments(self, adjustments: List[AdaptiveSignalAdjustment],
                              volatility_metrics: VolatilityMetrics) -> List[AdaptiveSignalAdjustment]:
        """æ‡‰ç”¨é¢¨éšªèª¿æ•´"""
        risk_factor = 1.0 - volatility_metrics.current_volatility * 0.5
        
        for adjustment in adjustments:
            # æ ¹æ“šæ³¢å‹•æ€§èª¿æ•´é¢¨éšªç·©è§£
            adjustment.risk_mitigation = max(0, min(0.5, (1 - volatility_metrics.current_volatility) * 0.5))
            
            # èª¿æ•´ä¿¡è™Ÿå¼·åº¦ä»¥åæ˜ é¢¨éšª
            if 'strength' in adjustment.adjusted_signal:
                adjustment.adjusted_signal['strength'] *= risk_factor
                adjustment.adjusted_signal['strength'] = min(1.0, adjustment.adjusted_signal['strength'])
        
        return adjustments
    
    async def _regime_monitor(self):
        """åˆ¶åº¦ç›£æ§ä»»å‹™"""
        while self.is_running:
            try:
                await asyncio.sleep(30)  # æ¯30ç§’æª¢æŸ¥ä¸€æ¬¡
                # åˆ¶åº¦ç›£æ§é‚è¼¯
                if len(self.volatility_history) > 0:
                    current_vol = list(self.volatility_history)[-1]
                    logger.debug(f"ç•¶å‰æ³¢å‹•æ€§: {current_vol:.4f}, åˆ¶åº¦: {self.current_regime.value}")
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"åˆ¶åº¦ç›£æ§éŒ¯èª¤: {e}")
    
    async def _performance_monitor(self):
        """æ€§èƒ½ç›£æ§ä»»å‹™"""
        while self.is_running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                # æ€§èƒ½ç›£æ§é‚è¼¯
                for layer, times in self.processing_times.items():
                    if times:
                        avg_time = np.mean(times)
                        logger.debug(f"{layer} å¹³å‡è™•ç†æ™‚é–“: {avg_time:.1f}ms")
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ€§èƒ½ç›£æ§éŒ¯èª¤: {e}")
    
    def _analyze_signal_continuity(self, signals: List[Dict[str, Any]]) -> SignalContinuityMetrics:
        """åˆ†æä¿¡è™Ÿé€£çºŒæ€§ - åŸºæ–¼çœŸå¯¦ä¿¡è™Ÿæ­·å²"""
        try:
            # è¨˜éŒ„ç•¶å‰ä¿¡è™Ÿåˆ°æ­·å²
            current_signals = {
                "timestamp": datetime.now(),
                "signals": signals,
                "signal_count": len(signals)
            }
            self.signal_history.append(current_signals)
            
            if len(self.signal_history) < 3:
                logger.info("ä¿¡è™Ÿæ­·å²ä¸è¶³ï¼Œä½¿ç”¨åŸºç¤è©•ä¼°")
                return self._get_basic_continuity_metrics(signals)
            
            # 1. ä¿¡è™ŸæŒçºŒæ€§ (ä¿¡è™Ÿåœ¨é€£çºŒæ™‚é–“æ®µå…§çš„å‡ºç¾ç‡)
            recent_periods = list(self.signal_history)[-10:]
            signal_appearances = sum(1 for period in recent_periods if period["signal_count"] > 0)
            signal_persistence = signal_appearances / len(recent_periods)
            
            # 2. ä¿¡è™Ÿåˆ†æ­§åº¦ (ä¸åŒä¿¡è™Ÿæºçš„ä¸€è‡´æ€§)
            if signals:
                signal_values = [s.get("value", 0) for s in signals if "value" in s]
                if len(signal_values) > 1:
                    signal_std = np.std(signal_values)
                    signal_mean = np.mean(signal_values)
                    signal_divergence = signal_std / (abs(signal_mean) + 1e-8)
                    signal_divergence = min(1.0, signal_divergence)
                else:
                    signal_divergence = 0.0
            else:
                signal_divergence = 1.0  # æ²’æœ‰ä¿¡è™Ÿæ™‚åˆ†æ­§åº¦æœ€é«˜
            
            # 3. å…±è­˜å¼·åº¦ (å¤šå€‹ä¿¡è™ŸæŒ‡å‘åŒä¸€æ–¹å‘çš„ç¨‹åº¦)
            if signals:
                positive_signals = sum(1 for s in signals if s.get("value", 0) > 0)
                negative_signals = sum(1 for s in signals if s.get("value", 0) < 0)
                total_signals = len(signals)
                
                if total_signals > 0:
                    max_consensus = max(positive_signals, negative_signals)
                    consensus_strength = max_consensus / total_signals
                else:
                    consensus_strength = 0.0
            else:
                consensus_strength = 0.0
            
            # 4. æ™‚é–“ä¸€è‡´æ€§ (ä¿¡è™Ÿå¼·åº¦åœ¨æ™‚é–“ä¸Šçš„ç©©å®šæ€§)
            if len(recent_periods) >= 5:
                signal_counts = [p["signal_count"] for p in recent_periods[-5:]]
                avg_count = np.mean(signal_counts)
                count_std = np.std(signal_counts)
                temporal_consistency = 1.0 - (count_std / (avg_count + 1e-8))
                temporal_consistency = max(0, min(1, temporal_consistency))
            else:
                temporal_consistency = 0.6
            
            # 5. è·¨æ¨¡çµ„ç›¸é—œæ€§ (ä¸åŒæ¨¡çµ„ä¿¡è™Ÿçš„ç›¸é—œæ€§)
            if len(signals) >= 2:
                module_values = {}
                for signal in signals:
                    module = signal.get("module", "unknown")
                    value = signal.get("value", 0)
                    if module not in module_values:
                        module_values[module] = []
                    module_values[module].append(value)
                
                # è¨ˆç®—æ¨¡çµ„é–“ç›¸é—œæ€§
                modules = list(module_values.keys())
                if len(modules) >= 2:
                    correlations = []
                    for i in range(len(modules)):
                        for j in range(i+1, len(modules)):
                            module1_values = module_values[modules[i]]
                            module2_values = module_values[modules[j]]
                            
                            # ç°¡åŒ–ç›¸é—œæ€§è¨ˆç®—
                            avg1 = np.mean(module1_values)
                            avg2 = np.mean(module2_values)
                            correlation = 1.0 - abs(avg1 - avg2) / 2.0  # ç°¡åŒ–çš„ç›¸é—œæ€§åº¦é‡
                            correlations.append(max(0, correlation))
                    
                    cross_module_correlation = np.mean(correlations) if correlations else 0.5
                else:
                    cross_module_correlation = 0.5
            else:
                cross_module_correlation = 0.5
            
            # 6. ä¿¡è™Ÿè¡°æ¸›ç‡ (ä¿¡è™Ÿå¼·åº¦éš¨æ™‚é–“çš„è¡°æ¸›)
            if len(recent_periods) >= 3:
                recent_counts = [p["signal_count"] for p in recent_periods[-3:]]
                if recent_counts[0] > 0:
                    decay_rate = (recent_counts[0] - recent_counts[-1]) / recent_counts[0]
                    decay_rate = max(0, min(1, decay_rate))
                else:
                    decay_rate = 0.5
            else:
                decay_rate = 0.3
            
            result = SignalContinuityMetrics(
                signal_persistence=signal_persistence,
                signal_divergence=signal_divergence,
                consensus_strength=consensus_strength,
                temporal_consistency=temporal_consistency,
                cross_module_correlation=cross_module_correlation,
                signal_decay_rate=decay_rate
            )
            
            logger.info(f"ä¿¡è™Ÿé€£çºŒæ€§åˆ†æå®Œæˆ: æŒçºŒæ€§={signal_persistence:.3f}, å…±è­˜={consensus_strength:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"ä¿¡è™Ÿé€£çºŒæ€§åˆ†æå¤±æ•—: {e}")
            return self._get_basic_continuity_metrics(signals)
    
    def _get_minimal_volatility_metrics(self) -> VolatilityMetrics:
        """ç²å–æœ€å°æ³¢å‹•æ€§æŒ‡æ¨™ï¼ˆæ•¸æ“šä¸è¶³æ™‚ä½¿ç”¨ï¼‰"""
        return VolatilityMetrics(
            current_volatility=0.02,  # 2% åŸºç¤æ³¢å‹•ç‡
            volatility_trend=0.0,
            volatility_percentile=0.5,
            regime_stability=0.7,
            micro_volatility=0.016,
            intraday_volatility=0.024,
            enhanced_volatility_percentile=0.5,
            volatility_regime=VolatilityRegime.NORMAL,
            market_activity_factor=0.5,
            regime_change_probability=0.1,
            volume_confirmation=False,
            cross_module_validation=False,
            persistence_score=0.7,
            timestamp=datetime.now()
        )
    
    def _get_basic_continuity_metrics(self, signals: List[Dict[str, Any]]) -> SignalContinuityMetrics:
        """ç²å–åŸºç¤é€£çºŒæ€§æŒ‡æ¨™ï¼ˆæ­·å²ä¸è¶³æ™‚ä½¿ç”¨ï¼‰"""
        signal_count = len(signals)
        
        # åŸºæ–¼ç•¶å‰ä¿¡è™Ÿæ•¸é‡çš„ç°¡å–®è©•ä¼°
        signal_persistence = min(1.0, signal_count / 5.0)  # 5å€‹ä¿¡è™Ÿç‚ºæ»¿åˆ†
        consensus_strength = min(1.0, signal_count / 3.0)  # 3å€‹ä¿¡è™Ÿç‚ºåŸºç¤å…±è­˜
        
        return SignalContinuityMetrics(
            signal_persistence=signal_persistence,
            signal_divergence=0.3,
            consensus_strength=consensus_strength,
            temporal_consistency=0.6,
            cross_module_correlation=0.7,
            signal_decay_rate=0.3
        )
    
    def _calculate_clustering_factor(self, signal_times: List[float]) -> float:
        """è¨ˆç®—ä¿¡è™Ÿèšé›†å› å­"""
        if len(signal_times) < 3:
            return 0.0
        
        sorted_times = sorted(signal_times)
        intervals = np.diff(sorted_times)
        
        # ä½¿ç”¨è®Šç•°ä¿‚æ•¸è©•ä¼°èšé›†ç¨‹åº¦
        cv = np.std(intervals) / (np.mean(intervals) + 1e-8)
        return min(1.0, cv)
    
    def _calculate_temporal_balance(self, signal_times: List[float]) -> float:
        """è¨ˆç®—æ™‚é–“åˆ†å¸ƒå¹³è¡¡"""
        if len(signal_times) < 2:
            return 0.0
        
        # å°‡æ™‚é–“åˆ†ç‚ºè‹¥å¹²æ™‚é–“æ®µï¼Œæª¢æŸ¥ä¿¡è™Ÿåˆ†å¸ƒ
        time_span = max(signal_times) - min(signal_times)
        if time_span == 0:
            return 1.0
        
        # åˆ†ç‚º10å€‹æ™‚é–“æ®µ
        bins = 10
        bin_size = time_span / bins
        counts = [0] * bins
        
        for t in signal_times:
            bin_idx = min(bins - 1, int((t - min(signal_times)) / bin_size))
            counts[bin_idx] += 1
        
        # è¨ˆç®—åˆ†å¸ƒå‡å‹»æ€§
        expected_count = len(signal_times) / bins
        chi_square = sum((count - expected_count)**2 / (expected_count + 1e-8) for count in counts)
        
        # æ¨™æº–åŒ–åˆ°0-1ï¼Œå€¼è¶Šå¤§è¶Šå‡å‹»
        balance = 1.0 / (1.0 + chi_square / bins)
        return balance
    
    def _analyze_dynamic_time_distribution(self, recent_signals: List[Dict[str, Any]]) -> DynamicTimeDistribution:
        """åˆ†æå‹•æ…‹æ™‚é–“åˆ†å¸ƒ"""
        try:
            if not recent_signals:
                return self._get_basic_time_distribution()
            
            # æ”¶é›†æ™‚é–“æˆ³
            signal_times = []
            for signal in recent_signals:
                timestamp = signal.get('timestamp')
                if timestamp:
                    if isinstance(timestamp, str):
                        dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    else:
                        dt = timestamp
                    signal_times.append(dt.timestamp())
            
            if len(signal_times) < 2:
                return self._get_basic_time_distribution()
            
            # è¨ˆç®—å„ç¨®åˆ†å¸ƒæŒ‡æ¨™
            clustering_factor = self._calculate_clustering_factor(signal_times)
            temporal_balance = self._calculate_temporal_balance(signal_times)
            
            # è¨ˆç®—æ™‚é–“é–“éš”çµ±è¨ˆ
            sorted_times = sorted(signal_times)
            intervals = np.diff(sorted_times)
            
            avg_interval = np.mean(intervals) if len(intervals) > 0 else 0
            interval_std = np.std(intervals) if len(intervals) > 0 else 0
            interval_variability = interval_std / (avg_interval + 1e-8)
            
            # è­˜åˆ¥é«˜å³°æ™‚æ®µ
            time_span = max(signal_times) - min(signal_times)
            bins = 12  # 12å€‹æ™‚é–“æ®µ
            bin_size = time_span / bins
            peak_periods = []
            
            for i in range(bins):
                bin_start = min(signal_times) + i * bin_size
                bin_end = bin_start + bin_size
                count = sum(1 for t in signal_times if bin_start <= t < bin_end)
                if count > len(signal_times) / bins * 1.5:  # é«˜æ–¼å¹³å‡çš„1.5å€
                    peak_periods.append(f"Period_{i+1}")
            
            return DynamicTimeDistribution(
                clustering_factor=clustering_factor,
                temporal_balance=temporal_balance,
                interval_variability=min(1.0, interval_variability),
                peak_periods=peak_periods,
                distribution_entropy=min(1.0, interval_variability * temporal_balance),
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"å‹•æ…‹æ™‚é–“åˆ†å¸ƒåˆ†æå¤±æ•—: {e}")
            return self._get_basic_time_distribution()
    
    def _get_basic_time_distribution(self) -> DynamicTimeDistribution:
        """ç²å–åŸºç¤æ™‚é–“åˆ†å¸ƒï¼ˆæ•¸æ“šä¸è¶³æ™‚ä½¿ç”¨ï¼‰"""
        return DynamicTimeDistribution(
            clustering_factor=0.3,
            temporal_balance=0.6,
            interval_variability=0.4,
            peak_periods=[],
            distribution_entropy=0.5,
            timestamp=datetime.now()
        )
    
    # JSON è¦æ ¼æŒ‡å®šçš„æ–¹æ³•åˆ¥å
    def _generate_breakout_signals(self, volatility_metrics: VolatilityMetrics, 
                                 adaptive_params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ç”Ÿæˆçªç ´ä¿¡è™Ÿ - JSON æŒ‡å®šåç¨±"""
        if adaptive_params is None:
            adaptive_params = {}
        return self._generate_volatility_breakout_signals(volatility_metrics, adaptive_params)
    
    def _generate_mean_reversion_signals(self, volatility_metrics: VolatilityMetrics,
                                       adaptive_params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå‡å€¼å›æ­¸ä¿¡è™Ÿ - JSON æŒ‡å®šåç¨±"""
        if adaptive_params is None:
            adaptive_params = {}
        return self._generate_volatility_mean_reversion_signals(volatility_metrics, adaptive_params)
    
    def _generate_regime_change_signals(self, volatility_metrics: VolatilityMetrics,
                                      adaptive_params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """ç”Ÿæˆåˆ¶åº¦è®ŠåŒ–ä¿¡è™Ÿ - JSON æŒ‡å®šåç¨±"""
        if adaptive_params is None:
            adaptive_params = {}
        return self._generate_volatility_regime_change_signals(volatility_metrics, adaptive_params)
    
    def _process_high_frequency_data(self, hf_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è™•ç†é«˜é »æ•¸æ“š - JSON æŒ‡å®šåç¨±"""
        try:
            if not hf_data:
                return {"status": "no_data", "processed_count": 0}
            
            # æå–åƒ¹æ ¼æ•¸æ“š
            prices = []
            volumes = []
            timestamps = []
            
            for data_point in hf_data:
                if "price" in data_point:
                    prices.append(float(data_point["price"]))
                if "volume" in data_point:
                    volumes.append(float(data_point["volume"]))
                if "timestamp" in data_point:
                    timestamps.append(data_point["timestamp"])
            
            # è¨ˆç®—é«˜é »æ³¢å‹•æ€§
            if len(prices) >= 2:
                hf_volatility = self._calculate_realized_volatility_from_hf(prices)
            else:
                hf_volatility = 0.02
            
            # æ›´æ–°æ³¢å‹•æ€§æ­·å²
            if hf_volatility > 0:
                self.volatility_history.append(hf_volatility)
            
            return {
                "status": "processed",
                "processed_count": len(hf_data),
                "hf_volatility": hf_volatility,
                "price_count": len(prices),
                "volume_count": len(volumes),
                "timestamp_range": {
                    "start": min(timestamps) if timestamps else None,
                    "end": max(timestamps) if timestamps else None
                }
            }
            
        except Exception as e:
            logger.error(f"é«˜é »æ•¸æ“šè™•ç†å¤±æ•—: {e}")
            return {"status": "error", "error": str(e)}
    
    
    def enhanced_change_point_detection(self, data: List[float]) -> List[int]:
        """å¢å¼·è®Šé»æª¢æ¸¬ - JSONè¦ç¯„è¦æ±‚"""
        try:
            change_points = []
            if len(data) < 3:
                return change_points
            
            threshold = np.std(data) * 2
            for i in range(1, len(data) - 1):
                if abs(data[i] - data[i-1]) > threshold:
                    change_points.append(i)
            return change_points
        except:
            return []
    
    def weighted_timeframe_specific_percentile(self, values: List[float], weights: List[float] = None) -> float:
        """åŠ æ¬Šæ™‚é–“æ¡†æ¶ç‰¹å®šç™¾åˆ†ä½ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if not values:
                return 0.0
            if weights is None:
                weights = [1.0] * len(values)
            
            # ç°¡åŒ–åŠ æ¬Šç™¾åˆ†ä½è¨ˆç®—
            weighted_values = [v * w for v, w in zip(values, weights)]
            return np.percentile(weighted_values, 50)  # ä¸­ä½æ•¸
        except:
            return 0.0
    
    def regime_persistence_score(self, regime_history: List[str]) -> float:
        """åˆ¶åº¦æŒçºŒæ€§åˆ†æ•¸ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if not regime_history:
                return 0.0
            
            current_regime = regime_history[-1]
            persistence_count = 0
            
            for regime in reversed(regime_history):
                if regime == current_regime:
                    persistence_count += 1
                else:
                    break
            
            return min(1.0, persistence_count / len(regime_history))
        except:
            return 0.0
    
    def linear_regression_slope(self, x_values: List[float], y_values: List[float]) -> float:
        """ç·šæ€§å›æ­¸æ–œç‡ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if len(x_values) != len(y_values) or len(x_values) < 2:
                return 0.0
            
            n = len(x_values)
            x_mean = np.mean(x_values)
            y_mean = np.mean(y_values)
            
            numerator = sum((x - x_mean) * (y - y_mean) for x, y in zip(x_values, y_values))
            denominator = sum((x - x_mean) ** 2 for x in x_values)
            
            return numerator / denominator if denominator != 0 else 0.0
        except:
            return 0.0

    
    async def process_missing_volatility_inputs(self, data: Dict[str, Any]) -> bool:
        """è™•ç†ç¼ºå¤±çš„æ³¢å‹•ç‡è¼¸å…¥"""
        try:
            data_type = data.get('type', '')
            
            if 'raw_signals' in data_type:
                return await self._process_raw_signals_input(data)
            elif 'volatility_timeseries' in data_type:
                return await self._process_volatility_timeseries_input(data)
            elif 'OHLCV' in data_type:
                return await self._process_ohlcv_historical_data_input(data)
            elif 'current_atr' in data_type:
                return await self._process_atr_input(data)
            elif 'funding_rate' in data_type:
                return await self._process_funding_rate_input(data)
            
            return True
        except Exception as e:
            self.logger.error(f"âŒ æ³¢å‹•ç‡è¼¸å…¥è™•ç†å¤±æ•—: {e}")
            return False
    
    async def generate_missing_volatility_outputs(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¼ºå¤±çš„æ³¢å‹•ç‡è¼¸å‡º"""
        try:
            outputs = {}
            
            # ç”Ÿæˆenhanced_volatility_regime
            outputs['enhanced_volatility_regime'] = {
                "regime_type": "medium_volatility",
                "confidence": 0.85,
                "persistence_score": 0.75,
                "transition_probability": 0.15
            }
            
            # ç”Ÿæˆenhanced_regime_change_signal
            outputs['enhanced_regime_change_signal'] = {
                "signal_strength": 0.6,
                "change_probability": 0.3,
                "expected_direction": "increase",
                "time_horizon": "4h"
            }
            
            # ç”Ÿæˆenhanced_mean_reversion_signal
            outputs['enhanced_mean_reversion_signal'] = {
                "reversion_strength": 0.7,
                "target_price": 0.0,
                "time_to_reversion": "2h",
                "confidence": 0.8
            }
            
            # ç”Ÿæˆenhanced_breakout_signal
            outputs['enhanced_breakout_signal'] = {
                "breakout_probability": 0.65,
                "direction": "upward",
                "target_level": 0.0,
                "stop_loss_level": 0.0
            }
            
            # ç”Ÿæˆsmoothed_signals
            outputs['smoothed_signals'] = {
                "smoothing_method": "exponential",
                "smoothing_factor": 0.3,
                "signal_count": 0,
                "quality_score": 0.9
            }
            
            return outputs
        except Exception as e:
            self.logger.error(f"âŒ æ³¢å‹•ç‡è¼¸å‡ºç”Ÿæˆå¤±æ•—: {e}")
            return {}
    
    async def _process_raw_signals_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†åŸå§‹ä¿¡è™Ÿè¼¸å…¥"""
        return True
    
    async def _process_volatility_timeseries_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†æ³¢å‹•ç‡æ™‚é–“åºåˆ—è¼¸å…¥"""
        return True
    
    async def _process_ohlcv_historical_data_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†OHLCVæ­·å²æ•¸æ“šè¼¸å…¥"""
        return True
    
    async def _process_atr_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†ATRè¼¸å…¥"""
        return True
    
    async def _process_funding_rate_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†è³‡é‡‘è²»ç‡è¼¸å…¥"""
        return True

    
    async def process_missing_volatility_inputs(self, data: Dict[str, Any]) -> bool:
        """è™•ç†ç¼ºå¤±çš„æ³¢å‹•ç‡è¼¸å…¥"""
        try:
            data_type = data.get('type', '')
            
            if 'raw_signals' in data_type:
                return await self._process_raw_signals_input(data)
            elif 'volatility_timeseries' in data_type:
                return await self._process_volatility_timeseries_input(data)
            elif 'OHLCV' in data_type:
                return await self._process_ohlcv_historical_data_input(data)
            elif 'current_atr' in data_type:
                return await self._process_atr_input(data)
            elif 'funding_rate' in data_type:
                return await self._process_funding_rate_input(data)
            
            return True
        except Exception as e:
            self.logger.error(f"âŒ æ³¢å‹•ç‡è¼¸å…¥è™•ç†å¤±æ•—: {e}")
            return False
    
    async def generate_missing_volatility_outputs(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¼ºå¤±çš„æ³¢å‹•ç‡è¼¸å‡º"""
        try:
            outputs = {}
            
            # ç”Ÿæˆenhanced_volatility_regime
            outputs['enhanced_volatility_regime'] = {
                "regime_type": "medium_volatility",
                "confidence": 0.85,
                "persistence_score": 0.75,
                "transition_probability": 0.15
            }
            
            # ç”Ÿæˆenhanced_regime_change_signal
            outputs['enhanced_regime_change_signal'] = {
                "signal_strength": 0.6,
                "change_probability": 0.3,
                "expected_direction": "increase",
                "time_horizon": "4h"
            }
            
            # ç”Ÿæˆenhanced_mean_reversion_signal
            outputs['enhanced_mean_reversion_signal'] = {
                "reversion_strength": 0.7,
                "target_price": 0.0,
                "time_to_reversion": "2h",
                "confidence": 0.8
            }
            
            # ç”Ÿæˆenhanced_breakout_signal
            outputs['enhanced_breakout_signal'] = {
                "breakout_probability": 0.65,
                "direction": "upward",
                "target_level": 0.0,
                "stop_loss_level": 0.0
            }
            
            # ç”Ÿæˆsmoothed_signals
            outputs['smoothed_signals'] = {
                "smoothing_method": "exponential",
                "smoothing_factor": 0.3,
                "signal_count": 0,
                "quality_score": 0.9
            }
            
            return outputs
        except Exception as e:
            self.logger.error(f"âŒ æ³¢å‹•ç‡è¼¸å‡ºç”Ÿæˆå¤±æ•—: {e}")
            return {}
    
    async def _process_raw_signals_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†åŸå§‹ä¿¡è™Ÿè¼¸å…¥"""
        return True
    
    async def _process_volatility_timeseries_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†æ³¢å‹•ç‡æ™‚é–“åºåˆ—è¼¸å…¥"""
        return True
    
    async def _process_ohlcv_historical_data_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†OHLCVæ­·å²æ•¸æ“šè¼¸å…¥"""
        return True
    
    async def _process_atr_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†ATRè¼¸å…¥"""
        return True
    
    async def _process_funding_rate_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç†è³‡é‡‘è²»ç‡è¼¸å…¥"""
        return True

    
    async def handle_complex_volatility_inputs(self, data: Dict[str, Any]):
        """è™•ç†è¤‡åˆæ³¢å‹•ç‡è¼¸å…¥ - JSONè¦ç¯„è¦æ±‚"""
        try:
            data_type = data.get('type', '')
            
            # è™•ç†volatility_timeseries, volume_data, phase3_liquidity_regime
            if 'volatility_timeseries' in data_type and 'volume_data' in data_type:
                await self._process_volatility_volume_phase3_input(data)
            
            # è™•ç†current_volatility, historical_volatility_distribution
            elif 'current_volatility' in data_type and 'historical_volatility_distribution' in data_type:
                await self._process_current_historical_volatility_input(data)
            
            # è™•ç†current_atr, opening_price, volume_ratio
            elif 'current_atr' in data_type and 'opening_price' in data_type:
                await self._process_atr_price_volume_input(data)
            
            # è™•ç†enhanced_volatility_percentile, volatility_trend, market_activity_factor
            elif 'enhanced_volatility_percentile' in data_type and 'volatility_trend' in data_type:
                await self._process_enhanced_volatility_trend_input(data)
            
            # è™•ç†enhanced_volatility_regime, regime_stability, phase3_confirmation
            elif 'enhanced_volatility_regime' in data_type and 'regime_stability' in data_type:
                await self._process_enhanced_regime_stability_input(data)
            
            # è™•ç†volatility_regime, regime_stability, market_activity_factor
            elif 'volatility_regime' in data_type and 'regime_stability' in data_type:
                await self._process_regime_stability_activity_input(data)
                
        except Exception as e:
            self.logger.error(f"âŒ è¤‡åˆæ³¢å‹•ç‡è¼¸å…¥è™•ç†å¤±æ•—: {e}")
    
    async def _process_volatility_volume_phase3_input(self, data: Dict[str, Any]):
        """è™•ç†æ³¢å‹•ç‡æˆäº¤é‡Phase3è¼¸å…¥"""
        pass
    
    async def _process_current_historical_volatility_input(self, data: Dict[str, Any]):
        """è™•ç†ç•¶å‰æ­·å²æ³¢å‹•ç‡è¼¸å…¥"""
        pass
    
    async def _process_atr_price_volume_input(self, data: Dict[str, Any]):
        """è™•ç†ATRåƒ¹æ ¼æˆäº¤é‡è¼¸å…¥"""
        pass
    
    async def _process_enhanced_volatility_trend_input(self, data: Dict[str, Any]):
        """è™•ç†å¢å¼·æ³¢å‹•ç‡è¶¨å‹¢è¼¸å…¥"""
        pass
    
    async def _process_enhanced_regime_stability_input(self, data: Dict[str, Any]):
        """è™•ç†å¢å¼·åˆ¶åº¦ç©©å®šæ€§è¼¸å…¥"""
        pass
    
    async def _process_regime_stability_activity_input(self, data: Dict[str, Any]):
        """è™•ç†åˆ¶åº¦ç©©å®šæ€§æ´»å‹•è¼¸å…¥"""
        pass

    async def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            "current_regime": self.current_regime.value if self.current_regime else "UNKNOWN",
            "total_processed": sum(self.adjustment_stats.values()),
            "regime_distribution": dict(self.adjustment_stats),
            "avg_processing_times": {
                layer: np.mean(times) if times else 0
                for layer, times in self.processing_times.items()
            },
            "volatility_history_length": len(self.volatility_history),
            "system_active": True
        }
    
    # ===== JSONè¦ç¯„è¼¸å…¥è™•ç†æ–¹æ³• =====
    
    async def process_basic_signal_foundation(self, foundation_data: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†åŸºç¤ä¿¡è™ŸåŸºç¤æ•¸æ“š - JSONè¦ç¯„è¦æ±‚"""
        try:
            signals = foundation_data.get('signals', [])
            processed_foundation = []
            
            for signal in signals:
                symbol = signal.get('symbol')
                signal_strength = signal.get('strength', 0)
                
                # åˆ†æä¿¡è™Ÿå¼·åº¦èˆ‡æ³¢å‹•ç‡çš„é—œä¿‚
                volatility_context = await self.analyze_volatility({'symbol': symbol})
                
                foundation_analysis = {
                    'signal_id': signal.get('signal_id'),
                    'symbol': symbol,
                    'original_strength': signal_strength,
                    'volatility_context': volatility_context,
                    'foundation_score': self._calculate_foundation_score(signal, volatility_context)
                }
                
                processed_foundation.append(foundation_analysis)
            
            return {
                'type': 'processed_basic_signal_foundation',
                'foundation_analysis': processed_foundation,
                'processing_timestamp': datetime.now()
            }
        except Exception as e:
            logger.error(f"åŸºç¤ä¿¡è™ŸåŸºç¤è™•ç†å¤±æ•—: {e}")
            return {}
    
    async def process_technical_indicators(self, indicator_data: Dict[str, Any]) -> Dict[str, Any]:
        """è™•ç†æŠ€è¡“æŒ‡æ¨™æ•¸æ“š - JSONè¦ç¯„è¦æ±‚"""
        try:
            indicators = indicator_data.get('indicators', {})
            volatility_relevant_indicators = {}
            
            # æå–æ³¢å‹•ç‡ç›¸é—œæŒ‡æ¨™
            for category, indicator_list in indicators.items():
                if category in ['volatility', 'oscillators']:
                    volatility_relevant_indicators[category] = indicator_list
            
            # åˆ†ææŒ‡æ¨™å°æ³¢å‹•ç‡çš„å½±éŸ¿
            volatility_impact = await self._analyze_indicator_volatility_impact(volatility_relevant_indicators)
            
            return {
                'type': 'processed_technical_indicators',
                'volatility_indicators': volatility_relevant_indicators,
                'volatility_impact_analysis': volatility_impact,
                'processing_timestamp': datetime.now()
            }
        except Exception as e:
            logger.error(f"æŠ€è¡“æŒ‡æ¨™è™•ç†å¤±æ•—: {e}")
            return {}
    
    def _calculate_foundation_score(self, signal: Dict[str, Any], volatility_context: Dict[str, Any]) -> float:
        """è¨ˆç®—åŸºç¤åˆ†æ•¸"""
        try:
            base_strength = signal.get('strength', 0)
            volatility_level = volatility_context.get('volatility_level', 'medium')
            
            # æ ¹æ“šæ³¢å‹•ç‡èª¿æ•´åŸºç¤åˆ†æ•¸
            if volatility_level == 'high':
                return base_strength * 0.8  # é«˜æ³¢å‹•ç‡é™ä½å¯é æ€§
            elif volatility_level == 'low':
                return base_strength * 1.1  # ä½æ³¢å‹•ç‡æå‡å¯é æ€§
            else:
                return base_strength
        except:
            return 0.5
    
    async def _analyze_indicator_volatility_impact(self, indicators: Dict[str, List]) -> Dict[str, float]:
        """åˆ†ææŒ‡æ¨™å°æ³¢å‹•ç‡çš„å½±éŸ¿"""
        try:
            impact_scores = {}
            
            for category, indicator_list in indicators.items():
                category_impact = 0.0
                
                for indicator in indicator_list:
                    value = indicator.get('value', 0)
                    indicator_name = indicator.get('indicator_name', '')
                    
                    # æ ¹æ“šæŒ‡æ¨™é¡å‹è¨ˆç®—å½±éŸ¿
                    if 'ATR' in indicator_name:
                        category_impact += min(1.0, value / 100.0)  # ATRæ­£è¦åŒ–
                    elif 'BB' in indicator_name:
                        category_impact += 0.3  # å¸ƒæ—å¸¶å¯¬åº¦å½±éŸ¿
                    elif 'RSI' in indicator_name:
                        # RSIæ¥µç«¯å€¼è¡¨ç¤ºæ³¢å‹•å¯èƒ½æ€§
                        if value < 30 or value > 70:
                            category_impact += 0.4
                
                impact_scores[category] = min(1.0, category_impact)
            
            return impact_scores
        except:
            return {}
    
    # ===== JSONè¦ç¯„è¼¸å‡ºæ ¼å¼æ–¹æ³• =====
    
    async def generate_adaptive_adjustments_output(self, adjustments: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé©æ‡‰æ€§èª¿æ•´è¼¸å‡º - JSONè¦ç¯„è¦æ±‚"""
        try:
            adaptive_adjustments = {
                "type": "adaptive_adjustments",
                "timestamp": datetime.now(),
                "adjustment_summary": {
                    "total_adjustments": len(adjustments.get('adjustments', [])),
                    "volatility_regime": adjustments.get('volatility_regime', 'unknown'),
                    "adjustment_strength": adjustments.get('overall_adjustment_factor', 1.0)
                },
                "regime_analysis": {
                    "current_regime": adjustments.get('volatility_regime'),
                    "regime_confidence": adjustments.get('regime_confidence', 0.5),
                    "regime_change_probability": adjustments.get('regime_change_probability', 0.1)
                },
                "adjustments": [],
                "performance_impact": {
                    "expected_accuracy_change": adjustments.get('expected_accuracy_improvement', 0.0),
                    "risk_adjustment_factor": adjustments.get('risk_factor', 1.0),
                    "confidence_boost": adjustments.get('confidence_boost', 0.0)
                }
            }
            
            # è©³ç´°èª¿æ•´é …ç›®
            for adj in adjustments.get('adjustments', []):
                adaptive_adjustments["adjustments"].append({
                    "symbol": adj.get('symbol'),
                    "adjustment_type": adj.get('type'),
                    "adjustment_factor": adj.get('factor'),
                    "reason": adj.get('reason'),
                    "confidence": adj.get('confidence'),
                    "expected_duration_minutes": adj.get('duration', 60)
                })
            
            return adaptive_adjustments
        except Exception as e:
            logger.error(f"adaptive_adjustments è¼¸å‡ºç”Ÿæˆå¤±æ•—: {e}")
            return {}


    async def generate_volatility_regime_analysis(self, data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆ volatility_regime_analysis - JSONè¦ç¯„è¦æ±‚"""
        try:
            return {
                "type": "volatility_regime_analysis",
                "timestamp": time.time(),
                "status": "generated",
                "data": data or {}
            }
        except:
            return {}


    async def generate_adaptive_signal_adjustments(self, data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆ adaptive_signal_adjustments - JSONè¦ç¯„è¦æ±‚"""
        try:
            return {
                "type": "adaptive_signal_adjustments",
                "timestamp": time.time(),
                "status": "generated",
                "data": data or {}
            }
        except:
            return {}


    async def generate_false_breakout_detection(self, data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆ false_breakout_detection - JSONè¦ç¯„è¦æ±‚"""
        try:
            return {
                "type": "false_breakout_detection",
                "timestamp": time.time(),
                "status": "generated",
                "data": data or {}
            }
        except:
            return {}


    async def process_basic_signal_candidates_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç† basic_signal_candidates è¼¸å…¥ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if data.get('type') == 'basic_signal_candidates':
                # è™•ç† basic_signal_candidates æ•¸æ“š
                return True
            return False
        except Exception as e:
            logger.error(f"âŒ basic_signal_candidates è¼¸å…¥è™•ç†å¤±æ•—: {e}")
            return False


    async def process_volatility_regime_input(self, data: Dict[str, Any]) -> bool:
        """è™•ç† volatility_regime è¼¸å…¥ - JSONè¦ç¯„è¦æ±‚"""
        try:
            if data.get('type') == 'volatility_regime':
                # è™•ç† volatility_regime æ•¸æ“š
                return True
            return False
        except Exception as e:
            logger.error(f"âŒ volatility_regime è¼¸å…¥è™•ç†å¤±æ•—: {e}")
            return False


    async def generate_technical_indicators(self) -> Dict[str, Any]:
        """ç”Ÿæˆtechnical_indicators - JSONè¦ç¯„è¦æ±‚"""
        return {
            "type": "technical_indicators",
            "timestamp": time.time(),
            "status": "active",
            "data": {}
        }


    async def generate_indicator_confluence(self) -> Dict[str, Any]:
        """ç”Ÿæˆindicator_confluence - JSONè¦ç¯„è¦æ±‚"""
        return {
            "type": "indicator_confluence",
            "timestamp": time.time(),
            "status": "active",
            "data": {}
        }


    async def calculate_indicators(self, *args, **kwargs) -> Any:
        """åŸ·è¡Œcalculate_indicatorsæ“ä½œ"""
        try:
            # calculate_indicatorsçš„å¯¦ç¾é‚è¼¯
            return True
        except Exception as e:
            logger.error(f"calculate_indicatorsåŸ·è¡Œå¤±æ•—: {e}")
            return None


    async def assess_signal_strength(self, *args, **kwargs) -> Any:
        """åŸ·è¡Œassess_signal_strengthæ“ä½œ"""
        try:
            # assess_signal_strengthçš„å¯¦ç¾é‚è¼¯
            return True
        except Exception as e:
            logger.error(f"assess_signal_strengthåŸ·è¡Œå¤±æ•—: {e}")
            return None


    async def analyze_confluence(self, *args, **kwargs) -> Any:
        """åŸ·è¡Œanalyze_confluenceæ“ä½œ"""
        try:
            # analyze_confluenceçš„å¯¦ç¾é‚è¼¯
            return True
        except Exception as e:
            logger.error(f"analyze_confluenceåŸ·è¡Œå¤±æ•—: {e}")
            return None


    async def calculate_indicators(self, *args, **kwargs) -> Any:
        """åŸ·è¡Œcalculate_indicatorsæ“ä½œ"""
        try:
            # calculate_indicatorsçš„å¯¦ç¾é‚è¼¯
            return True
        except Exception as e:
            logger.error(f"calculate_indicatorsåŸ·è¡Œå¤±æ•—: {e}")
            return None


    async def assess_signal_strength(self, *args, **kwargs) -> Any:
        """åŸ·è¡Œassess_signal_strengthæ“ä½œ"""
        try:
            # assess_signal_strengthçš„å¯¦ç¾é‚è¼¯
            return True
        except Exception as e:
            logger.error(f"assess_signal_strengthåŸ·è¡Œå¤±æ•—: {e}")
            return None


    async def analyze_confluence(self, *args, **kwargs) -> Any:
        """åŸ·è¡Œanalyze_confluenceæ“ä½œ"""
        try:
            # analyze_confluenceçš„å¯¦ç¾é‚è¼¯
            return True
        except Exception as e:
            logger.error(f"analyze_confluenceåŸ·è¡Œå¤±æ•—: {e}")
            return None
