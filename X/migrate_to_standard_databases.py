#!/usr/bin/env python3
"""
é·ç§»è…³æœ¬ï¼šå°‡ç¾æœ‰æ•¸æ“šé·ç§»åˆ°æ¨™æº–ä¸‰åˆ†é¡è³‡æ–™åº«æ¶æ§‹
"""

import asyncio
import json
import os
import sys
from pathlib import Path
from datetime import datetime
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def migrate_liquidity_events():
    """é·ç§»æµå‹•æ€§äº‹ä»¶åˆ° extreme_events.db"""
    
    try:
        # å°å…¥æ¥µç«¯äº‹ä»¶è³‡æ–™åº«ç®¡ç†å™¨
        from app.core.database_separated import SeparatedDatabaseManager
        from app.models.extreme_models import LiquidityEvent
        
        # åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨
        db_manager = SeparatedDatabaseManager()
        
        # è®€å–ç¾æœ‰çš„æµå‹•æ€§äº‹ä»¶ JSON æ–‡ä»¶
        liquidity_json_path = "./data/liquidity_events.json"
        if not os.path.exists(liquidity_json_path):
            logger.info("ğŸ“‚ æµå‹•æ€§äº‹ä»¶ JSON æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³éé·ç§»")
            return
            
        with open(liquidity_json_path, 'r', encoding='utf-8') as f:
            liquidity_data = json.load(f)
        
        # å‰µå»ºæ¥µç«¯äº‹ä»¶è³‡æ–™åº«æœƒè©±
        async with db_manager.get_db_session("extreme_events") as session:
            
            migrated_count = 0
            
            # é·ç§»æ¯å€‹æµå‹•æ€§äº‹ä»¶
            for event_data in liquidity_data.get("events", []):
                try:
                    # å‰µå»º LiquidityEvent å°è±¡
                    liquidity_event = LiquidityEvent(
                        event_id=event_data.get("event_id", f"liquidity_{migrated_count}"),
                        symbol=event_data.get("symbol", "UNKNOWN"),
                        event_type=event_data.get("event_type", "LIQUIDITY_DROP"),
                        severity=event_data.get("severity", "MEDIUM"),
                        timestamp=datetime.fromisoformat(event_data.get("timestamp", datetime.now().isoformat())),
                        
                        # æµå‹•æ€§å…·é«”æ•¸æ“š
                        before_liquidity=float(event_data.get("before_liquidity", 0)),
                        after_liquidity=float(event_data.get("after_liquidity", 0)),
                        liquidity_change_pct=float(event_data.get("liquidity_change_pct", 0)),
                        bid_ask_spread_before=float(event_data.get("bid_ask_spread_before", 0)),
                        bid_ask_spread_after=float(event_data.get("bid_ask_spread_after", 0)),
                        
                        # å¸‚å ´å½±éŸ¿
                        market_impact=event_data.get("market_impact", {}),
                        
                        # ç³»çµ±éŸ¿æ‡‰
                        response_actions=event_data.get("response_actions", []),
                        
                        # ç‹€æ…‹
                        status="PROCESSED",
                        resolution_time=datetime.now() if event_data.get("resolved", False) else None,
                        
                        # å…ƒæ•¸æ“š
                        metadata=event_data.get("metadata", {}),
                        notes=f"å¾ liquidity_events.json é·ç§»æ–¼ {datetime.now().isoformat()}"
                    )
                    
                    session.add(liquidity_event)
                    migrated_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ é·ç§»æµå‹•æ€§äº‹ä»¶å¤±æ•—: {e}")
                    continue
            
            # æäº¤äº‹å‹™
            await session.commit()
            logger.info(f"âœ… æˆåŠŸé·ç§» {migrated_count} å€‹æµå‹•æ€§äº‹ä»¶åˆ° extreme_events.db")
            
            # å‚™ä»½åŸå§‹ JSON æ–‡ä»¶
            backup_path = f"./data/liquidity_events_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.rename(liquidity_json_path, backup_path)
            logger.info(f"ğŸ“¦ åŸå§‹æ–‡ä»¶å·²å‚™ä»½ç‚º: {backup_path}")
            
    except Exception as e:
        logger.error(f"âŒ æµå‹•æ€§äº‹ä»¶é·ç§»å¤±æ•—: {e}")

async def validate_database_integration():
    """é©—è­‰ä¸‰è³‡æ–™åº«æ•´åˆæ˜¯å¦æˆåŠŸ"""
    
    try:
        from app.core.database_separated import SeparatedDatabaseManager
        
        db_manager = SeparatedDatabaseManager()
        
        print("ğŸ” é©—è­‰æ¨™æº–ä¸‰åˆ†é¡è³‡æ–™åº«æ¶æ§‹æ•´åˆ...")
        print("="*60)
        
        # æª¢æŸ¥è³‡æ–™åº«æ–‡ä»¶
        for db_name, db_path in db_manager.databases.items():
            exists = db_path.exists()
            size = db_path.stat().st_size if exists else 0
            size_kb = size / 1024
            
            print(f"ğŸ“Š {db_name}:")
            print(f"   è·¯å¾‘: {db_path}")
            print(f"   å­˜åœ¨: {'âœ…' if exists else 'âŒ'}")
            print(f"   å¤§å°: {size_kb:.1f} KB")
            print()
        
        # æ¸¬è©¦é€£æ¥
        try:
            for db_name in ["market_data", "learning_records", "extreme_events"]:
                engine = db_manager.get_engine(db_name)
                if engine:
                    print(f"âœ… {db_name} å¼•æ“é€£æ¥æ­£å¸¸")
                else:
                    print(f"âŒ {db_name} å¼•æ“é€£æ¥å¤±æ•—")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
        
        print("\nğŸ‰ æ¨™æº–ä¸‰åˆ†é¡è³‡æ–™åº«æ¶æ§‹æ•´åˆé©—è­‰å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«æ•´åˆé©—è­‰å¤±æ•—: {e}")

async def main():
    """ä¸»è¦é·ç§»æµç¨‹"""
    
    print("ğŸš€ é–‹å§‹é·ç§»åˆ°æ¨™æº–ä¸‰åˆ†é¡è³‡æ–™åº«æ¶æ§‹...")
    print("="*60)
    
    # 1. é·ç§»æµå‹•æ€§äº‹ä»¶
    print("ğŸ“Š æ­¥é©Ÿ 1: é·ç§»æµå‹•æ€§äº‹ä»¶...")
    await migrate_liquidity_events()
    
    # 2. é©—è­‰æ•´åˆ
    print("\nğŸ” æ­¥é©Ÿ 2: é©—è­‰è³‡æ–™åº«æ•´åˆ...")
    await validate_database_integration()
    
    print("\nâœ… é·ç§»å®Œæˆï¼ç¾åœ¨æ‰€æœ‰çµ„ä»¶éƒ½ä½¿ç”¨æ¨™æº–ä¸‰åˆ†é¡è³‡æ–™åº«æ¶æ§‹ï¼š")
    print("   ğŸ“Š market_data.db - Phase1A ä¿¡è™Ÿç”Ÿæˆ")
    print("   ğŸ“ learning_records.db - Phase2 å­¸ç¿’è¨˜éŒ„")
    print("   ğŸ›¡ï¸ extreme_events.db - ç³»çµ±ä¿è­·èˆ‡æ¥µç«¯äº‹ä»¶")

if __name__ == "__main__":
    asyncio.run(main())
