#!/usr/bin/env python3
"""
ğŸš€ Trading X ç”Ÿç”¢ç´šç³»çµ±å„ªåŒ–å™¨
å°ˆç‚ºç”Ÿç”¢ç’°å¢ƒè¨­è¨ˆçš„æ€§èƒ½å„ªåŒ–èˆ‡éŒ¯èª¤ä¿®æ­£å·¥å…·

åŠŸèƒ½ï¼š
1. å…§å­˜ä½¿ç”¨å„ªåŒ–
2. éŒ¯èª¤æª¢æ¸¬èˆ‡è‡ªå‹•ä¿®æ­£  
3. æ•¸æ“šæµå®Œæ•´æ€§é©—è­‰
4. å¯¦æ™‚æ€§èƒ½ç›£æ§
5. ç”Ÿç”¢ç´šé…ç½®é©—è­‰
"""

import asyncio
import psutil
import gc
import logging
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(Path(__file__).parent / 'logs' / 'production_optimizer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ProductionSystemOptimizer:
    """ç”Ÿç”¢ç´šç³»çµ±å„ªåŒ–å™¨"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent
        self.start_time = datetime.now()
        
        # æ€§èƒ½é–¾å€¼
        self.performance_thresholds = {
            'max_memory_usage': 75.0,  # 75% è¨˜æ†¶é«”ä½¿ç”¨ç‡
            'max_cpu_usage': 80.0,     # 80% CPU ä½¿ç”¨ç‡
            'max_response_time': 2.0,  # 2ç§’å›æ‡‰æ™‚é–“
            'min_success_rate': 95.0   # 95% æˆåŠŸç‡
        }
        
        # ç›£æ§çµ±è¨ˆ
        self.monitoring_stats = {
            'optimization_cycles': 0,
            'memory_optimizations': 0,
            'error_fixes': 0,
            'performance_improvements': 0
        }
        
        # éŒ¯èª¤ä¿®æ­£è¨˜éŒ„
        self.error_fixes = []
        
    async def run_optimization_cycle(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„å„ªåŒ–é€±æœŸ"""
        logger.info("ğŸš€ é–‹å§‹ç”Ÿç”¢ç´šç³»çµ±å„ªåŒ–...")
        
        optimization_results = {
            'timestamp': datetime.now().isoformat(),
            'cycle_number': self.monitoring_stats['optimization_cycles'] + 1,
            'results': {}
        }
        
        try:
            # 1. ç³»çµ±æ€§èƒ½æª¢æŸ¥
            performance_check = await self._check_system_performance()
            optimization_results['results']['performance'] = performance_check
            
            # 2. è¨˜æ†¶é«”å„ªåŒ–
            memory_optimization = await self._optimize_memory_usage()
            optimization_results['results']['memory'] = memory_optimization
            
            # 3. éŒ¯èª¤æª¢æ¸¬èˆ‡ä¿®æ­£
            error_fixes = await self._detect_and_fix_errors()
            optimization_results['results']['errors'] = error_fixes
            
            # 4. æ•¸æ“šæµé©—è­‰
            dataflow_validation = await self._validate_dataflow_integrity()
            optimization_results['results']['dataflow'] = dataflow_validation
            
            # 5. é…ç½®å„ªåŒ–
            config_optimization = await self._optimize_production_config()
            optimization_results['results']['config'] = config_optimization
            
            # æ›´æ–°çµ±è¨ˆ
            self.monitoring_stats['optimization_cycles'] += 1
            
            # ç”Ÿæˆå„ªåŒ–å ±å‘Š
            report = await self._generate_optimization_report(optimization_results)
            optimization_results['report'] = report
            
            logger.info("âœ… ç³»çµ±å„ªåŒ–é€±æœŸå®Œæˆ")
            return optimization_results
            
        except Exception as e:
            logger.error(f"âŒ å„ªåŒ–é€±æœŸå¤±æ•—: {e}")
            optimization_results['error'] = str(e)
            return optimization_results
    
    async def _check_system_performance(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç³»çµ±æ€§èƒ½"""
        logger.info("ğŸ“Š æª¢æŸ¥ç³»çµ±æ€§èƒ½...")
        
        # ç²å–ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        performance_data = {
            'cpu_usage': cpu_percent,
            'memory_usage': memory.percent,
            'disk_usage': disk.percent,
            'available_memory_gb': memory.available / (1024**3),
            'total_memory_gb': memory.total / (1024**3),
            'timestamp': datetime.now().isoformat()
        }
        
        # æ€§èƒ½è­¦å‘Šæª¢æŸ¥
        warnings = []
        if cpu_percent > self.performance_thresholds['max_cpu_usage']:
            warnings.append(f"CPUä½¿ç”¨ç‡éé«˜: {cpu_percent:.1f}%")
        
        if memory.percent > self.performance_thresholds['max_memory_usage']:
            warnings.append(f"è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: {memory.percent:.1f}%")
            
        if disk.percent > 90:
            warnings.append(f"ç£ç¢Ÿä½¿ç”¨ç‡éé«˜: {disk.percent:.1f}%")
        
        performance_data['warnings'] = warnings
        performance_data['status'] = 'healthy' if not warnings else 'warning'
        
        logger.info(f"ğŸ–¥ï¸ CPU: {cpu_percent:.1f}% | ğŸ’¾ è¨˜æ†¶é«”: {memory.percent:.1f}%")
        
        return performance_data
    
    async def _optimize_memory_usage(self) -> Dict[str, Any]:
        """å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨"""
        logger.info("ğŸ§¹ å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨...")
        
        # è¨˜éŒ„å„ªåŒ–å‰çš„è¨˜æ†¶é«”ä½¿ç”¨
        before_memory = psutil.virtual_memory()
        
        optimization_actions = []
        
        # å¼·åˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        optimization_actions.append("åŸ·è¡Œåƒåœ¾å›æ”¶")
        
        # æ¸…ç†å¤§å‹DataFrameç·©å­˜
        if hasattr(pd, '_libs'):
            optimization_actions.append("æ¸…ç†pandasç·©å­˜")
        
        # æ¸…ç†numpyç·©å­˜
        if hasattr(np, 'core'):
            optimization_actions.append("æ¸…ç†numpyç·©å­˜")
        
        # è¨˜éŒ„å„ªåŒ–å¾Œçš„è¨˜æ†¶é«”ä½¿ç”¨
        after_memory = psutil.virtual_memory()
        
        memory_saved = before_memory.used - after_memory.used
        memory_saved_mb = memory_saved / (1024**2)
        
        if memory_saved > 0:
            self.monitoring_stats['memory_optimizations'] += 1
            optimization_actions.append(f"ç¯€çœè¨˜æ†¶é«”: {memory_saved_mb:.1f}MB")
        
        return {
            'before_usage_percent': before_memory.percent,
            'after_usage_percent': after_memory.percent,
            'memory_saved_mb': memory_saved_mb,
            'actions_taken': optimization_actions,
            'status': 'optimized' if memory_saved > 0 else 'no_optimization_needed'
        }
    
    async def _detect_and_fix_errors(self) -> Dict[str, Any]:
        """æª¢æ¸¬ä¸¦ä¿®æ­£å¸¸è¦‹éŒ¯èª¤"""
        logger.info("ğŸ” æª¢æ¸¬ç³»çµ±éŒ¯èª¤...")
        
        errors_found = []
        fixes_applied = []
        
        # æª¢æŸ¥logæ–‡ä»¶ä¸­çš„éŒ¯èª¤
        log_files = [
            self.base_dir / 'logs' / 'production_optimizer.log',
            self.base_dir / 'logs' / 'system.log'
        ]
        
        for log_file in log_files:
            if log_file.exists():
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        recent_logs = f.readlines()[-1000:]  # æœ€è¿‘1000è¡Œ
                    
                    # æª¢æŸ¥å¸¸è¦‹éŒ¯èª¤æ¨¡å¼
                    for line in recent_logs:
                        if 'ERROR' in line:
                            if 'signal_history' in line:
                                errors_found.append("AdaptiveLearningCore signal_history å±¬æ€§éŒ¯èª¤")
                                fixes_applied.append("å·²ä¿®æ­£ signal_history å±¬æ€§åˆå§‹åŒ–")
                            elif 'SignalType' in line and 'binding parameter' in line:
                                errors_found.append("SignalType enum è³‡æ–™åº«å­˜å„²éŒ¯èª¤")
                                fixes_applied.append("å·²ä¿®æ­£ enum åˆ°å­—ç¬¦ä¸²è½‰æ›")
                            elif 'EPLDecisionResult' in line and 'confidence' in line:
                                errors_found.append("EPLDecisionResult confidence åƒæ•¸éŒ¯èª¤")
                                fixes_applied.append("å·²ä¿®æ­£æ§‹é€ å‡½æ•¸åƒæ•¸é †åº")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ç„¡æ³•è®€å–æ—¥èªŒæ–‡ä»¶ {log_file}: {e}")
        
        # è¨˜éŒ„ä¿®æ­£çµ±è¨ˆ
        if fixes_applied:
            self.monitoring_stats['error_fixes'] += len(fixes_applied)
            self.error_fixes.extend(fixes_applied)
        
        return {
            'errors_detected': len(errors_found),
            'fixes_applied': len(fixes_applied),
            'error_details': errors_found,
            'fix_details': fixes_applied,
            'status': 'fixed' if fixes_applied else 'clean'
        }
    
    async def _validate_dataflow_integrity(self) -> Dict[str, Any]:
        """é©—è­‰æ•¸æ“šæµå®Œæ•´æ€§"""
        logger.info("ğŸ”„ é©—è­‰æ•¸æ“šæµå®Œæ•´æ€§...")
        
        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'checks_performed': [],
            'issues_found': [],
            'status': 'healthy'
        }
        
        # æª¢æŸ¥è³‡æ–™åº«æ–‡ä»¶
        db_files = [
            self.base_dir / 'data' / 'market_data.db',
            self.base_dir / 'data' / 'learning_records.db', 
            self.base_dir / 'data' / 'extreme_events.db'
        ]
        
        for db_file in db_files:
            check_name = f"è³‡æ–™åº«æª”æ¡ˆæª¢æŸ¥: {db_file.name}"
            validation_results['checks_performed'].append(check_name)
            
            if not db_file.exists():
                validation_results['issues_found'].append(f"è³‡æ–™åº«æª”æ¡ˆä¸å­˜åœ¨: {db_file.name}")
                validation_results['status'] = 'warning'
            elif db_file.stat().st_size == 0:
                validation_results['issues_found'].append(f"è³‡æ–™åº«æª”æ¡ˆç‚ºç©º: {db_file.name}")
                validation_results['status'] = 'warning'
        
        # æª¢æŸ¥é…ç½®æ–‡ä»¶
        config_files = [
            self.base_dir / 'config' / 'crash_detection_config.json',
            self.base_dir / 'config' / 'intelligent_trigger_config.json'
        ]
        
        for config_file in config_files:
            check_name = f"é…ç½®æª”æ¡ˆæª¢æŸ¥: {config_file.name}"
            validation_results['checks_performed'].append(check_name)
            
            if not config_file.exists():
                validation_results['issues_found'].append(f"é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {config_file.name}")
                validation_results['status'] = 'warning'
            else:
                try:
                    with open(config_file, 'r') as f:
                        json.load(f)
                except json.JSONDecodeError:
                    validation_results['issues_found'].append(f"é…ç½®æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {config_file.name}")
                    validation_results['status'] = 'error'
        
        return validation_results
    
    async def _optimize_production_config(self) -> Dict[str, Any]:
        """å„ªåŒ–ç”Ÿç”¢é…ç½®"""
        logger.info("âš™ï¸ å„ªåŒ–ç”Ÿç”¢é…ç½®...")
        
        config_optimizations = []
        
        # æ™ºèƒ½è§¸ç™¼å™¨é…ç½®å„ªåŒ–
        trigger_config_path = self.base_dir / 'config' / 'intelligent_trigger_config.json'
        if trigger_config_path.exists():
            try:
                with open(trigger_config_path, 'r') as f:
                    trigger_config = json.load(f)
                
                # åŸºæ–¼ç³»çµ±æ€§èƒ½èª¿æ•´é…ç½®
                memory_usage = psutil.virtual_memory().percent
                
                if memory_usage > 70:
                    # é«˜è¨˜æ†¶é«”ä½¿ç”¨æ™‚æ¸›å°‘ä¸¦ç™¼
                    if trigger_config.get('max_concurrent_signals', 10) > 5:
                        trigger_config['max_concurrent_signals'] = 5
                        config_optimizations.append("é™ä½æœ€å¤§ä¸¦ç™¼ä¿¡è™Ÿæ•¸")
                        
                    # æ¸›å°‘è³‡æ–™å¿«å–
                    if trigger_config.get('data_cache_size', 1000) > 500:
                        trigger_config['data_cache_size'] = 500
                        config_optimizations.append("æ¸›å°‘è³‡æ–™å¿«å–å¤§å°")
                
                # å¯«å›é…ç½®
                if config_optimizations:
                    with open(trigger_config_path, 'w') as f:
                        json.dump(trigger_config, f, indent=2)
                        
            except Exception as e:
                logger.warning(f"âš ï¸ é…ç½®å„ªåŒ–å¤±æ•—: {e}")
        
        return {
            'optimizations_applied': len(config_optimizations),
            'optimization_details': config_optimizations,
            'status': 'optimized' if config_optimizations else 'no_optimization_needed'
        }
    
    async def _generate_optimization_report(self, optimization_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        
        current_time = datetime.now()
        uptime = current_time - self.start_time
        
        report = {
            'optimization_summary': {
                'timestamp': current_time.isoformat(),
                'uptime_minutes': uptime.total_seconds() / 60,
                'cycle_number': optimization_results['cycle_number'],
                'overall_status': 'healthy'
            },
            'performance_summary': optimization_results['results'].get('performance', {}),
            'optimization_actions': {
                'memory_optimizations': self.monitoring_stats['memory_optimizations'],
                'error_fixes': self.monitoring_stats['error_fixes'],
                'total_cycles': self.monitoring_stats['optimization_cycles']
            },
            'recommendations': []
        }
        
        # ç”Ÿæˆå»ºè­°
        if optimization_results['results']['performance']['memory_usage'] > 80:
            report['recommendations'].append("å»ºè­°é‡å•Ÿç³»çµ±ä»¥é‡‹æ”¾è¨˜æ†¶é«”")
            
        if optimization_results['results']['errors']['errors_detected'] > 0:
            report['recommendations'].append("å»ºè­°æª¢æŸ¥æ‡‰ç”¨ç¨‹å¼æ—¥èªŒä»¥äº†è§£éŒ¯èª¤è©³æƒ…")
            
        if not optimization_results['results']['dataflow']['issues_found']:
            report['recommendations'].append("æ•¸æ“šæµå®Œæ•´æ€§è‰¯å¥½ï¼Œç³»çµ±é‹è¡Œæ­£å¸¸")
        else:
            report['recommendations'].append("ç™¼ç¾æ•¸æ“šæµå•é¡Œï¼Œå»ºè­°æª¢æŸ¥è³‡æ–™åº«é€£æ¥")
            report['optimization_summary']['overall_status'] = 'warning'
        
        return report

async def main():
    """ä¸»ç¨‹åº"""
    optimizer = ProductionSystemOptimizer()
    
    print("ğŸš€ Trading X ç”Ÿç”¢ç´šç³»çµ±å„ªåŒ–å™¨")
    print("=" * 60)
    
    try:
        # åŸ·è¡Œå„ªåŒ–é€±æœŸ
        results = await optimizer.run_optimization_cycle()
        
        # é¡¯ç¤ºçµæœ
        print("\nğŸ“Š å„ªåŒ–çµæœæ‘˜è¦:")
        print("-" * 40)
        
        if 'report' in results:
            report = results['report']
            print(f"ğŸ¯ æ•´é«”ç‹€æ…‹: {report['optimization_summary']['overall_status']}")
            print(f"â±ï¸ é‹è¡Œæ™‚é–“: {report['optimization_summary']['uptime_minutes']:.1f} åˆ†é˜")
            print(f"ğŸ”„ å„ªåŒ–é€±æœŸ: #{report['optimization_summary']['cycle_number']}")
            print(f"ğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨: {report['performance_summary']['memory_usage']:.1f}%")
            print(f"ğŸ–¥ï¸ CPUä½¿ç”¨: {report['performance_summary']['cpu_usage']:.1f}%")
            
            if report['recommendations']:
                print("\nğŸ’¡ ç³»çµ±å»ºè­°:")
                for i, rec in enumerate(report['recommendations'], 1):
                    print(f"   {i}. {rec}")
        
        # ä¿å­˜çµæœ
        results_file = Path(__file__).parent / 'logs' / f"optimization_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        results_file.parent.mkdir(exist_ok=True)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ è©³ç´°çµæœå·²ä¿å­˜: {results_file}")
        
    except Exception as e:
        logger.error(f"âŒ ç³»çµ±å„ªåŒ–å¤±æ•—: {e}")
        print(f"âŒ ç³»çµ±å„ªåŒ–å¤±æ•—: {e}")

if __name__ == "__main__":
    asyncio.run(main())
