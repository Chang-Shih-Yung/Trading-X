"""
ğŸ” EPL Decision History Tracking æ•¸æ“šæµåŒ¹é…é©—è­‰
===============================================

æª¢æŸ¥ JSON é…ç½®æ˜¯å¦èˆ‡å…¶ä»– Phase çš„æ•¸æ“šè¼¸å…¥è¼¸å‡ºåŒ¹é…
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

logger = logging.getLogger(__name__)

class EPLDataFlowValidator:
    """EPL æ•¸æ“šæµé©—è­‰å™¨"""
    
    def __init__(self):
        self.epl_config_path = Path("/Users/henrychang/Desktop/Trading-X/X/backend/phase4_output_monitoring/3_epl_decision_history_tracking/epl_decision_history_tracking_config.json")
        
    def load_epl_config(self) -> Dict[str, Any]:
        """è¼‰å…¥ EPL é…ç½®"""
        try:
            with open(self.epl_config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥ EPL é…ç½®å¤±æ•—: {e}")
            return {}
    
    def validate_phase1_data_integration(self, config: Dict) -> Dict[str, Any]:
        """é©—è­‰ Phase1 æ•¸æ“šæ•´åˆ"""
        integration_score = 0
        max_score = 25
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        decision_arch = epl_config.get("decision_tracking_architecture", {})
        
        # Phase1 ä¿¡è™Ÿå€™é¸æ•¸æ“š (10åˆ†)
        phase1_score = 0
        decision_creation = decision_arch.get("decision_lifecycle_monitoring", {}).get("decision_creation", {})
        input_capture = decision_creation.get("input_data_capture", {})
        
        # æª¢æŸ¥æ˜¯å¦æ¥æ”¶å®Œæ•´çš„ SignalCandidate ç‰©ä»¶
        if "original_signal_candidate" in input_capture:
            if "complete_SignalCandidate_object" in str(input_capture["original_signal_candidate"]):
                phase1_score += 5
                details["signal_candidate_capture"] = "âœ… å®Œæ•´ SignalCandidate ç‰©ä»¶"
            else:
                details["signal_candidate_capture"] = "âš ï¸ ä¸å®Œæ•´çš„ä¿¡è™Ÿå€™é¸è³‡æ–™"
        else:
            details["signal_candidate_capture"] = "âŒ ç¼ºå°‘ä¿¡è™Ÿå€™é¸è³‡æ–™æ“·å–"
        
        # æª¢æŸ¥å¸‚å ´ä¸Šä¸‹æ–‡æ“·å–
        if "market_context" in input_capture:
            if "real_time_market_snapshot" in str(input_capture["market_context"]):
                phase1_score += 5
                details["market_context_capture"] = "âœ… å³æ™‚å¸‚å ´å¿«ç…§"
            else:
                details["market_context_capture"] = "âš ï¸ å¸‚å ´ä¸Šä¸‹æ–‡ä¸å®Œæ•´"
        else:
            details["market_context_capture"] = "âŒ ç¼ºå°‘å¸‚å ´ä¸Šä¸‹æ–‡"
        
        integration_score += phase1_score
        
        # Phase1 ä¿¡è™Ÿå“è³ªè¿½è¹¤ (8åˆ†)
        quality_tracking = 0
        
        # æª¢æŸ¥æ˜¯å¦è¿½è¹¤ä¿¡è™Ÿå“è³ªæŒ‡æ¨™
        signal_quality_indicators = [
            "confidence_metrics", "quality_score", "technical_strength", 
            "market_timing", "source_reliability"
        ]
        
        found_quality_indicators = 0
        for indicator in signal_quality_indicators:
            if indicator in str(decision_creation):
                found_quality_indicators += 1
        
        quality_tracking = min(8, found_quality_indicators * 2)
        details["signal_quality_tracking"] = f"âœ… {found_quality_indicators}/5 å“è³ªæŒ‡æ¨™å·²è¿½è¹¤"
        
        integration_score += quality_tracking
        
        # Phase1 è¼¸å‡ºæ ¼å¼ç›¸å®¹æ€§ (7åˆ†)
        output_compatibility = 0
        
        # æª¢æŸ¥æ±ºç­–è¼¸å‡ºæ˜¯å¦åŒ…å« Phase1 éœ€è¦çš„å›é¥‹
        decision_output = decision_creation.get("decision_output_capture", {})
        required_outputs = ["decision_type", "execution_parameters", "reasoning_chain", "confidence_metrics"]
        
        for output in required_outputs:
            if output in decision_output:
                output_compatibility += 1.75  # 7/4
        
        details["phase1_output_compatibility"] = f"âœ… {sum(1 for out in required_outputs if out in decision_output)}/4 è¼¸å‡ºæ ¼å¼"
        
        integration_score += int(output_compatibility)
        
        return {
            "phase1_integration_score": integration_score,
            "max_score": max_score,
            "percentage": (integration_score / max_score) * 100,
            "details": details,
            "status": "excellent" if integration_score >= 20 else "good" if integration_score >= 15 else "needs_improvement"
        }
    
    def validate_phase2_data_integration(self, config: Dict) -> Dict[str, Any]:
        """é©—è­‰ Phase2 æ•¸æ“šæ•´åˆ"""
        integration_score = 0
        max_score = 25
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        decision_arch = epl_config.get("decision_tracking_architecture", {})
        
        # Phase2 é è©•ä¼°çµæœ (10åˆ†)
        phase2_score = 0
        decision_creation = decision_arch.get("decision_lifecycle_monitoring", {}).get("decision_creation", {})
        input_capture = decision_creation.get("input_data_capture", {})
        
        # æª¢æŸ¥æ˜¯å¦æ¥æ”¶ Phase2 é è©•ä¼°çµæœ
        if "pre_evaluation_result" in input_capture:
            if "Phase2_PreEvaluationResult" in str(input_capture["pre_evaluation_result"]):
                phase2_score += 5
                details["pre_evaluation_capture"] = "âœ… Phase2 é è©•ä¼°çµæœ"
            else:
                details["pre_evaluation_capture"] = "âš ï¸ é è©•ä¼°çµæœæ ¼å¼ä¸ç¬¦"
        else:
            details["pre_evaluation_capture"] = "âŒ ç¼ºå°‘é è©•ä¼°çµæœ"
        
        # æª¢æŸ¥æŠ•è³‡çµ„åˆç‹€æ…‹
        if "portfolio_state" in input_capture:
            if "current_positions_and_risk_metrics" in str(input_capture["portfolio_state"]):
                phase2_score += 5
                details["portfolio_state_capture"] = "âœ… æŠ•è³‡çµ„åˆç‹€æ…‹å’Œé¢¨éšªæŒ‡æ¨™"
            else:
                details["portfolio_state_capture"] = "âš ï¸ æŠ•è³‡çµ„åˆç‹€æ…‹ä¸å®Œæ•´"
        else:
            details["portfolio_state_capture"] = "âŒ ç¼ºå°‘æŠ•è³‡çµ„åˆç‹€æ…‹"
        
        integration_score += phase2_score
        
        # Phase2 æ±ºç­–å¼•æ“æ•´åˆ (10åˆ†)
        engine_integration = 0
        decision_engine = decision_creation.get("decision_engine_process", {})
        
        # æª¢æŸ¥æ±ºç­–å¼•æ“æµç¨‹
        required_processes = [
            "scenario_routing", "parallel_evaluation", 
            "risk_validation", "priority_classification"
        ]
        
        for process in required_processes:
            if process in decision_engine:
                engine_integration += 2.5  # 10/4
        
        details["decision_engine_integration"] = f"âœ… {sum(1 for proc in required_processes if proc in decision_engine)}/4 æ±ºç­–æµç¨‹"
        
        integration_score += int(engine_integration)
        
        # Phase2 é¢¨éšªè©•ä¼°æ•´åˆ (5åˆ†)
        risk_integration = 0
        
        # æª¢æŸ¥å¤šå±¤é¢¨éšªè©•ä¼°
        if "multi_level_risk_assessment" in str(decision_engine.get("risk_validation", "")):
            risk_integration += 5
            details["risk_assessment_integration"] = "âœ… å¤šå±¤é¢¨éšªè©•ä¼°"
        else:
            details["risk_assessment_integration"] = "âŒ ç¼ºå°‘å¤šå±¤é¢¨éšªè©•ä¼°"
        
        integration_score += risk_integration
        
        return {
            "phase2_integration_score": integration_score,
            "max_score": max_score,
            "percentage": (integration_score / max_score) * 100,
            "details": details,
            "status": "excellent" if integration_score >= 20 else "good" if integration_score >= 15 else "needs_improvement"
        }
    
    def validate_phase3_data_integration(self, config: Dict) -> Dict[str, Any]:
        """é©—è­‰ Phase3 æ•¸æ“šæ•´åˆ"""
        integration_score = 0
        max_score = 30
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        decision_arch = epl_config.get("decision_tracking_architecture", {})
        
        # Phase3 åŸ·è¡Œè¿½è¹¤ (12åˆ†)
        execution_tracking = 0
        execution_track = decision_arch.get("decision_lifecycle_monitoring", {}).get("decision_execution_tracking", {})
        
        # åŸ·è¡Œå•Ÿå‹•è¿½è¹¤
        execution_initiation = execution_track.get("execution_initiation", {})
        required_initiation = ["execution_timestamp", "execution_latency", "market_conditions_at_execution", "slippage_measurement"]
        
        for item in required_initiation:
            if item in execution_initiation:
                execution_tracking += 3  # 12/4
        
        details["execution_initiation_tracking"] = f"âœ… {sum(1 for item in required_initiation if item in execution_initiation)}/4 åŸ·è¡Œå•Ÿå‹•æŒ‡æ¨™"
        
        integration_score += int(execution_tracking)
        
        # Phase3 åŸ·è¡Œç›£æ§ (12åˆ†)
        execution_monitoring = 0
        exec_monitoring = execution_track.get("execution_monitoring", {})
        
        required_monitoring = [
            "position_establishment", "risk_parameter_application",
            "portfolio_impact", "correlation_effects"
        ]
        
        for item in required_monitoring:
            if item in exec_monitoring:
                execution_monitoring += 3  # 12/4
        
        details["execution_monitoring"] = f"âœ… {sum(1 for item in required_monitoring if item in exec_monitoring)}/4 åŸ·è¡Œç›£æ§æŒ‡æ¨™"
        
        integration_score += int(execution_monitoring)
        
        # Phase3 çµæœæ¸¬é‡ (6åˆ†)
        outcome_measurement = 0
        outcome_track = decision_arch.get("decision_lifecycle_monitoring", {}).get("outcome_measurement", {})
        
        # å³æ™‚çµæœ
        immediate_outcomes = outcome_track.get("immediate_outcomes", {})
        if len(immediate_outcomes) >= 3:  # è‡³å°‘3å€‹å³æ™‚çµæœæŒ‡æ¨™
            outcome_measurement += 3
            details["immediate_outcomes"] = "âœ… å³æ™‚çµæœè¿½è¹¤"
        else:
            details["immediate_outcomes"] = "âš ï¸ å³æ™‚çµæœè¿½è¹¤ä¸è¶³"
        
        # å»¶ä¼¸çµæœ
        extended_outcomes = outcome_track.get("extended_outcomes", {})
        if len(extended_outcomes) >= 3:  # è‡³å°‘3å€‹å»¶ä¼¸çµæœæŒ‡æ¨™
            outcome_measurement += 3
            details["extended_outcomes"] = "âœ… å»¶ä¼¸çµæœè¿½è¹¤"
        else:
            details["extended_outcomes"] = "âš ï¸ å»¶ä¼¸çµæœè¿½è¹¤ä¸è¶³"
        
        integration_score += outcome_measurement
        
        return {
            "phase3_integration_score": integration_score,
            "max_score": max_score,
            "percentage": (integration_score / max_score) * 100,
            "details": details,
            "status": "excellent" if integration_score >= 24 else "good" if integration_score >= 18 else "needs_improvement"
        }
    
    def validate_phase4_internal_consistency(self, config: Dict) -> Dict[str, Any]:
        """é©—è­‰ Phase4 å…§éƒ¨ä¸€è‡´æ€§"""
        consistency_score = 0
        max_score = 20
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        
        # æ±ºç­–é¡å‹åˆ†æä¸€è‡´æ€§ (8åˆ†)
        decision_analytics = 0
        decision_type_analytics = epl_config.get("decision_tracking_architecture", {}).get("decision_type_analytics", {})
        
        # æª¢æŸ¥å››ç¨®æ±ºç­–é¡å‹çš„å®Œæ•´è¿½è¹¤
        required_decision_types = [
            "replacement_decision_tracking",
            "strengthening_decision_tracking", 
            "new_position_decision_tracking",
            "ignore_decision_tracking"
        ]
        
        for decision_type in required_decision_types:
            if decision_type in decision_type_analytics:
                decision_analytics += 2  # 8/4
        
        details["decision_type_completeness"] = f"âœ… {sum(1 for dt in required_decision_types if dt in decision_type_analytics)}/4 æ±ºç­–é¡å‹è¿½è¹¤"
        
        consistency_score += int(decision_analytics)
        
        # å­¸ç¿’ç³»çµ±ä¸€è‡´æ€§ (6åˆ†)
        learning_consistency = 0
        learning_config = epl_config.get("learning_and_optimization", {})
        
        required_learning = ["pattern_recognition", "adaptive_learning", "feedback_integration"]
        
        for learning_component in required_learning:
            if learning_component in learning_config:
                learning_consistency += 2  # 6/3
        
        details["learning_system_consistency"] = f"âœ… {sum(1 for lc in required_learning if lc in learning_config)}/3 å­¸ç¿’çµ„ä»¶"
        
        consistency_score += int(learning_consistency)
        
        # å ±å‘Šåˆ†æä¸€è‡´æ€§ (6åˆ†)
        reporting_consistency = 0
        reporting_config = epl_config.get("reporting_and_analytics", {})
        
        if "real_time_dashboards" in reporting_config:
            reporting_consistency += 3
            details["real_time_reporting"] = "âœ… å³æ™‚å ±å‘Š"
        else:
            details["real_time_reporting"] = "âŒ ç¼ºå°‘å³æ™‚å ±å‘Š"
        
        if "historical_analysis" in reporting_config:
            reporting_consistency += 3
            details["historical_analysis"] = "âœ… æ­·å²åˆ†æ"
        else:
            details["historical_analysis"] = "âŒ ç¼ºå°‘æ­·å²åˆ†æ"
        
        consistency_score += reporting_consistency
        
        return {
            "phase4_consistency_score": consistency_score,
            "max_score": max_score,
            "percentage": (consistency_score / max_score) * 100,
            "details": details,
            "status": "excellent" if consistency_score >= 16 else "good" if consistency_score >= 12 else "needs_improvement"
        }
    
    def check_data_flow_completeness(self, config: Dict) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šæµå®Œæ•´æ€§"""
        completeness_score = 0
        max_score = 20
        details = {}
        
        epl_config = config.get("PHASE4_EPL_DECISION_HISTORY_TRACKING", {})
        
        # æ•¸æ“šå­˜å„²å®Œæ•´æ€§ (10åˆ†)
        storage_completeness = 0
        storage_config = epl_config.get("data_storage_and_retrieval", {})
        
        if "decision_data_storage" in storage_config:
            storage_data = storage_config["decision_data_storage"]
            
            if "raw_decision_data" in storage_data:
                storage_completeness += 5
                details["raw_data_storage"] = "âœ… åŸå§‹æ±ºç­–æ•¸æ“šå­˜å„²"
            else:
                details["raw_data_storage"] = "âŒ ç¼ºå°‘åŸå§‹æ•¸æ“šå­˜å„²"
            
            if "aggregated_analytics" in storage_data:
                storage_completeness += 5
                details["aggregated_storage"] = "âœ… èšåˆåˆ†ææ•¸æ“šå­˜å„²"
            else:
                details["aggregated_storage"] = "âŒ ç¼ºå°‘èšåˆæ•¸æ“šå­˜å„²"
        
        completeness_score += storage_completeness
        
        # API æ¥å£å®Œæ•´æ€§ (10åˆ†)
        api_completeness = 0
        api_config = epl_config.get("api_interfaces", {})
        
        required_apis = ["decision_tracking_api", "performance_analytics_api", "learning_data_api"]
        
        for api in required_apis:
            if api in api_config:
                api_completeness += 3.33  # 10/3
        
        details["api_completeness"] = f"âœ… {sum(1 for api in required_apis if api in api_config)}/3 API æ¥å£"
        
        completeness_score += int(api_completeness)
        
        return {
            "completeness_score": completeness_score,
            "max_score": max_score,
            "percentage": (completeness_score / max_score) * 100,
            "details": details,
            "status": "excellent" if completeness_score >= 16 else "good" if completeness_score >= 12 else "needs_improvement"
        }
    
    def generate_validation_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆé©—è­‰å ±å‘Š"""
        print("ğŸ” EPL Decision History Tracking æ•¸æ“šæµåŒ¹é…é©—è­‰")
        print("=" * 70)
        
        # è¼‰å…¥é…ç½®
        config = self.load_epl_config()
        if not config:
            return {"validation_status": "failed", "error": "ç„¡æ³•è¼‰å…¥é…ç½®"}
        
        print("âœ… EPL é…ç½®è¼‰å…¥æˆåŠŸ")
        
        # å„é …é©—è­‰
        phase1_validation = self.validate_phase1_data_integration(config)
        phase2_validation = self.validate_phase2_data_integration(config)
        phase3_validation = self.validate_phase3_data_integration(config)
        phase4_validation = self.validate_phase4_internal_consistency(config)
        completeness_validation = self.check_data_flow_completeness(config)
        
        # è¨ˆç®—ç¸½åˆ†
        total_score = (
            phase1_validation["phase1_integration_score"] +
            phase2_validation["phase2_integration_score"] +
            phase3_validation["phase3_integration_score"] +
            phase4_validation["phase4_consistency_score"] +
            completeness_validation["completeness_score"]
        )
        max_total_score = (
            phase1_validation["max_score"] +
            phase2_validation["max_score"] +
            phase3_validation["max_score"] +
            phase4_validation["max_score"] +
            completeness_validation["max_score"]
        )
        
        overall_percentage = (total_score / max_total_score) * 100
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            "validation_timestamp": datetime.now().isoformat(),
            "overall_score": {
                "total_points": f"{total_score}/{max_total_score}",
                "percentage": f"{overall_percentage:.1f}%",
                "grade": self._get_grade(overall_percentage)
            },
            "detailed_validations": {
                "phase1_integration": phase1_validation,
                "phase2_integration": phase2_validation,
                "phase3_integration": phase3_validation,
                "phase4_consistency": phase4_validation,
                "data_flow_completeness": completeness_validation
            },
            "recommendations": self._generate_recommendations(overall_percentage, [
                phase1_validation, phase2_validation, phase3_validation, 
                phase4_validation, completeness_validation
            ])
        }
        
        # æ‰“å°çµæœ
        self._print_results(report)
        
        return report
    
    def _get_grade(self, percentage: float) -> str:
        """ç²å–è©•ç´š"""
        if percentage >= 95:
            return "A+ (å®Œç¾åŒ¹é…)"
        elif percentage >= 85:
            return "A (å„ªç§€åŒ¹é…)"
        elif percentage >= 75:
            return "B+ (è‰¯å¥½åŒ¹é…)"
        elif percentage >= 65:
            return "B (å¯æ¥å—åŒ¹é…)"
        elif percentage >= 55:
            return "C (éœ€æ”¹é€²åŒ¹é…)"
        else:
            return "D (éœ€é‡æ§‹åŒ¹é…)"
    
    def _generate_recommendations(self, overall_percentage, validations) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        if overall_percentage >= 90:
            recommendations.append("æ•¸æ“šæµåŒ¹é…å„ªç§€ï¼Œå¯ä»¥ç¹¼çºŒ Python å¯¦ç¾å„ªåŒ–")
        elif overall_percentage >= 80:
            recommendations.append("æ•¸æ“šæµåŒ¹é…è‰¯å¥½ï¼Œå»ºè­°å°å¹…èª¿æ•´å¾Œç¹¼çºŒ")
        else:
            # æª¢æŸ¥å…·é«”å•é¡Œ
            for validation in validations:
                if validation.get("percentage", 0) < 70:
                    recommendations.append("éœ€è¦ä¿®æ­£ä½åˆ†é©—è­‰é …ç›®çš„æ•¸æ“šæµé…ç½®")
        
        # å…·é«”å»ºè­°
        if validations[0].get("percentage", 0) < 80:  # Phase1
            recommendations.append("æ”¹é€² Phase1 ä¿¡è™Ÿæ•¸æ“šæ“·å–é…ç½®")
        if validations[1].get("percentage", 0) < 80:  # Phase2
            recommendations.append("å®Œå–„ Phase2 é è©•ä¼°çµæœæ•´åˆ")
        if validations[2].get("percentage", 0) < 80:  # Phase3
            recommendations.append("åŠ å¼· Phase3 åŸ·è¡Œè¿½è¹¤é…ç½®")
        
        return recommendations if recommendations else ["æ•¸æ“šæµé…ç½®éœ€è¦å…¨é¢æª¢è¨"]
    
    def _print_results(self, report: Dict):
        """æ‰“å°çµæœ"""
        print(f"\nğŸ“Š ç¸½é«”è©•åˆ†: {report['overall_score']['percentage']} - {report['overall_score']['grade']}")
        
        print("\nğŸ“‹ è©³ç´°é©—è­‰:")
        for category, details in report["detailed_validations"].items():
            percentage = details["percentage"]
            status_icon = "âœ…" if percentage >= 80 else "âš ï¸" if percentage >= 60 else "âŒ"
            print(f"  {status_icon} {category}: {percentage:.1f}% ({details['status']})")
            
            if 'details' in details:
                for detail_key, detail_value in details["details"].items():
                    print(f"    {detail_value}")
        
        print("\nğŸ’¡ å»ºè­°:")
        for rec in report["recommendations"]:
            print(f"  â€¢ {rec}")

def main():
    """ä¸»å‡½æ•¸"""
    validator = EPLDataFlowValidator()
    report = validator.generate_validation_report()
    
    print(f"\nğŸ¯ EPL æ•¸æ“šæµåŒ¹é…é©—è­‰å®Œæˆ")
    print(f"ğŸ“Š æœ€çµ‚è©•åˆ†: {report['overall_score']['percentage']}")
    
    # åˆ¤æ–·æ˜¯å¦å¯ä»¥ç¹¼çºŒ
    percentage = float(report['overall_score']['percentage'].rstrip('%'))
    if percentage >= 80:
        print("âœ… æ•¸æ“šæµåŒ¹é…è‰¯å¥½ï¼Œå¯ä»¥ç¹¼çºŒ Python å¯¦ç¾å„ªåŒ–")
        return True
    else:
        print("âŒ æ•¸æ“šæµåŒ¹é…éœ€è¦æ”¹é€²ï¼Œå»ºè­°å…ˆä¿®æ­£ JSON é…ç½®")
        return False

if __name__ == "__main__":
    main()
