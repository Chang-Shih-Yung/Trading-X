#!/usr/bin/env python3
"""
å®Œæ•´å„€è¡¨æ¿å¯¦ç¾é©—è­‰è…³æœ¬
é©—è­‰é‡å¯«å¾Œçš„ unified_monitoring_dashboard.py æ˜¯å¦å®Œå…¨åŒ¹é… JSON é…ç½®
"""

import json
import ast
import inspect
from pathlib import Path
from typing import Dict, List, Any, Tuple

class CompleteImplementationValidator:
    def __init__(self):
        self.base_path = Path("/Users/henrychang/Desktop/Trading-X")
        self.json_path = self.base_path / "X/backend/phase4_output_monitoring/1_unified_monitoring_dashboard/unified_monitoring_dashboard_config.json"
        self.py_path = self.base_path / "X/backend/phase4_output_monitoring/1_unified_monitoring_dashboard/unified_monitoring_dashboard.py"
        
    def load_json_config(self) -> Dict[str, Any]:
        """è¼‰å…¥JSONé…ç½®"""
        try:
            with open(self.json_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ ç„¡æ³•è¼‰å…¥JSONé…ç½®: {e}")
            return {}
    
    def analyze_python_implementation(self) -> Dict[str, Any]:
        """åˆ†æPythonå¯¦ç¾"""
        try:
            with open(self.py_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            classes = []
            methods = []
            functions = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    classes.append(node.name)
                elif isinstance(node, ast.FunctionDef):
                    methods.append(node.name)
                elif isinstance(node, ast.AsyncFunctionDef):
                    methods.append(node.name)
            
            return {
                "classes": classes,
                "methods": methods,
                "functions": functions,
                "line_count": len(content.split('\n'))
            }
        except Exception as e:
            print(f"âŒ ç„¡æ³•åˆ†æPythonå¯¦ç¾: {e}")
            return {}
    
    def validate_widget_implementation(self, config: Dict[str, Any], py_analysis: Dict[str, Any]) -> Tuple[int, int]:
        """é©—è­‰Widgetå¯¦ç¾"""
        score = 0
        max_score = 50
        
        dashboard_config = config.get("PHASE4_UNIFIED_MONITORING_DASHBOARD", {})
        widgets_config = dashboard_config.get("dashboard_widgets", {})
        
        required_widgets = list(widgets_config.keys())
        
        print("\nğŸ” é©—è­‰Widgetå¯¦ç¾:")
        print("-" * 40)
        
        # æª¢æŸ¥å¿…è¦çš„widgetç”Ÿæˆæ–¹æ³•
        required_methods = [
            "generate_system_status_overview_data",
            "generate_signal_processing_analytics_data", 
            "generate_epl_decision_tracking_data",
            "generate_notification_success_monitoring_data",
            "generate_system_performance_monitoring_data"
        ]
        
        for method in required_methods:
            if method in py_analysis.get("methods", []):
                score += 10
                print(f"âœ… {method} å·²å¯¦ç¾")
            else:
                print(f"âŒ ç¼ºå°‘æ–¹æ³•: {method}")
        
        return score, max_score
    
    def validate_data_structures(self, py_analysis: Dict[str, Any]) -> Tuple[int, int]:
        """é©—è­‰æ•¸æ“šçµæ§‹"""
        score = 0
        max_score = 30
        
        print("\nğŸ” é©—è­‰æ•¸æ“šçµæ§‹:")
        print("-" * 40)
        
        required_classes = [
            "SystemStatus",
            "WidgetType", 
            "SignalPriority",
            "EPLDecisionType",
            "MetricValue",
            "TimeSeriesData",
            "WidgetData",
            "SystemHealthIndicator",
            "NotificationDeliveryMetrics",
            "EPLDecisionMetrics",
            "SignalProcessingStats",
            "UnifiedMonitoringDashboard"
        ]
        
        implemented_classes = py_analysis.get("classes", [])
        
        for cls in required_classes:
            if cls in implemented_classes:
                score += 2
                print(f"âœ… {cls} é¡å·²å®šç¾©")
            else:
                print(f"âŒ ç¼ºå°‘é¡: {cls}")
        
        # é¡å¤–åˆ†æ•¸çµ¦ä¸»é¡
        if "UnifiedMonitoringDashboard" in implemented_classes:
            score += 6
            print("âœ… ä¸»è¦ç›£æ§é¡å·²å¯¦ç¾")
        
        return score, max_score
    
    def validate_core_functionality(self, py_analysis: Dict[str, Any]) -> Tuple[int, int]:
        """é©—è­‰æ ¸å¿ƒåŠŸèƒ½"""
        score = 0
        max_score = 40
        
        print("\nğŸ” é©—è­‰æ ¸å¿ƒåŠŸèƒ½:")
        print("-" * 40)
        
        required_core_methods = [
            "record_signal_processed",
            "record_epl_decision", 
            "record_notification_delivery",
            "update_system_performance",
            "update_all_widgets",
            "get_widget_data",
            "get_all_widgets_data",
            "get_real_time_api_data",
            "start_real_time_monitoring"
        ]
        
        implemented_methods = py_analysis.get("methods", [])
        
        for method in required_core_methods:
            if method in implemented_methods:
                score += 4
                print(f"âœ… {method} å·²å¯¦ç¾")
            else:
                print(f"âŒ ç¼ºå°‘æ ¸å¿ƒæ–¹æ³•: {method}")
        
        # æª¢æŸ¥è¼”åŠ©æ–¹æ³•
        helper_methods = [m for m in implemented_methods if m.startswith("_")]
        if len(helper_methods) >= 20:
            score += 4
            print(f"âœ… è±å¯Œçš„è¼”åŠ©æ–¹æ³• ({len(helper_methods)} å€‹)")
        
        return score, max_score
    
    def validate_json_compliance(self, config: Dict[str, Any]) -> Tuple[int, int]:
        """é©—è­‰JSONé…ç½®åˆè¦æ€§"""
        score = 0
        max_score = 30
        
        print("\nğŸ” é©—è­‰JSONé…ç½®åˆè¦æ€§:")
        print("-" * 40)
        
        try:
            dashboard_config = config["PHASE4_UNIFIED_MONITORING_DASHBOARD"]
            
            # æª¢æŸ¥é›†æˆæ¨™æº–
            if "integration_standards" in dashboard_config:
                score += 5
                print("âœ… é›†æˆæ¨™æº–å·²å®šç¾©")
                
                integration = dashboard_config["integration_standards"]
                if "upstream_integration" in integration:
                    upstream = integration["upstream_integration"]
                    
                    # æª¢æŸ¥ä¿®æ­£å¾Œçš„Phase2é…ç½®
                    if "phase2_pre_evaluation" in upstream:
                        phase2 = upstream["phase2_pre_evaluation"]
                        if phase2.get("monitoring_input") == "EnhancedRealDataQualityMonitoringEngine":
                            score += 5
                            print("âœ… Phase2 monitoring_input é…ç½®æ­£ç¢º")
                        if phase2.get("quality_scores") == "real_data_quality_monitoring":
                            score += 5
                            print("âœ… Phase2 quality_scores é…ç½®æ­£ç¢º")
            
            # æª¢æŸ¥å„€è¡¨æ¿æ¶æ§‹
            if "monitoring_architecture" in dashboard_config:
                score += 5
                print("âœ… ç›£æ§æ¶æ§‹å·²å®šç¾©")
            
            # æª¢æŸ¥Widgeté…ç½®
            if "dashboard_widgets" in dashboard_config:
                score += 5
                print("âœ… å„€è¡¨æ¿Widgetå·²é…ç½®")
                
                widgets = dashboard_config["dashboard_widgets"]
                required_widgets = [
                    "system_status_overview",
                    "signal_processing_analytics",
                    "epl_decision_tracking", 
                    "notification_success_monitoring",
                    "system_performance_monitoring"
                ]
                
                for widget in required_widgets:
                    if widget in widgets:
                        score += 1
                        print(f"âœ… Widget {widget} å·²é…ç½®")
            
        except Exception as e:
            print(f"âŒ JSONé…ç½®é©—è­‰éŒ¯èª¤: {e}")
        
        return score, max_score
    
    def validate_performance_targets(self, config: Dict[str, Any]) -> Tuple[int, int]:
        """é©—è­‰æ€§èƒ½ç›®æ¨™"""
        score = 0
        max_score = 20
        
        print("\nğŸ” é©—è­‰æ€§èƒ½ç›®æ¨™:")
        print("-" * 40)
        
        try:
            dashboard_config = config["PHASE4_UNIFIED_MONITORING_DASHBOARD"]
            
            if "performance_targets" in dashboard_config:
                targets = dashboard_config["performance_targets"]
                
                if "dashboard_performance" in targets:
                    score += 10
                    print("âœ… å„€è¡¨æ¿æ€§èƒ½ç›®æ¨™å·²å®šç¾©")
                
                if "data_accuracy" in targets:
                    score += 10
                    print("âœ… æ•¸æ“šæº–ç¢ºæ€§ç›®æ¨™å·²å®šç¾©")
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½ç›®æ¨™é©—è­‰éŒ¯èª¤: {e}")
        
        return score, max_score
    
    def run_complete_validation(self):
        """åŸ·è¡Œå®Œæ•´é©—è­‰"""
        print("ğŸš€ é–‹å§‹å®Œæ•´å„€è¡¨æ¿å¯¦ç¾é©—è­‰...")
        print("=" * 60)
        
        # è¼‰å…¥é…ç½®å’Œåˆ†æä»£ç¢¼
        config = self.load_json_config()
        py_analysis = self.analyze_python_implementation()
        
        if not config or not py_analysis:
            print("âŒ ç„¡æ³•è¼‰å…¥å¿…è¦æ–‡ä»¶")
            return
        
        print(f"ğŸ“Š Pythonå¯¦ç¾çµ±è¨ˆ:")
        print(f"   - é¡å®šç¾©: {len(py_analysis.get('classes', []))}")
        print(f"   - æ–¹æ³•: {len(py_analysis.get('methods', []))}")
        print(f"   - å‡½æ•¸: {len(py_analysis.get('functions', []))}")
        print(f"   - ä»£ç¢¼è¡Œæ•¸: {py_analysis.get('line_count', 0)}")
        
        # åŸ·è¡Œå„é …é©—è­‰
        widget_score, widget_max = self.validate_widget_implementation(config, py_analysis)
        data_score, data_max = self.validate_data_structures(py_analysis)
        core_score, core_max = self.validate_core_functionality(py_analysis)
        json_score, json_max = self.validate_json_compliance(config)
        perf_score, perf_max = self.validate_performance_targets(config)
        
        # è¨ˆç®—ç¸½åˆ†
        total_score = widget_score + data_score + core_score + json_score + perf_score
        total_max = widget_max + data_max + core_max + json_max + perf_max
        percentage = (total_score / total_max) * 100
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ é©—è­‰çµæœæ‘˜è¦:")
        print(f"   ğŸ¯ Widgetå¯¦ç¾: {widget_score}/{widget_max}")
        print(f"   ğŸ—ï¸ æ•¸æ“šçµæ§‹: {data_score}/{data_max}") 
        print(f"   âš™ï¸ æ ¸å¿ƒåŠŸèƒ½: {core_score}/{core_max}")
        print(f"   ğŸ“„ JSONåˆè¦æ€§: {json_score}/{json_max}")
        print(f"   ğŸª æ€§èƒ½ç›®æ¨™: {perf_score}/{perf_max}")
        print("-" * 60)
        print(f"ğŸ¯ ç¸½é©—è­‰çµæœ: {total_score}/{total_max} ({percentage:.1f}%)")
        
        if percentage >= 95:
            print("ğŸ‰ å®Œç¾å¯¦ç¾ï¼å„€è¡¨æ¿å®Œå…¨åŒ¹é…JSONé…ç½®")
            print("âœ… å¯ä»¥é€²è¡Œç”Ÿç”¢éƒ¨ç½²")
        elif percentage >= 85:
            print("âœ… å¯¦ç¾è‰¯å¥½ï¼å»ºè­°é€²è¡Œç´°å¾®èª¿æ•´")
            print("âš ï¸ å¯ä»¥é€²è¡Œæ¸¬è©¦éƒ¨ç½²")
        elif percentage >= 70:
            print("âš ï¸ åŸºæœ¬å¯¦ç¾å®Œæˆï¼Œéœ€è¦è£œå……ç¼ºå¤±åŠŸèƒ½")
            print("ğŸ”§ å»ºè­°å®Œå–„å¾Œå†éƒ¨ç½²")
        else:
            print("âŒ å¯¦ç¾ä¸å®Œæ•´ï¼Œéœ€è¦é‡å¤§æ”¹é€²")
            print("ğŸ›‘ ä¸å»ºè­°éƒ¨ç½²")
        
        return percentage

if __name__ == "__main__":
    validator = CompleteImplementationValidator()
    result = validator.run_complete_validation()
