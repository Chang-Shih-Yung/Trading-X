"""
å‹•æ…‹é‡åˆ†é…ç®—æ³• - Trading X Phase 3
å¯¦æ™‚ç›£æ§ä¸¦é‡æ–°åˆ†é…æ¬Šé‡ä»¥å„ªåŒ–äº¤æ˜“æ€§èƒ½
"""

from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import logging
import asyncio
import math
from collections import defaultdict, deque
import numpy as np

logger = logging.getLogger(__name__)

class ReallocationTrigger(Enum):
    """é‡åˆ†é…è§¸ç™¼æ¢ä»¶"""
    PERFORMANCE_DEGRADATION = "performance_degradation"    # æ€§èƒ½ä¸‹é™
    SIGNAL_QUALITY_CHANGE = "signal_quality_change"       # ä¿¡è™Ÿå“è³ªè®ŠåŒ–
    MARKET_REGIME_SHIFT = "market_regime_shift"           # å¸‚å ´åˆ¶åº¦è½‰æ›
    VOLATILITY_SPIKE = "volatility_spike"                 # æ³¢å‹•ç‡çªå¢
    CORRELATION_BREAKDOWN = "correlation_breakdown"        # ç›¸é—œæ€§ç ´è£‚
    RISK_THRESHOLD_BREACH = "risk_threshold_breach"       # é¢¨éšªé–¾å€¼çªç ´
    MANUAL_OVERRIDE = "manual_override"                   # æ‰‹å‹•è¦†è“‹

class OptimizationMethod(Enum):
    """å„ªåŒ–æ–¹æ³•"""
    GRADIENT_ASCENT = "gradient_ascent"           # æ¢¯åº¦ä¸Šå‡
    GENETIC_ALGORITHM = "genetic_algorithm"       # éºå‚³ç®—æ³•
    PARTICLE_SWARM = "particle_swarm"            # ç²’å­ç¾¤å„ªåŒ–
    BAYESIAN_OPTIMIZATION = "bayesian_optimization" # è²è‘‰æ–¯å„ªåŒ–
    ADAPTIVE_MOMENTUM = "adaptive_momentum"       # è‡ªé©æ‡‰å‹•é‡

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™"""
    symbol: str
    timeframe: str
    
    # æ”¶ç›Šç›¸é—œ
    total_return: float = 0.0
    sharpe_ratio: float = 0.0
    max_drawdown: float = 0.0
    win_rate: float = 0.0
    
    # é¢¨éšªç›¸é—œ
    volatility: float = 0.0
    var_95: float = 0.0        # 95% VaR
    beta: float = 1.0
    
    # äº¤æ˜“ç›¸é—œ
    total_trades: int = 0
    avg_trade_duration: float = 0.0
    profit_factor: float = 1.0
    
    # ä¿¡è™Ÿå“è³ª
    signal_accuracy: float = 0.0
    false_positive_rate: float = 0.0
    signal_timeliness: float = 0.0
    
    # æ™‚é–“æˆ³
    measurement_time: datetime = field(default_factory=datetime.now)
    measurement_period_hours: int = 24

@dataclass
class WeightOptimizationResult:
    """æ¬Šé‡å„ªåŒ–çµæœ"""
    original_weights: Dict[str, float]
    optimized_weights: Dict[str, float]
    expected_improvement: float
    confidence_score: float
    optimization_method: OptimizationMethod
    iterations: int
    convergence_achieved: bool
    
    # è©³ç´°åˆ†æ
    performance_projection: PerformanceMetrics
    risk_assessment: Dict[str, float]
    sensitivity_analysis: Dict[str, float]
    
    optimization_time: datetime = field(default_factory=datetime.now)
    explanation: str = ""

@dataclass
class ReallocationEvent:
    """é‡åˆ†é…äº‹ä»¶è¨˜éŒ„"""
    event_id: str
    trigger: ReallocationTrigger
    symbol: str
    timeframe: str
    
    before_weights: Dict[str, float]
    after_weights: Dict[str, float]
    expected_impact: float
    actual_impact: Optional[float] = None
    
    trigger_data: Dict[str, Any] = field(default_factory=dict)
    event_time: datetime = field(default_factory=datetime.now)
    validation_time: Optional[datetime] = None

class DynamicReallocationEngine:
    """å‹•æ…‹é‡åˆ†é…å¼•æ“"""
    
    def __init__(self):
        # æ€§èƒ½è¿½è¹¤
        self.performance_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        self.weight_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=50))
        self.reallocation_history: deque = deque(maxlen=200)
        
        # å„ªåŒ–åƒæ•¸
        self.optimization_params = {
            "learning_rate": 0.01,
            "momentum": 0.9,
            "decay_rate": 0.95,
            "convergence_threshold": 0.001,
            "max_iterations": 100,
            "improvement_threshold": 0.02,  # 2% æœ€å°æ”¹å–„è¦æ±‚
            "risk_penalty": 0.1
        }
        
        # è§¸ç™¼é–¾å€¼
        self.trigger_thresholds = {
            "performance_degradation": -0.05,      # 5% æ€§èƒ½ä¸‹é™
            "signal_quality_change": -0.10,        # 10% å“è³ªä¸‹é™
            "volatility_spike": 2.0,                # 2å€æ³¢å‹•ç‡å¢åŠ 
            "correlation_breakdown": -0.3,          # ç›¸é—œæ€§é™è‡³-0.3ä»¥ä¸‹
            "risk_threshold_breach": 0.15           # 15% VaR çªç ´
        }
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            "total_reallocations": 0,
            "successful_reallocations": 0,
            "total_improvement": 0.0,
            "avg_improvement": 0.0,
            "last_reallocation": None
        }
        
        # é‹è¡Œç‹€æ…‹
        self.is_monitoring = False
        self.monitoring_task: Optional[asyncio.Task] = None
        
        logger.info("âš™ï¸ å‹•æ…‹é‡åˆ†é…å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    async def start_monitoring(self):
        """å•Ÿå‹•å‹•æ…‹ç›£æ§"""
        if self.is_monitoring:
            logger.warning("âš ï¸ å‹•æ…‹ç›£æ§å·²ç¶“åœ¨é‹è¡Œä¸­")
            return
        
        self.is_monitoring = True
        self.monitoring_task = asyncio.create_task(self._monitoring_loop())
        logger.info("ğŸš€ å‹•æ…‹é‡åˆ†é…ç›£æ§å·²å•Ÿå‹•")
    
    async def stop_monitoring(self):
        """åœæ­¢å‹•æ…‹ç›£æ§"""
        self.is_monitoring = False
        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass
        logger.info("â¹ï¸ å‹•æ…‹é‡åˆ†é…ç›£æ§å·²åœæ­¢")
    
    async def _monitoring_loop(self):
        """ç›£æ§å¾ªç’°"""
        while self.is_monitoring:
            try:
                # æª¢æŸ¥æ‰€æœ‰æ´»èºçš„äº¤æ˜“å°å’Œæ™‚é–“æ¡†æ¶
                await self._check_reallocation_triggers()
                
                # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥ (æ¯5åˆ†é˜)
                await asyncio.sleep(300)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ ç›£æ§å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(60)
    
    async def _check_reallocation_triggers(self):
        """æª¢æŸ¥é‡åˆ†é…è§¸ç™¼æ¢ä»¶"""
        # æ¨¡æ“¬æª¢æŸ¥å¤šå€‹äº¤æ˜“å°
        symbols = ["BTCUSDT", "ETHUSDT", "ADAUSDT"]
        timeframes = ["short", "medium", "long"]
        
        for symbol in symbols:
            for timeframe in timeframes:
                try:
                    # ç²å–æœ€æ–°æ€§èƒ½æŒ‡æ¨™
                    current_metrics = await self._get_performance_metrics(symbol, timeframe)
                    if not current_metrics:
                        continue
                    
                    # æª¢æŸ¥è§¸ç™¼æ¢ä»¶
                    trigger = await self._evaluate_triggers(symbol, timeframe, current_metrics)
                    
                    if trigger:
                        logger.info(f"ğŸ¯ æª¢æ¸¬åˆ°é‡åˆ†é…è§¸ç™¼: {symbol} {timeframe} - {trigger.value}")
                        
                        # åŸ·è¡Œé‡åˆ†é…
                        await self.execute_reallocation(symbol, timeframe, trigger)
                        
                except Exception as e:
                    logger.error(f"âŒ æª¢æŸ¥ {symbol} {timeframe} è§¸ç™¼æ¢ä»¶å¤±æ•—: {e}")
    
    async def _get_performance_metrics(self, symbol: str, timeframe: str) -> Optional[PerformanceMetrics]:
        """ç²å–æ€§èƒ½æŒ‡æ¨™ (æ¨¡æ“¬å¯¦ç¾)"""
        # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒå¾æ•¸æ“šåº«æˆ–äº¤æ˜“å¼•æ“ç²å–çœŸå¯¦çš„æ€§èƒ½æ•¸æ“š
        import random
        
        metrics = PerformanceMetrics(
            symbol=symbol,
            timeframe=timeframe,
            total_return=random.uniform(-0.1, 0.15),
            sharpe_ratio=random.uniform(0.5, 2.5),
            max_drawdown=random.uniform(-0.20, -0.05),
            win_rate=random.uniform(0.45, 0.75),
            volatility=random.uniform(0.15, 0.45),
            var_95=random.uniform(-0.08, -0.02),
            total_trades=random.randint(10, 100),
            signal_accuracy=random.uniform(0.6, 0.9),
            false_positive_rate=random.uniform(0.05, 0.25)
        )
        
        # å­˜å„²æ­·å²æ•¸æ“š
        key = f"{symbol}_{timeframe}"
        self.performance_history[key].append(metrics)
        
        return metrics
    
    async def _evaluate_triggers(self, 
                               symbol: str, 
                               timeframe: str, 
                               current_metrics: PerformanceMetrics) -> Optional[ReallocationTrigger]:
        """è©•ä¼°æ˜¯å¦è§¸ç™¼é‡åˆ†é…"""
        key = f"{symbol}_{timeframe}"
        history = self.performance_history[key]
        
        if len(history) < 2:
            return None
        
        # ç²å–æ­·å²åŸºæº– - ä¿®å¾© deque åˆ‡ç‰‡å•é¡Œ
        history_list = list(history)[:-1]  # è½‰æ›ç‚ºåˆ—è¡¨å†é€²è¡Œåˆ‡ç‰‡
        historical_avg = self._calculate_historical_average(history_list)
        
        # æª¢æŸ¥æ€§èƒ½ä¸‹é™
        if current_metrics.total_return < historical_avg.total_return + self.trigger_thresholds["performance_degradation"]:
            return ReallocationTrigger.PERFORMANCE_DEGRADATION
        
        # æª¢æŸ¥ä¿¡è™Ÿå“è³ªè®ŠåŒ–
        if current_metrics.signal_accuracy < historical_avg.signal_accuracy + self.trigger_thresholds["signal_quality_change"]:
            return ReallocationTrigger.SIGNAL_QUALITY_CHANGE
        
        # æª¢æŸ¥æ³¢å‹•ç‡çªå¢
        if current_metrics.volatility > historical_avg.volatility * self.trigger_thresholds["volatility_spike"]:
            return ReallocationTrigger.VOLATILITY_SPIKE
        
        # æª¢æŸ¥é¢¨éšªé–¾å€¼çªç ´
        if abs(current_metrics.var_95) > abs(historical_avg.var_95) + self.trigger_thresholds["risk_threshold_breach"]:
            return ReallocationTrigger.RISK_THRESHOLD_BREACH
        
        return None
    
    def _calculate_historical_average(self, metrics_list: List[PerformanceMetrics]) -> PerformanceMetrics:
        """è¨ˆç®—æ­·å²å¹³å‡å€¼"""
        if not metrics_list:
            return PerformanceMetrics("", "")
        
        avg_metrics = PerformanceMetrics(
            symbol=metrics_list[0].symbol,
            timeframe=metrics_list[0].timeframe
        )
        
        # è¨ˆç®—å„é …æŒ‡æ¨™çš„å¹³å‡å€¼
        n = len(metrics_list)
        avg_metrics.total_return = sum(m.total_return for m in metrics_list) / n
        avg_metrics.sharpe_ratio = sum(m.sharpe_ratio for m in metrics_list) / n
        avg_metrics.max_drawdown = sum(m.max_drawdown for m in metrics_list) / n
        avg_metrics.win_rate = sum(m.win_rate for m in metrics_list) / n
        avg_metrics.volatility = sum(m.volatility for m in metrics_list) / n
        avg_metrics.var_95 = sum(m.var_95 for m in metrics_list) / n
        avg_metrics.signal_accuracy = sum(m.signal_accuracy for m in metrics_list) / n
        avg_metrics.false_positive_rate = sum(m.false_positive_rate for m in metrics_list) / n
        
        return avg_metrics
    
    async def execute_reallocation(self, 
                                 symbol: str, 
                                 timeframe: str, 
                                 trigger: ReallocationTrigger,
                                 current_weights: Dict[str, float] = None) -> Optional[WeightOptimizationResult]:
        """åŸ·è¡Œæ¬Šé‡é‡åˆ†é…"""
        try:
            # ç²å–ç•¶å‰æ¬Šé‡ (å¦‚æœæœªæä¾›)
            if current_weights is None:
                current_weights = await self._get_current_weights(symbol, timeframe)
            
            # ç²å–æ€§èƒ½æ•¸æ“š
            performance_data = await self._get_performance_metrics(symbol, timeframe)
            if not performance_data:
                logger.error(f"âŒ ç„¡æ³•ç²å– {symbol} {timeframe} çš„æ€§èƒ½æ•¸æ“š")
                return None
            
            # é¸æ“‡å„ªåŒ–æ–¹æ³•
            optimization_method = self._select_optimization_method(trigger, performance_data)
            
            # åŸ·è¡Œæ¬Šé‡å„ªåŒ–
            optimization_result = await self._optimize_weights(
                symbol, timeframe, current_weights, performance_data, optimization_method
            )
            
            if not optimization_result:
                logger.warning(f"âš ï¸ {symbol} {timeframe} æ¬Šé‡å„ªåŒ–å¤±æ•—")
                return None
            
            # é©—è­‰å„ªåŒ–çµæœ
            if optimization_result.expected_improvement < self.optimization_params["improvement_threshold"]:
                logger.info(f"ğŸ“Š {symbol} {timeframe} å„ªåŒ–æ”¹å–„ä¸è¶³ï¼Œè·³éé‡åˆ†é…")
                return optimization_result
            
            # è¨˜éŒ„é‡åˆ†é…äº‹ä»¶
            event_id = f"{symbol}_{timeframe}_{int(datetime.now().timestamp())}"
            reallocation_event = ReallocationEvent(
                event_id=event_id,
                trigger=trigger,
                symbol=symbol,
                timeframe=timeframe,
                before_weights=current_weights,
                after_weights=optimization_result.optimized_weights,
                expected_impact=optimization_result.expected_improvement,
                trigger_data={
                    "performance_metrics": performance_data.__dict__,
                    "optimization_method": optimization_method.value
                }
            )
            
            self.reallocation_history.append(reallocation_event)
            
            # æ›´æ–°çµ±è¨ˆæ•¸æ“š
            self.stats["total_reallocations"] += 1
            self.stats["last_reallocation"] = datetime.now()
            
            logger.info(f"âœ… å®Œæˆæ¬Šé‡é‡åˆ†é…: {symbol} {timeframe} (é æœŸæ”¹å–„: {optimization_result.expected_improvement:.2%})")
            
            return optimization_result
            
        except Exception as e:
            logger.error(f"âŒ åŸ·è¡Œé‡åˆ†é…å¤±æ•—: {symbol} {timeframe} - {e}")
            return None
    
    async def _get_current_weights(self, symbol: str, timeframe: str) -> Dict[str, float]:
        """ç²å–ç•¶å‰æ¬Šé‡ (æ¨¡æ“¬å¯¦ç¾)"""
        # åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒå¾æ¬Šé‡å¼•æ“ç²å–ç•¶å‰æ¬Šé‡
        return {
            "precision_filter_weight": 0.25,
            "market_condition_weight": 0.20,
            "technical_analysis_weight": 0.20,
            "regime_analysis_weight": 0.15,
            "fear_greed_weight": 0.10,
            "trend_alignment_weight": 0.05,
            "market_depth_weight": 0.03,
            "funding_rate_weight": 0.01,
            "smart_money_weight": 0.01
        }
    
    def _select_optimization_method(self, 
                                  trigger: ReallocationTrigger, 
                                  performance_data: PerformanceMetrics) -> OptimizationMethod:
        """é¸æ“‡å„ªåŒ–æ–¹æ³•"""
        # æ ¹æ“šè§¸ç™¼æ¢ä»¶å’Œæ€§èƒ½ç‰¹å¾µé¸æ“‡æœ€é©åˆçš„å„ªåŒ–æ–¹æ³•
        
        if trigger == ReallocationTrigger.PERFORMANCE_DEGRADATION:
            # æ€§èƒ½ä¸‹é™æ™‚ä½¿ç”¨æ¢¯åº¦ä¸Šå‡å¿«é€Ÿèª¿æ•´
            return OptimizationMethod.GRADIENT_ASCENT
        
        elif trigger == ReallocationTrigger.VOLATILITY_SPIKE:
            # æ³¢å‹•ç‡çªå¢æ™‚ä½¿ç”¨è‡ªé©æ‡‰å‹•é‡
            return OptimizationMethod.ADAPTIVE_MOMENTUM
        
        elif trigger == ReallocationTrigger.MARKET_REGIME_SHIFT:
            # å¸‚å ´åˆ¶åº¦è½‰æ›æ™‚ä½¿ç”¨éºå‚³ç®—æ³•æ¢ç´¢æ–°ç©ºé–“
            return OptimizationMethod.GENETIC_ALGORITHM
        
        elif trigger == ReallocationTrigger.SIGNAL_QUALITY_CHANGE:
            # ä¿¡è™Ÿå“è³ªè®ŠåŒ–æ™‚ä½¿ç”¨ç²’å­ç¾¤å„ªåŒ–
            return OptimizationMethod.PARTICLE_SWARM
        
        else:
            # é è¨­ä½¿ç”¨è²è‘‰æ–¯å„ªåŒ–
            return OptimizationMethod.BAYESIAN_OPTIMIZATION
    
    async def _optimize_weights(self, 
                              symbol: str,
                              timeframe: str,
                              current_weights: Dict[str, float],
                              performance_data: PerformanceMetrics,
                              method: OptimizationMethod) -> Optional[WeightOptimizationResult]:
        """æ¬Šé‡å„ªåŒ–æ ¸å¿ƒç®—æ³•"""
        
        # å°‡æ¬Šé‡è½‰æ›ç‚ºå‘é‡
        weight_names = list(current_weights.keys())
        current_vector = np.array([current_weights[name] for name in weight_names])
        
        # æ ¹æ“šæ–¹æ³•åŸ·è¡Œå„ªåŒ–
        if method == OptimizationMethod.GRADIENT_ASCENT:
            optimized_vector, iterations, converged = await self._gradient_ascent_optimization(
                current_vector, performance_data
            )
        
        elif method == OptimizationMethod.ADAPTIVE_MOMENTUM:
            optimized_vector, iterations, converged = await self._adaptive_momentum_optimization(
                current_vector, performance_data
            )
        
        else:
            # ç°¡åŒ–å¯¦ç¾ï¼šä½¿ç”¨æ¢¯åº¦ä¸Šå‡ä½œç‚ºé è¨­
            optimized_vector, iterations, converged = await self._gradient_ascent_optimization(
                current_vector, performance_data
            )
        
        # ç¢ºä¿æ¬Šé‡å’Œç‚º1
        optimized_vector = optimized_vector / np.sum(optimized_vector)
        
        # è½‰æ›å›å­—å…¸æ ¼å¼
        optimized_weights = {
            name: max(0.001, float(weight))  # ç¢ºä¿æœ€å°æ¬Šé‡
            for name, weight in zip(weight_names, optimized_vector)
        }
        
        # è¨ˆç®—é æœŸæ”¹å–„
        current_score = self._calculate_objective_function(current_vector, performance_data)
        optimized_score = self._calculate_objective_function(optimized_vector, performance_data)
        expected_improvement = (optimized_score - current_score) / abs(current_score) if current_score != 0 else 0
        
        # è¨ˆç®—ä¿¡å¿ƒåº¦
        confidence_score = min(1.0, max(0.0, 0.5 + expected_improvement))
        
        return WeightOptimizationResult(
            original_weights=current_weights,
            optimized_weights=optimized_weights,
            expected_improvement=expected_improvement,
            confidence_score=confidence_score,
            optimization_method=method,
            iterations=iterations,
            convergence_achieved=converged,
            performance_projection=performance_data,  # ç°¡åŒ–å¯¦ç¾
            risk_assessment={"overall_risk": performance_data.volatility},
            sensitivity_analysis={name: abs(optimized_weights[name] - current_weights[name]) 
                                for name in weight_names},
            explanation=f"ä½¿ç”¨ {method.value} æ–¹æ³•å„ªåŒ–ï¼Œç¶“é {iterations} æ¬¡è¿­ä»£"
        )
    
    async def _gradient_ascent_optimization(self, 
                                          current_weights: np.ndarray,
                                          performance_data: PerformanceMetrics) -> Tuple[np.ndarray, int, bool]:
        """æ¢¯åº¦ä¸Šå‡å„ªåŒ–"""
        weights = current_weights.copy()
        learning_rate = self.optimization_params["learning_rate"]
        max_iterations = self.optimization_params["max_iterations"]
        convergence_threshold = self.optimization_params["convergence_threshold"]
        
        for iteration in range(max_iterations):
            # è¨ˆç®—æ¢¯åº¦ (æ•¸å€¼æ¢¯åº¦)
            gradient = np.zeros_like(weights)
            epsilon = 1e-6
            
            current_score = self._calculate_objective_function(weights, performance_data)
            
            for i in range(len(weights)):
                weights_plus = weights.copy()
                weights_plus[i] += epsilon
                weights_plus = weights_plus / np.sum(weights_plus)  # é‡æ–°æ¨™æº–åŒ–
                
                score_plus = self._calculate_objective_function(weights_plus, performance_data)
                gradient[i] = (score_plus - current_score) / epsilon
            
            # æ›´æ–°æ¬Šé‡
            old_weights = weights.copy()
            weights += learning_rate * gradient
            
            # ç¢ºä¿æ¬Šé‡ç‚ºæ­£ä¸”å’Œç‚º1
            weights = np.maximum(weights, 0.001)
            weights = weights / np.sum(weights)
            
            # æª¢æŸ¥æ”¶æ–‚
            if np.linalg.norm(weights - old_weights) < convergence_threshold:
                return weights, iteration + 1, True
        
        return weights, max_iterations, False
    
    async def _adaptive_momentum_optimization(self, 
                                            current_weights: np.ndarray,
                                            performance_data: PerformanceMetrics) -> Tuple[np.ndarray, int, bool]:
        """è‡ªé©æ‡‰å‹•é‡å„ªåŒ–"""
        weights = current_weights.copy()
        velocity = np.zeros_like(weights)
        
        learning_rate = self.optimization_params["learning_rate"]
        momentum = self.optimization_params["momentum"]
        max_iterations = self.optimization_params["max_iterations"]
        convergence_threshold = self.optimization_params["convergence_threshold"]
        
        for iteration in range(max_iterations):
            # è¨ˆç®—æ¢¯åº¦
            gradient = np.zeros_like(weights)
            epsilon = 1e-6
            current_score = self._calculate_objective_function(weights, performance_data)
            
            for i in range(len(weights)):
                weights_plus = weights.copy()
                weights_plus[i] += epsilon
                weights_plus = weights_plus / np.sum(weights_plus)
                
                score_plus = self._calculate_objective_function(weights_plus, performance_data)
                gradient[i] = (score_plus - current_score) / epsilon
            
            # æ›´æ–°é€Ÿåº¦å’Œæ¬Šé‡
            velocity = momentum * velocity + learning_rate * gradient
            old_weights = weights.copy()
            weights += velocity
            
            # ç¢ºä¿æ¬Šé‡ç‚ºæ­£ä¸”å’Œç‚º1
            weights = np.maximum(weights, 0.001)
            weights = weights / np.sum(weights)
            
            # æª¢æŸ¥æ”¶æ–‚
            if np.linalg.norm(weights - old_weights) < convergence_threshold:
                return weights, iteration + 1, True
        
        return weights, max_iterations, False
    
    def _calculate_objective_function(self, 
                                    weights: np.ndarray, 
                                    performance_data: PerformanceMetrics) -> float:
        """è¨ˆç®—ç›®æ¨™å‡½æ•¸ (é¢¨éšªèª¿æ•´æ”¶ç›Š)"""
        # ç°¡åŒ–çš„ç›®æ¨™å‡½æ•¸ï¼šå¤æ™®æ¯”ç‡ + å‹ç‡ - é¢¨éšªæ‡²ç½°
        risk_penalty = self.optimization_params["risk_penalty"]
        
        # åŸºç¤åˆ†æ•¸ï¼šå¤æ™®æ¯”ç‡å’Œå‹ç‡çš„åŠ æ¬Šçµ„åˆ
        base_score = (
            performance_data.sharpe_ratio * 0.4 +
            performance_data.win_rate * 0.3 +
            performance_data.signal_accuracy * 0.2 +
            (1.0 - performance_data.false_positive_rate) * 0.1
        )
        
        # é¢¨éšªæ‡²ç½°ï¼šåŸºæ–¼æ³¢å‹•ç‡å’Œæœ€å¤§å›æ’¤
        risk_score = (
            performance_data.volatility * 0.6 +
            abs(performance_data.max_drawdown) * 0.4
        )
        
        # æ¬Šé‡åˆ†æ•£åº¦çå‹µ (é¿å…éåº¦é›†ä¸­)
        entropy = -np.sum(weights * np.log(weights + 1e-10))
        diversity_bonus = entropy / np.log(len(weights)) * 0.1
        
        final_score = base_score - risk_penalty * risk_score + diversity_bonus
        
        return final_score
    
    def get_reallocation_history(self, hours_back: int = 168) -> List[ReallocationEvent]:
        """ç²å–é‡åˆ†é…æ­·å²"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        return [
            event for event in self.reallocation_history
            if event.event_time >= cutoff_time
        ]
    
    def validate_reallocation_performance(self, 
                                        event_id: str, 
                                        actual_performance: PerformanceMetrics) -> bool:
        """é©—è­‰é‡åˆ†é…æ€§èƒ½"""
        # æ‰¾åˆ°å°æ‡‰çš„é‡åˆ†é…äº‹ä»¶
        target_event = None
        for event in self.reallocation_history:
            if event.event_id == event_id:
                target_event = event
                break
        
        if not target_event:
            logger.error(f"âŒ æ‰¾ä¸åˆ°é‡åˆ†é…äº‹ä»¶: {event_id}")
            return False
        
        # è¨ˆç®—å¯¦éš›æ”¹å–„
        # é€™è£¡éœ€è¦èˆ‡é‡åˆ†é…å‰çš„æ€§èƒ½é€²è¡Œæ¯”è¼ƒ
        # ç°¡åŒ–å¯¦ç¾ï¼šå‡è¨­æˆ‘å€‘æœ‰åŸºæº–æ€§èƒ½
        baseline_return = 0.0  # æ‡‰è©²å¾æ­·å²æ•¸æ“šç²å–
        actual_improvement = actual_performance.total_return - baseline_return
        
        target_event.actual_impact = actual_improvement
        target_event.validation_time = datetime.now()
        
        # æ›´æ–°çµ±è¨ˆæ•¸æ“š
        if actual_improvement > 0:
            self.stats["successful_reallocations"] += 1
            self.stats["total_improvement"] += actual_improvement
            self.stats["avg_improvement"] = self.stats["total_improvement"] / self.stats["successful_reallocations"]
        
        success = actual_improvement >= target_event.expected_impact * 0.5  # è‡³å°‘é”åˆ°50%é æœŸ
        
        if success:
            logger.info(f"âœ… é‡åˆ†é…é©—è­‰æˆåŠŸ: {event_id} (å¯¦éš›æ”¹å–„: {actual_improvement:.2%})")
        else:
            logger.warning(f"âš ï¸ é‡åˆ†é…æ•ˆæœä¸å¦‚é æœŸ: {event_id} (å¯¦éš›: {actual_improvement:.2%}, é æœŸ: {target_event.expected_impact:.2%})")
        
        return success
    
    def export_engine_status(self) -> Dict:
        """å°å‡ºå¼•æ“ç‹€æ…‹"""
        return {
            "is_monitoring": self.is_monitoring,
            "stats": self.stats,
            "optimization_params": self.optimization_params,
            "trigger_thresholds": self.trigger_thresholds,
            "recent_reallocations": [
                {
                    "event_id": event.event_id,
                    "trigger": event.trigger.value,
                    "symbol": event.symbol,
                    "timeframe": event.timeframe,
                    "expected_impact": event.expected_impact,
                    "actual_impact": event.actual_impact,
                    "event_time": event.event_time.isoformat()
                }
                for event in list(self.reallocation_history)[-10:]
            ],
            "performance_tracking": {
                key: len(history) for key, history in self.performance_history.items()
            },
            "export_time": datetime.now().isoformat()
        }

# å…¨å±€å¯¦ä¾‹
dynamic_reallocation_engine = DynamicReallocationEngine()
