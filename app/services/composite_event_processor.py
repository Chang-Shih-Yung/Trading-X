"""
è¤‡åˆäº‹ä»¶è™•ç†å™¨ - Trading X Phase 3 Week 1
è™•ç†å¤šå€‹åŒæ™‚ç™¼ç”Ÿæˆ–ç›¸é—œè¯çš„è¤‡é›œäº‹ä»¶ç³»çµ±
"""

import logging
import asyncio
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import json
from collections import defaultdict, deque
import networkx as nx

logger = logging.getLogger(__name__)

class EventRelationType(Enum):
    """äº‹ä»¶é—œè¯é¡å‹"""
    CAUSAL = "causal"              # å› æœé—œä¿‚
    CORRELATED = "correlated"      # ç›¸é—œé—œä¿‚  
    CONFLICTING = "conflicting"    # è¡çªé—œä¿‚
    REINFORCING = "reinforcing"    # å¢å¼·é—œä¿‚
    SEQUENTIAL = "sequential"      # åºåˆ—é—œä¿‚
    INDEPENDENT = "independent"    # ç¨ç«‹é—œä¿‚

class CompositePriority(Enum):
    """è¤‡åˆäº‹ä»¶å„ªå…ˆç´š"""
    CRITICAL = "critical"          # é—œéµç´š
    HIGH = "high"                 # é«˜ç´š
    MEDIUM = "medium"             # ä¸­ç´š  
    LOW = "low"                   # ä½ç´š
    MONITORING = "monitoring"      # ç›£æ§ç´š

@dataclass
class EventRelation:
    """äº‹ä»¶é—œè¯"""
    source_event_id: str
    target_event_id: str
    relation_type: EventRelationType
    correlation_strength: float    # é—œè¯å¼·åº¦ (0.0-1.0)
    time_lag_hours: float         # æ™‚é–“æ»¯å¾Œï¼ˆå°æ™‚ï¼‰
    confidence: float             # é—œè¯ä¿¡å¿ƒåº¦
    historical_validation_count: int  # æ­·å²é©—è­‰æ¬¡æ•¸
    last_observed: datetime

@dataclass
class CompositeEvent:
    """è¤‡åˆäº‹ä»¶"""
    composite_id: str
    component_event_ids: List[str]
    event_relations: List[EventRelation]
    composite_priority: CompositePriority
    aggregate_confidence: float
    composite_impact_magnitude: float
    expected_start_time: datetime
    expected_duration_hours: float
    affected_symbols: List[str]
    dominant_event_category: str
    conflict_resolution_strategy: str
    composite_timestamp: datetime

@dataclass
class EventChain:
    """äº‹ä»¶éˆ"""
    chain_id: str
    event_sequence: List[str]     # äº‹ä»¶IDåºåˆ—
    chain_confidence: float
    total_expected_duration: float
    chain_impact_profile: Dict[str, float]  # å„éšæ®µå½±éŸ¿
    trigger_conditions: Dict[str, Any]
    chain_completion_probability: float
    created_timestamp: datetime

@dataclass
class ConflictResolution:
    """è¡çªè§£æ±ºçµæœ"""
    conflict_id: str
    conflicting_event_ids: List[str]
    resolution_strategy: str
    resolved_weights: Dict[str, float]  # å„äº‹ä»¶çš„è§£æ±ºæ¬Šé‡
    resolution_confidence: float
    impact_adjustment: float
    resolution_timestamp: datetime

class CompositeEventProcessor:
    """è¤‡åˆäº‹ä»¶è™•ç†å™¨"""
    
    def __init__(self):
        # äº‹ä»¶é—œè¯ç¶²è·¯
        self.event_network = nx.DiGraph()
        
        # è³‡æ–™å­˜å„²
        self.active_events = {}           # ç•¶å‰æ´»èºäº‹ä»¶
        self.composite_events = {}        # è¤‡åˆäº‹ä»¶
        self.event_chains = {}           # äº‹ä»¶éˆ
        self.conflict_resolutions = {}   # è¡çªè§£æ±ºè¨˜éŒ„
        self.relation_database = {}      # é—œè¯æ•¸æ“šåº«
        
        # è™•ç†æ­·å²
        self.processing_history = deque(maxlen=500)
        
        # é…ç½®
        self.config = {
            "min_correlation_threshold": 0.3,
            "max_composite_events": 10,
            "conflict_resolution_timeout": 300,  # 5åˆ†é˜
            "chain_max_length": 8,
            "relation_decay_hours": 168,  # 7å¤©
            "composite_confidence_threshold": 0.4,
            "priority_weights": {
                CompositePriority.CRITICAL: 1.0,
                CompositePriority.HIGH: 0.8, 
                CompositePriority.MEDIUM: 0.6,
                CompositePriority.LOW: 0.4,
                CompositePriority.MONITORING: 0.2
            }
        }
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            "total_composite_events": 0,
            "active_composite_events": 0,
            "total_conflicts_resolved": 0,
            "event_chains_detected": 0,
            "avg_composite_accuracy": 0.0,
            "relation_learning_count": 0,
            "processing_efficiency": 0.0
        }
        
        # åˆå§‹åŒ–åŸºç¤é—œè¯
        self._initialize_base_relations()
        logger.info("âœ… è¤‡åˆäº‹ä»¶è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_base_relations(self):
        """åˆå§‹åŒ–åŸºç¤äº‹ä»¶é—œè¯"""
        base_relations = [
            # FOMCæœƒè­° â†’ å¸‚å ´æ³¢å‹•
            EventRelation(
                source_event_id="fomc_meeting",
                target_event_id="volatility_spike",
                relation_type=EventRelationType.CAUSAL,
                correlation_strength=0.85,
                time_lag_hours=2.0,
                confidence=0.78,
                historical_validation_count=24,
                last_observed=datetime.now() - timedelta(days=30)
            ),
            
            # æŠ€è¡“çªç ´ â†’ æˆäº¤é‡æ¿€å¢
            EventRelation(
                source_event_id="technical_breakout",
                target_event_id="volume_spike",
                relation_type=EventRelationType.CAUSAL,
                correlation_strength=0.72,
                time_lag_hours=1.0,
                confidence=0.68,
                historical_validation_count=89,
                last_observed=datetime.now() - timedelta(days=7)
            ),
            
            # å®è§€äº‹ä»¶ â†” æµå‹•æ€§è®ŠåŒ– (ç›¸é—œ)
            EventRelation(
                source_event_id="macro_economic_event",
                target_event_id="liquidity_change",
                relation_type=EventRelationType.CORRELATED,
                correlation_strength=0.65,
                time_lag_hours=4.0,
                confidence=0.71,
                historical_validation_count=156,
                last_observed=datetime.now() - timedelta(days=14)
            ),
            
            # ç‰›å¸‚ä¿¡è™Ÿ â†” ç†Šå¸‚ä¿¡è™Ÿ (è¡çª)
            EventRelation(
                source_event_id="bullish_signal",
                target_event_id="bearish_signal",
                relation_type=EventRelationType.CONFLICTING,
                correlation_strength=0.90,
                time_lag_hours=0.0,
                confidence=0.95,
                historical_validation_count=234,
                last_observed=datetime.now() - timedelta(days=2)
            ),
            
            # æˆäº¤é‡ç•°å¸¸ â†’ åƒ¹æ ¼çªç ´ (åºåˆ—)
            EventRelation(
                source_event_id="volume_anomaly",
                target_event_id="price_breakout",
                relation_type=EventRelationType.SEQUENTIAL,
                correlation_strength=0.58,
                time_lag_hours=6.0,
                confidence=0.62,
                historical_validation_count=78,
                last_observed=datetime.now() - timedelta(days=5)
            )
        ]
        
        for relation in base_relations:
            relation_key = f"{relation.source_event_id}_{relation.target_event_id}"
            self.relation_database[relation_key] = relation
            
            # æ·»åŠ åˆ°ç¶²è·¯åœ–
            self.event_network.add_edge(
                relation.source_event_id,
                relation.target_event_id,
                relation_type=relation.relation_type,
                weight=relation.correlation_strength
            )
        
        logger.info(f"ğŸ•¸ï¸ åˆå§‹åŒ– {len(base_relations)} å€‹åŸºç¤äº‹ä»¶é—œè¯")
    
    async def process_events(self, events: List[Dict]) -> List[CompositeEvent]:
        """è™•ç†äº‹ä»¶ä¸¦è­˜åˆ¥è¤‡åˆäº‹ä»¶"""
        try:
            # æ›´æ–°æ´»èºäº‹ä»¶
            self._update_active_events(events)
            
            # è­˜åˆ¥äº‹ä»¶é—œè¯
            detected_relations = await self._detect_event_relations()
            
            # æ§‹å»ºè¤‡åˆäº‹ä»¶
            composite_events = await self._build_composite_events(detected_relations)
            
            # è§£æ±ºè¡çª
            resolved_composites = await self._resolve_conflicts(composite_events)
            
            # æª¢æ¸¬äº‹ä»¶éˆ
            event_chains = await self._detect_event_chains()
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats["total_composite_events"] += len(resolved_composites)
            self.stats["active_composite_events"] = len(resolved_composites)
            
            logger.info(f"ğŸ”— è™•ç†å®Œæˆ: {len(resolved_composites)} å€‹è¤‡åˆäº‹ä»¶, {len(event_chains)} å€‹äº‹ä»¶éˆ")
            return resolved_composites
            
        except Exception as e:
            logger.error(f"âŒ è¤‡åˆäº‹ä»¶è™•ç†å¤±æ•—: {e}")
            return []
    
    def _update_active_events(self, events: List[Dict]):
        """æ›´æ–°æ´»èºäº‹ä»¶åˆ—è¡¨"""
        current_time = datetime.now()
        
        # æ·»åŠ æ–°äº‹ä»¶
        for event in events:
            event_id = event.get("event_id", f"event_{len(self.active_events)}")
            self.active_events[event_id] = {
                **event,
                "last_updated": current_time
            }
        
        # æ¸…ç†éæœŸäº‹ä»¶
        expired_events = []
        for event_id, event_data in self.active_events.items():
            last_updated = event_data.get("last_updated", current_time)
            if (current_time - last_updated).total_seconds() > 3600:  # 1å°æ™‚éæœŸ
                expired_events.append(event_id)
        
        for event_id in expired_events:
            del self.active_events[event_id]
    
    async def _detect_event_relations(self) -> List[EventRelation]:
        """æª¢æ¸¬äº‹ä»¶é–“é—œè¯"""
        try:
            detected_relations = []
            active_event_ids = list(self.active_events.keys())
            
            # æª¢æŸ¥æ¯å°äº‹ä»¶çš„é—œè¯
            for i, event_id_1 in enumerate(active_event_ids):
                for event_id_2 in active_event_ids[i+1:]:
                    relation = await self._analyze_event_pair(event_id_1, event_id_2)
                    if relation:
                        detected_relations.append(relation)
            
            # å­¸ç¿’æ–°é—œè¯
            await self._learn_new_relations(detected_relations)
            
            return detected_relations
            
        except Exception as e:
            logger.error(f"âŒ äº‹ä»¶é—œè¯æª¢æ¸¬å¤±æ•—: {e}")
            return []
    
    async def _analyze_event_pair(self, event_id_1: str, event_id_2: str) -> Optional[EventRelation]:
        """åˆ†æå…©å€‹äº‹ä»¶é–“çš„é—œè¯"""
        try:
            event_1 = self.active_events.get(event_id_1)
            event_2 = self.active_events.get(event_id_2)
            
            if not event_1 or not event_2:
                return None
            
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰å·²çŸ¥é—œè¯
            relation_key = f"{event_id_1}_{event_id_2}"
            reverse_key = f"{event_id_2}_{event_id_1}"
            
            if relation_key in self.relation_database:
                return self.relation_database[relation_key]
            elif reverse_key in self.relation_database:
                return self.relation_database[reverse_key]
            
            # è¨ˆç®—æ–°é—œè¯
            correlation_strength = self._calculate_correlation(event_1, event_2)
            
            if correlation_strength < self.config["min_correlation_threshold"]:
                return None
            
            # ç¢ºå®šé—œè¯é¡å‹
            relation_type = self._determine_relation_type(event_1, event_2)
            
            # è¨ˆç®—æ™‚é–“æ»¯å¾Œ
            time_lag = self._calculate_time_lag(event_1, event_2)
            
            # å‰µå»ºæ–°é—œè¯
            new_relation = EventRelation(
                source_event_id=event_id_1,
                target_event_id=event_id_2,
                relation_type=relation_type,
                correlation_strength=correlation_strength,
                time_lag_hours=time_lag,
                confidence=correlation_strength * 0.8,  # æ–°é—œè¯çš„ä¿¡å¿ƒåº¦è¼ƒä½
                historical_validation_count=1,
                last_observed=datetime.now()
            )
            
            return new_relation
            
        except Exception as e:
            logger.error(f"âŒ äº‹ä»¶å°åˆ†æå¤±æ•—: {e}")
            return None
    
    def _calculate_correlation(self, event_1: Dict, event_2: Dict) -> float:
        """è¨ˆç®—äº‹ä»¶ç›¸é—œæ€§"""
        try:
            # åŸºæ–¼äº‹ä»¶å±¬æ€§è¨ˆç®—ç›¸é—œæ€§
            correlation_factors = []
            
            # 1. æ™‚é–“ç›¸é—œæ€§
            time_1 = event_1.get("event_time", datetime.now())
            time_2 = event_2.get("event_time", datetime.now())
            if isinstance(time_1, str):
                time_1 = datetime.fromisoformat(time_1.replace('Z', '+00:00'))
            if isinstance(time_2, str):
                time_2 = datetime.fromisoformat(time_2.replace('Z', '+00:00'))
            
            time_diff_hours = abs((time_1 - time_2).total_seconds() / 3600)
            time_correlation = max(0.0, 1.0 - (time_diff_hours / 48))  # 48å°æ™‚å…§ç›¸é—œæ€§ç·šæ€§è¡°æ¸›
            correlation_factors.append(time_correlation * 0.3)
            
            # 2. å½±éŸ¿æ¨™çš„ç›¸é—œæ€§
            symbols_1 = set(event_1.get("affected_symbols", []))
            symbols_2 = set(event_2.get("affected_symbols", []))
            if symbols_1 or symbols_2:
                symbol_overlap = len(symbols_1.intersection(symbols_2)) / len(symbols_1.union(symbols_2))
                correlation_factors.append(symbol_overlap * 0.4)
            
            # 3. äº‹ä»¶é¡å‹ç›¸é—œæ€§
            category_1 = event_1.get("event_category", "unknown")
            category_2 = event_2.get("event_category", "unknown")
            category_correlation = 1.0 if category_1 == category_2 else 0.5
            correlation_factors.append(category_correlation * 0.2)
            
            # 4. å½±éŸ¿å¹…åº¦ç›¸é—œæ€§
            impact_1 = event_1.get("expected_impact_magnitude", 0.5)
            impact_2 = event_2.get("expected_impact_magnitude", 0.5)
            impact_correlation = 1.0 - abs(impact_1 - impact_2)
            correlation_factors.append(impact_correlation * 0.1)
            
            return sum(correlation_factors)
            
        except Exception as e:
            logger.error(f"âŒ ç›¸é—œæ€§è¨ˆç®—å¤±æ•—: {e}")
            return 0.0
    
    def _determine_relation_type(self, event_1: Dict, event_2: Dict) -> EventRelationType:
        """ç¢ºå®šé—œè¯é¡å‹"""
        try:
            # åŸºæ–¼äº‹ä»¶ç‰¹å¾µç¢ºå®šé—œè¯é¡å‹
            direction_1 = event_1.get("direction", "neutral")
            direction_2 = event_2.get("direction", "neutral")
            
            # æª¢æŸ¥è¡çªé—œä¿‚
            if (direction_1 == "bullish" and direction_2 == "bearish") or \
               (direction_1 == "bearish" and direction_2 == "bullish"):
                return EventRelationType.CONFLICTING
            
            # æª¢æŸ¥å¢å¼·é—œä¿‚
            if direction_1 == direction_2 and direction_1 != "neutral":
                return EventRelationType.REINFORCING
            
            # æª¢æŸ¥å› æœé—œä¿‚ (åŸºæ–¼äº‹ä»¶é¡å‹)
            category_1 = event_1.get("event_category", "")
            category_2 = event_2.get("event_category", "")
            
            causal_pairs = [
                ("macro_economic", "volatility_spike"),
                ("technical_breakout", "volume_anomaly"),
                ("news_event", "sentiment_change")
            ]
            
            for cause, effect in causal_pairs:
                if (cause in category_1 and effect in category_2) or \
                   (effect in category_1 and cause in category_2):
                    return EventRelationType.CAUSAL
            
            # é»˜èªç‚ºç›¸é—œé—œä¿‚
            return EventRelationType.CORRELATED
            
        except Exception as e:
            logger.error(f"âŒ é—œè¯é¡å‹ç¢ºå®šå¤±æ•—: {e}")
            return EventRelationType.CORRELATED
    
    def _calculate_time_lag(self, event_1: Dict, event_2: Dict) -> float:
        """è¨ˆç®—æ™‚é–“æ»¯å¾Œ"""
        try:
            time_1 = event_1.get("event_time", datetime.now())
            time_2 = event_2.get("event_time", datetime.now())
            
            if isinstance(time_1, str):
                time_1 = datetime.fromisoformat(time_1.replace('Z', '+00:00'))
            if isinstance(time_2, str):
                time_2 = datetime.fromisoformat(time_2.replace('Z', '+00:00'))
            
            time_diff = (time_2 - time_1).total_seconds() / 3600  # è½‰æ›ç‚ºå°æ™‚
            return abs(time_diff)
            
        except Exception as e:
            logger.error(f"âŒ æ™‚é–“æ»¯å¾Œè¨ˆç®—å¤±æ•—: {e}")
            return 0.0
    
    async def _learn_new_relations(self, detected_relations: List[EventRelation]):
        """å­¸ç¿’æ–°çš„äº‹ä»¶é—œè¯"""
        try:
            learned_count = 0
            
            for relation in detected_relations:
                relation_key = f"{relation.source_event_id}_{relation.target_event_id}"
                
                if relation_key not in self.relation_database:
                    # æ·»åŠ æ–°é—œè¯
                    self.relation_database[relation_key] = relation
                    
                    # æ›´æ–°ç¶²è·¯åœ–
                    self.event_network.add_edge(
                        relation.source_event_id,
                        relation.target_event_id,
                        relation_type=relation.relation_type,
                        weight=relation.correlation_strength
                    )
                    
                    learned_count += 1
                else:
                    # æ›´æ–°å·²æœ‰é—œè¯
                    existing_relation = self.relation_database[relation_key]
                    existing_relation.historical_validation_count += 1
                    existing_relation.last_observed = datetime.now()
                    
                    # èª¿æ•´ä¿¡å¿ƒåº¦
                    existing_relation.confidence = min(1.0, 
                        existing_relation.confidence * 0.9 + relation.confidence * 0.1
                    )
            
            self.stats["relation_learning_count"] += learned_count
            logger.info(f"ğŸ§  å­¸ç¿’ {learned_count} å€‹æ–°çš„äº‹ä»¶é—œè¯")
            
        except Exception as e:
            logger.error(f"âŒ é—œè¯å­¸ç¿’å¤±æ•—: {e}")
    
    async def _build_composite_events(self, relations: List[EventRelation]) -> List[CompositeEvent]:
        """æ§‹å»ºè¤‡åˆäº‹ä»¶"""
        try:
            composite_events = []
            processed_events = set()
            
            # æ ¹æ“šé—œè¯æ§‹å»ºè¤‡åˆäº‹ä»¶
            for relation in relations:
                if relation.source_event_id in processed_events or \
                   relation.target_event_id in processed_events:
                    continue
                
                # æ‰¾åˆ°ç›¸é—œçš„äº‹ä»¶çµ„
                event_group = self._find_connected_events(relation, relations)
                
                if len(event_group) >= 2:  # è‡³å°‘éœ€è¦2å€‹äº‹ä»¶
                    composite = await self._create_composite_event(event_group, relations)
                    if composite:
                        composite_events.append(composite)
                        processed_events.update(event_group)
            
            return composite_events
            
        except Exception as e:
            logger.error(f"âŒ è¤‡åˆäº‹ä»¶æ§‹å»ºå¤±æ•—: {e}")
            return []
    
    def _find_connected_events(self, seed_relation: EventRelation, 
                              all_relations: List[EventRelation]) -> Set[str]:
        """æ‰¾åˆ°é€£æ¥çš„äº‹ä»¶çµ„"""
        try:
            connected_events = {seed_relation.source_event_id, seed_relation.target_event_id}
            
            # å»£åº¦å„ªå…ˆæœç´¢æ‰¾åˆ°æ‰€æœ‰é€£æ¥çš„äº‹ä»¶
            to_explore = list(connected_events)
            explored = set()
            
            while to_explore:
                current_event = to_explore.pop(0)
                if current_event in explored:
                    continue
                explored.add(current_event)
                
                # æ‰¾åˆ°èˆ‡ç•¶å‰äº‹ä»¶ç›¸é—œçš„æ‰€æœ‰é—œè¯
                for relation in all_relations:
                    if relation.source_event_id == current_event:
                        if relation.target_event_id not in connected_events:
                            connected_events.add(relation.target_event_id)
                            to_explore.append(relation.target_event_id)
                    elif relation.target_event_id == current_event:
                        if relation.source_event_id not in connected_events:
                            connected_events.add(relation.source_event_id)
                            to_explore.append(relation.source_event_id)
            
            return connected_events
            
        except Exception as e:
            logger.error(f"âŒ é€£æ¥äº‹ä»¶æŸ¥æ‰¾å¤±æ•—: {e}")
            return set()
    
    async def _create_composite_event(self, event_group: Set[str], 
                                    relations: List[EventRelation]) -> Optional[CompositeEvent]:
        """å‰µå»ºè¤‡åˆäº‹ä»¶"""
        try:
            event_ids = list(event_group)
            
            # ç²å–ç›¸é—œé—œè¯
            relevant_relations = [
                r for r in relations 
                if r.source_event_id in event_group and r.target_event_id in event_group
            ]
            
            # è¨ˆç®—èšåˆä¿¡å¿ƒåº¦
            confidences = []
            for event_id in event_ids:
                event = self.active_events.get(event_id)
                if event:
                    confidences.append(event.get("confidence", 0.5))
            
            if not confidences:
                return None
            
            aggregate_confidence = np.mean(confidences)
            
            # æª¢æŸ¥ä¿¡å¿ƒåº¦é–¾å€¼
            if aggregate_confidence < self.config["composite_confidence_threshold"]:
                return None
            
            # è¨ˆç®—è¤‡åˆå½±éŸ¿å¹…åº¦
            impact_magnitudes = []
            for event_id in event_ids:
                event = self.active_events.get(event_id)
                if event:
                    impact_magnitudes.append(event.get("expected_impact_magnitude", 0.0))
            
            composite_impact = np.sqrt(np.sum(np.square(impact_magnitudes)))  # å‘é‡å’Œ
            composite_impact = min(1.0, composite_impact)
            
            # ç¢ºå®šä¸»å°äº‹ä»¶é¡åˆ¥
            categories = []
            for event_id in event_ids:
                event = self.active_events.get(event_id)
                if event:
                    categories.append(event.get("event_category", "unknown"))
            
            dominant_category = max(set(categories), key=categories.count) if categories else "composite"
            
            # è¨ˆç®—é æœŸé–‹å§‹æ™‚é–“
            start_times = []
            for event_id in event_ids:
                event = self.active_events.get(event_id)
                if event:
                    event_time = event.get("event_time", datetime.now())
                    if isinstance(event_time, str):
                        event_time = datetime.fromisoformat(event_time.replace('Z', '+00:00'))
                    start_times.append(event_time)
            
            expected_start = min(start_times) if start_times else datetime.now()
            
            # æ”¶é›†å½±éŸ¿æ¨™çš„
            all_symbols = set()
            for event_id in event_ids:
                event = self.active_events.get(event_id)
                if event:
                    all_symbols.update(event.get("affected_symbols", []))
            
            # ç¢ºå®šå„ªå…ˆç´š
            priority = self._determine_composite_priority(aggregate_confidence, composite_impact)
            
            # å‰µå»ºè¤‡åˆäº‹ä»¶
            composite = CompositeEvent(
                composite_id=f"composite_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(event_ids)}",
                component_event_ids=event_ids,
                event_relations=relevant_relations,
                composite_priority=priority,
                aggregate_confidence=aggregate_confidence,
                composite_impact_magnitude=composite_impact,
                expected_start_time=expected_start,
                expected_duration_hours=self._estimate_composite_duration(event_ids),
                affected_symbols=list(all_symbols),
                dominant_event_category=dominant_category,
                conflict_resolution_strategy="weighted_average",  # é»˜èªç­–ç•¥
                composite_timestamp=datetime.now()
            )
            
            return composite
            
        except Exception as e:
            logger.error(f"âŒ è¤‡åˆäº‹ä»¶å‰µå»ºå¤±æ•—: {e}")
            return None
    
    def _determine_composite_priority(self, confidence: float, impact: float) -> CompositePriority:
        """ç¢ºå®šè¤‡åˆäº‹ä»¶å„ªå…ˆç´š"""
        composite_score = (confidence * 0.6 + impact * 0.4)
        
        if composite_score >= 0.85:
            return CompositePriority.CRITICAL
        elif composite_score >= 0.7:
            return CompositePriority.HIGH
        elif composite_score >= 0.5:
            return CompositePriority.MEDIUM
        elif composite_score >= 0.3:
            return CompositePriority.LOW
        else:
            return CompositePriority.MONITORING
    
    def _estimate_composite_duration(self, event_ids: List[str]) -> float:
        """ä¼°ç®—è¤‡åˆäº‹ä»¶æŒçºŒæ™‚é–“"""
        try:
            durations = []
            for event_id in event_ids:
                event = self.active_events.get(event_id)
                if event:
                    duration = event.get("expected_duration_hours", 24.0)
                    durations.append(duration)
            
            if durations:
                # ä½¿ç”¨æœ€é•·æŒçºŒæ™‚é–“ä½œç‚ºè¤‡åˆäº‹ä»¶æŒçºŒæ™‚é–“
                return max(durations)
            else:
                return 24.0  # é»˜èª24å°æ™‚
                
        except Exception as e:
            logger.error(f"âŒ è¤‡åˆäº‹ä»¶æŒçºŒæ™‚é–“ä¼°ç®—å¤±æ•—: {e}")
            return 24.0
    
    async def _resolve_conflicts(self, composite_events: List[CompositeEvent]) -> List[CompositeEvent]:
        """è§£æ±ºè¡çªäº‹ä»¶"""
        try:
            resolved_events = []
            conflict_groups = self._identify_conflicts(composite_events)
            
            for conflict_group in conflict_groups:
                if len(conflict_group) > 1:
                    # æœ‰è¡çªéœ€è¦è§£æ±º
                    resolution = await self._resolve_conflict_group(conflict_group)
                    if resolution:
                        # æ‡‰ç”¨è¡çªè§£æ±º
                        resolved_event = self._apply_conflict_resolution(conflict_group, resolution)
                        if resolved_event:
                            resolved_events.append(resolved_event)
                else:
                    # ç„¡è¡çªï¼Œç›´æ¥æ·»åŠ 
                    resolved_events.append(conflict_group[0])
            
            return resolved_events
            
        except Exception as e:
            logger.error(f"âŒ è¡çªè§£æ±ºå¤±æ•—: {e}")
            return composite_events
    
    def _identify_conflicts(self, composite_events: List[CompositeEvent]) -> List[List[CompositeEvent]]:
        """è­˜åˆ¥è¡çªäº‹ä»¶çµ„"""
        try:
            conflict_groups = []
            processed = set()
            
            for i, event_1 in enumerate(composite_events):
                if i in processed:
                    continue
                
                conflict_group = [event_1]
                processed.add(i)
                
                for j, event_2 in enumerate(composite_events[i+1:], i+1):
                    if j in processed:
                        continue
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰è¡çªé—œè¯
                    has_conflict = False
                    for relation in event_1.event_relations:
                        if (relation.relation_type == EventRelationType.CONFLICTING and
                            (relation.source_event_id in event_2.component_event_ids or
                             relation.target_event_id in event_2.component_event_ids)):
                            has_conflict = True
                            break
                    
                    if has_conflict:
                        conflict_group.append(event_2)
                        processed.add(j)
                
                conflict_groups.append(conflict_group)
            
            return conflict_groups
            
        except Exception as e:
            logger.error(f"âŒ è¡çªè­˜åˆ¥å¤±æ•—: {e}")
            return [[event] for event in composite_events]
    
    async def _resolve_conflict_group(self, conflict_group: List[CompositeEvent]) -> Optional[ConflictResolution]:
        """è§£æ±ºè¡çªçµ„"""
        try:
            if len(conflict_group) <= 1:
                return None
            
            conflict_id = f"conflict_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            event_ids = [event.composite_id for event in conflict_group]
            
            # è¨ˆç®—æ¬Šé‡ (åŸºæ–¼ä¿¡å¿ƒåº¦å’Œå½±éŸ¿)
            weights = {}
            total_weight = 0.0
            
            for event in conflict_group:
                weight = (event.aggregate_confidence * 0.7 + 
                         event.composite_impact_magnitude * 0.3)
                weights[event.composite_id] = weight
                total_weight += weight
            
            # æ¨™æº–åŒ–æ¬Šé‡
            if total_weight > 0:
                for event_id in weights:
                    weights[event_id] /= total_weight
            
            # è¨ˆç®—è§£æ±ºä¿¡å¿ƒåº¦
            confidence_scores = [event.aggregate_confidence for event in conflict_group]
            resolution_confidence = np.mean(confidence_scores) * 0.8  # è¡çªé™ä½ä¿¡å¿ƒåº¦
            
            # è¨ˆç®—å½±éŸ¿èª¿æ•´
            impact_adjustment = 0.9  # è¡çªé€šå¸¸é™ä½æ•´é«”å½±éŸ¿
            
            resolution = ConflictResolution(
                conflict_id=conflict_id,
                conflicting_event_ids=event_ids,
                resolution_strategy="confidence_weighted",
                resolved_weights=weights,
                resolution_confidence=resolution_confidence,
                impact_adjustment=impact_adjustment,
                resolution_timestamp=datetime.now()
            )
            
            # ä¿å­˜è§£æ±ºè¨˜éŒ„
            self.conflict_resolutions[conflict_id] = resolution
            self.stats["total_conflicts_resolved"] += 1
            
            return resolution
            
        except Exception as e:
            logger.error(f"âŒ è¡çªçµ„è§£æ±ºå¤±æ•—: {e}")
            return None
    
    def _apply_conflict_resolution(self, conflict_group: List[CompositeEvent], 
                                 resolution: ConflictResolution) -> Optional[CompositeEvent]:
        """æ‡‰ç”¨è¡çªè§£æ±º"""
        try:
            if not conflict_group:
                return None
            
            # é¸æ“‡ä¸»å°äº‹ä»¶ (æ¬Šé‡æœ€é«˜çš„)
            dominant_event = max(conflict_group, 
                               key=lambda e: resolution.resolved_weights.get(e.composite_id, 0))
            
            # èª¿æ•´ä¸»å°äº‹ä»¶çš„å±¬æ€§
            resolved_event = CompositeEvent(
                composite_id=f"resolved_{dominant_event.composite_id}",
                component_event_ids=dominant_event.component_event_ids,
                event_relations=dominant_event.event_relations,
                composite_priority=dominant_event.composite_priority,
                aggregate_confidence=resolution.resolution_confidence,
                composite_impact_magnitude=dominant_event.composite_impact_magnitude * resolution.impact_adjustment,
                expected_start_time=dominant_event.expected_start_time,
                expected_duration_hours=dominant_event.expected_duration_hours,
                affected_symbols=dominant_event.affected_symbols,
                dominant_event_category=dominant_event.dominant_event_category,
                conflict_resolution_strategy=resolution.resolution_strategy,
                composite_timestamp=datetime.now()
            )
            
            return resolved_event
            
        except Exception as e:
            logger.error(f"âŒ è¡çªè§£æ±ºæ‡‰ç”¨å¤±æ•—: {e}")
            return None
    
    async def _detect_event_chains(self) -> List[EventChain]:
        """æª¢æ¸¬äº‹ä»¶éˆ"""
        try:
            event_chains = []
            
            # ä½¿ç”¨ç¶²è·¯åœ–æª¢æ¸¬è·¯å¾‘
            if len(self.event_network.nodes) < 2:
                return []
            
            # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„è·¯å¾‘
            nodes = list(self.event_network.nodes)
            for start_node in nodes:
                for end_node in nodes:
                    if start_node != end_node:
                        try:
                            # æ‰¾åˆ°æ‰€æœ‰ç°¡å–®è·¯å¾‘
                            paths = list(nx.all_simple_paths(
                                self.event_network, 
                                start_node, 
                                end_node, 
                                cutoff=self.config["chain_max_length"]
                            ))
                            
                            for path in paths:
                                if len(path) >= 3:  # è‡³å°‘3å€‹äº‹ä»¶æ‰çµ„æˆéˆ
                                    chain = await self._create_event_chain(path)
                                    if chain:
                                        event_chains.append(chain)
                                        
                        except nx.NetworkXNoPath:
                            continue
            
            # ä¿å­˜äº‹ä»¶éˆ
            for chain in event_chains:
                self.event_chains[chain.chain_id] = chain
            
            self.stats["event_chains_detected"] += len(event_chains)
            return event_chains
            
        except Exception as e:
            logger.error(f"âŒ äº‹ä»¶éˆæª¢æ¸¬å¤±æ•—: {e}")
            return []
    
    async def _create_event_chain(self, event_path: List[str]) -> Optional[EventChain]:
        """å‰µå»ºäº‹ä»¶éˆ"""
        try:
            if len(event_path) < 3:
                return None
            
            # è¨ˆç®—éˆçš„ä¿¡å¿ƒåº¦
            path_confidences = []
            for i in range(len(event_path) - 1):
                relation_key = f"{event_path[i]}_{event_path[i+1]}"
                if relation_key in self.relation_database:
                    relation = self.relation_database[relation_key]
                    path_confidences.append(relation.confidence)
            
            if not path_confidences:
                return None
            
            chain_confidence = np.prod(path_confidences) ** (1.0 / len(path_confidences))  # å¹¾ä½•å¹³å‡
            
            # è¨ˆç®—ç¸½æŒçºŒæ™‚é–“
            total_duration = 0.0
            for i in range(len(event_path) - 1):
                relation_key = f"{event_path[i]}_{event_path[i+1]}"
                if relation_key in self.relation_database:
                    relation = self.relation_database[relation_key]
                    total_duration += relation.time_lag_hours
            
            # è¨ˆç®—å®Œæˆæ¦‚ç‡
            completion_probability = chain_confidence * 0.8  # éˆè¶Šé•·å®Œæˆæ¦‚ç‡è¶Šä½
            
            chain = EventChain(
                chain_id=f"chain_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(event_path)}",
                event_sequence=event_path,
                chain_confidence=chain_confidence,
                total_expected_duration=total_duration,
                chain_impact_profile={f"stage_{i}": 1.0/len(event_path) for i in range(len(event_path))},
                trigger_conditions={"chain_start": event_path[0]},
                chain_completion_probability=completion_probability,
                created_timestamp=datetime.now()
            )
            
            return chain
            
        except Exception as e:
            logger.error(f"âŒ äº‹ä»¶éˆå‰µå»ºå¤±æ•—: {e}")
            return None
    
    def get_processing_summary(self) -> Dict[str, Any]:
        """ç²å–è™•ç†å™¨æ‘˜è¦"""
        try:
            return {
                "processor_status": "active",
                "active_events_count": len(self.active_events),
                "total_relations": len(self.relation_database),
                "active_composite_events": self.stats["active_composite_events"],
                "event_chains_active": len(self.event_chains),
                "conflicts_resolved_today": self.stats["total_conflicts_resolved"],
                "processing_stats": self.stats,
                "network_complexity": {
                    "nodes": len(self.event_network.nodes),
                    "edges": len(self.event_network.edges),
                    "density": nx.density(self.event_network) if len(self.event_network.nodes) > 1 else 0.0
                },
                "last_processing_time": datetime.now().isoformat(),
                "system_health": "good" if self.stats["avg_composite_accuracy"] > 0.6 else "needs_attention"
            }
            
        except Exception as e:
            logger.error(f"âŒ è™•ç†å™¨æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            return {"processor_status": "error", "error": str(e)}

# å…¨å±€å¯¦ä¾‹
composite_event_processor = CompositeEventProcessor()
