"""
æ—¥èªŒç®¡ç†å’Œæ¸…ç†å·¥å…·
æä¾›è‡ªå‹•æ¸…ç†èˆŠæ—¥èªŒæ–‡ä»¶çš„åŠŸèƒ½
"""

import os
import glob
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional
import shutil

logger = logging.getLogger(__name__)

class LogManager:
    """æ—¥èªŒç®¡ç†å™¨ - è² è²¬æ¸…ç†å’Œç¶­è­·æ—¥èªŒæ–‡ä»¶"""
    
    def __init__(self, 
                 log_dir: str = ".",
                 max_log_size_mb: int = 100,  # å–®å€‹æ—¥èªŒæ–‡ä»¶æœ€å¤§å¤§å° (MB)
                 max_backup_age_hours: int = 24,  # å‚™ä»½æ–‡ä»¶ä¿ç•™æ™‚é–“ (å°æ™‚)
                 max_backup_count: int = 10,  # æœ€å¤šä¿ç•™å‚™ä»½æ–‡ä»¶æ•¸é‡
                 cleanup_interval_hours: int = 1):  # æ¸…ç†é–“éš” (å°æ™‚)
        
        self.log_dir = Path(log_dir)
        self.max_log_size_bytes = max_log_size_mb * 1024 * 1024
        self.max_backup_age = timedelta(hours=max_backup_age_hours)
        self.max_backup_count = max_backup_count
        self.cleanup_interval = timedelta(hours=cleanup_interval_hours)
        
        self.running = False
        self.cleanup_task = None
        
        # ç¢ºä¿æ—¥èªŒç›®éŒ„å­˜åœ¨
        self.log_dir.mkdir(exist_ok=True)
        
    async def start_auto_cleanup(self):
        """å•Ÿå‹•è‡ªå‹•æ¸…ç†ä»»å‹™"""
        if self.running:
            logger.warning("æ—¥èªŒæ¸…ç†ä»»å‹™å·²åœ¨é‹è¡Œ")
            return
            
        self.running = True
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        logger.info(f"âœ… æ—¥èªŒè‡ªå‹•æ¸…ç†å·²å•Ÿå‹• - æ¯ {self.cleanup_interval.total_seconds()/3600:.1f} å°æ™‚åŸ·è¡Œä¸€æ¬¡")
        
    async def stop_auto_cleanup(self):
        """åœæ­¢è‡ªå‹•æ¸…ç†ä»»å‹™"""
        if not self.running:
            return
            
        self.running = False
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass
                
        logger.info("âœ… æ—¥èªŒè‡ªå‹•æ¸…ç†å·²åœæ­¢")
        
    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç’°"""
        while self.running:
            try:
                # åŸ·è¡Œæ¸…ç†
                await self.cleanup_logs()
                
                # ç­‰å¾…ä¸‹æ¬¡æ¸…ç†
                await asyncio.sleep(self.cleanup_interval.total_seconds())
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ—¥èªŒæ¸…ç†å¾ªç’°éŒ¯èª¤: {e}")
                # å‡ºéŒ¯æ™‚ç­‰å¾…è¼ƒçŸ­æ™‚é–“å†é‡è©¦
                await asyncio.sleep(300)  # 5åˆ†é˜
                
    async def cleanup_logs(self):
        """åŸ·è¡Œæ—¥èªŒæ¸…ç†"""
        try:
            cleaned_files = []
            freed_space = 0
            
            # 1. æª¢æŸ¥ä¸»æ—¥èªŒæ–‡ä»¶å¤§å°
            main_log_files = ["server.log", "app.log", "trading.log"]
            for log_name in main_log_files:
                log_path = self.log_dir / log_name
                if log_path.exists():
                    size_mb = log_path.stat().st_size / (1024 * 1024)
                    if log_path.stat().st_size > self.max_log_size_bytes:
                        backup_name = f"{log_name}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        backup_path = self.log_dir / backup_name
                        
                        # ç§»å‹•åˆ°å‚™ä»½
                        shutil.move(str(log_path), str(backup_path))
                        cleaned_files.append(f"è¼ªè½‰ {log_name} ({size_mb:.1f}MB)")
                        
                        logger.info(f"ğŸ“¦ æ—¥èªŒè¼ªè½‰: {log_name} -> {backup_name}")
            
            # 2. æ¸…ç†èˆŠçš„å‚™ä»½æ–‡ä»¶
            backup_patterns = [
                "*.log.backup_*",
                "server_backup_*.log",
                "app_backup_*.log",
                "*.log.*"
            ]
            
            current_time = datetime.now()
            
            for pattern in backup_patterns:
                backup_files = list(self.log_dir.glob(pattern))
                
                # æŒ‰ä¿®æ”¹æ™‚é–“æ’åº
                backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                
                for i, backup_file in enumerate(backup_files):
                    file_time = datetime.fromtimestamp(backup_file.stat().st_mtime)
                    file_age = current_time - file_time
                    file_size = backup_file.stat().st_size
                    
                    should_delete = False
                    reason = ""
                    
                    # æª¢æŸ¥å¹´é½¡
                    if file_age > self.max_backup_age:
                        should_delete = True
                        reason = f"éæœŸ ({file_age.total_seconds()/3600:.1f}h)"
                    
                    # æª¢æŸ¥æ•¸é‡é™åˆ¶
                    elif i >= self.max_backup_count:
                        should_delete = True
                        reason = f"è¶…å‡ºæ•¸é‡é™åˆ¶ (#{i+1})"
                    
                    if should_delete:
                        try:
                            backup_file.unlink()
                            freed_space += file_size
                            cleaned_files.append(f"åˆªé™¤ {backup_file.name} ({reason})")
                            logger.info(f"ğŸ—‘ï¸ æ¸…ç†å‚™ä»½: {backup_file.name} - {reason}")
                        except Exception as e:
                            logger.error(f"âŒ æ¸…ç†å¤±æ•— {backup_file.name}: {e}")
            
            # 3. æ¸…ç†ç©ºçš„æˆ–æå£çš„æ—¥èªŒæ–‡ä»¶
            all_log_files = list(self.log_dir.glob("*.log*"))
            for log_file in all_log_files:
                try:
                    if log_file.stat().st_size == 0:
                        log_file.unlink()
                        cleaned_files.append(f"åˆªé™¤ç©ºæ–‡ä»¶ {log_file.name}")
                        logger.info(f"ğŸ—‘ï¸ æ¸…ç†ç©ºæ–‡ä»¶: {log_file.name}")
                except Exception as e:
                    logger.error(f"âŒ æª¢æŸ¥æ–‡ä»¶å¤±æ•— {log_file.name}: {e}")
            
            # 4. å ±å‘Šæ¸…ç†çµæœ
            if cleaned_files:
                freed_mb = freed_space / (1024 * 1024)
                logger.info(f"ğŸ§¹ æ—¥èªŒæ¸…ç†å®Œæˆ:")
                logger.info(f"   - è™•ç†äº† {len(cleaned_files)} å€‹æ–‡ä»¶")
                logger.info(f"   - é‡‹æ”¾äº† {freed_mb:.1f} MB ç©ºé–“")
                for action in cleaned_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                    logger.info(f"   - {action}")
                if len(cleaned_files) > 5:
                    logger.info(f"   - ... é‚„æœ‰ {len(cleaned_files) - 5} å€‹æ–‡ä»¶")
            else:
                logger.debug("ğŸ§¹ æ—¥èªŒæ¸…ç†: æ²’æœ‰éœ€è¦æ¸…ç†çš„æ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"âŒ æ—¥èªŒæ¸…ç†å¤±æ•—: {e}")
            
    def get_log_statistics(self) -> dict:
        """ç²å–æ—¥èªŒçµ±è¨ˆä¿¡æ¯"""
        try:
            stats = {
                "main_logs": {},
                "backup_logs": [],
                "total_size_mb": 0,
                "oldest_backup": None,
                "newest_backup": None
            }
            
            # ä¸»æ—¥èªŒæ–‡ä»¶
            main_logs = ["server.log", "app.log", "trading.log"]
            for log_name in main_logs:
                log_path = self.log_dir / log_name
                if log_path.exists():
                    size_mb = log_path.stat().st_size / (1024 * 1024)
                    modified = datetime.fromtimestamp(log_path.stat().st_mtime)
                    stats["main_logs"][log_name] = {
                        "size_mb": round(size_mb, 2),
                        "modified": modified.isoformat(),
                        "needs_rotation": size_mb > (self.max_log_size_bytes / (1024 * 1024))
                    }
                    stats["total_size_mb"] += size_mb
            
            # å‚™ä»½æ–‡ä»¶
            backup_patterns = ["*.log.backup_*", "server_backup_*.log", "app_backup_*.log"]
            backup_files = []
            
            for pattern in backup_patterns:
                backup_files.extend(self.log_dir.glob(pattern))
            
            current_time = datetime.now()
            backup_times = []
            
            for backup_file in backup_files:
                size_mb = backup_file.stat().st_size / (1024 * 1024)
                modified = datetime.fromtimestamp(backup_file.stat().st_mtime)
                age_hours = (current_time - modified).total_seconds() / 3600
                
                backup_info = {
                    "name": backup_file.name,
                    "size_mb": round(size_mb, 2),
                    "modified": modified.isoformat(),
                    "age_hours": round(age_hours, 1),
                    "expired": age_hours > self.max_backup_age.total_seconds() / 3600
                }
                
                stats["backup_logs"].append(backup_info)
                stats["total_size_mb"] += size_mb
                backup_times.append(modified)
            
            if backup_times:
                stats["oldest_backup"] = min(backup_times).isoformat()
                stats["newest_backup"] = max(backup_times).isoformat()
            
            stats["total_size_mb"] = round(stats["total_size_mb"], 2)
            stats["backup_count"] = len(backup_files)
            
            return stats
            
        except Exception as e:
            logger.error(f"ç²å–æ—¥èªŒçµ±è¨ˆå¤±æ•—: {e}")
            return {"error": str(e)}
            
    async def force_cleanup(self) -> dict:
        """æ‰‹å‹•è§¸ç™¼æ¸…ç†ä¸¦è¿”å›çµæœ"""
        logger.info("ğŸ§¹ æ‰‹å‹•è§¸ç™¼æ—¥èªŒæ¸…ç†...")
        
        before_stats = self.get_log_statistics()
        await self.cleanup_logs()
        after_stats = self.get_log_statistics()
        
        return {
            "before": before_stats,
            "after": after_stats,
            "space_freed_mb": round(before_stats.get("total_size_mb", 0) - after_stats.get("total_size_mb", 0), 2)
        }

# å…¨å±€æ—¥èªŒç®¡ç†å™¨å¯¦ä¾‹
log_manager = LogManager(
    log_dir="/Users/henrychang/Desktop/Trading-X",
    max_log_size_mb=50,  # 50MB å¾Œè¼ªè½‰
    max_backup_age_hours=24,  # ä¿ç•™24å°æ™‚
    max_backup_count=5,  # æœ€å¤š5å€‹å‚™ä»½
    cleanup_interval_hours=1  # æ¯å°æ™‚æ¸…ç†
)

async def start_log_management():
    """å•Ÿå‹•æ—¥èªŒç®¡ç†"""
    await log_manager.start_auto_cleanup()

async def stop_log_management():
    """åœæ­¢æ—¥èªŒç®¡ç†"""
    await log_manager.stop_auto_cleanup()

def get_log_stats():
    """ç²å–æ—¥èªŒçµ±è¨ˆä¿¡æ¯"""
    return log_manager.get_log_statistics()

async def manual_cleanup():
    """æ‰‹å‹•æ¸…ç†æ—¥èªŒ"""
    return await log_manager.force_cleanup()
