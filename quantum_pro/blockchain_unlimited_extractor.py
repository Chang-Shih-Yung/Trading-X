#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡å­ç´šå€å¡Šéˆä¸»æ± æ­·å²æ•¸æ“šæ’ˆå–å™¨ï¼šç›®å‰æš«æ™‚åœç”¨ï¼Œå› ç‚ºæ’ˆå–å¤ªä¹…å¤ªåˆ†æ•£
åŸºæ–¼ phase1a_basic_signal_generation.py æˆåŠŸçš„BSCä¸»æ± å¯¦ç¾
ç›´æ¥å¾PancakeSwap V2/V3ä¸»æ± æ’ˆå–å‰µä¸–ä»¥ä¾†çš„æ‰€æœ‰æ­·å²æ•¸æ“š
ç„¡APIé™åˆ¶ï¼Œä½¿ç”¨å¤šç¯€é»å†—é¤˜ç¢ºä¿æ•¸æ“šå®Œæ•´æ€§
"""

import asyncio
import aiohttp
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

import pandas as pd
import numpy as np
from web3 import Web3
from eth_utils import to_checksum_address

# ğŸ­ ä½¿ç”¨èˆ‡ phase1a ç›¸åŒçš„BSCç”¢å“ç´šé…ç½®
class ProductionConfig:
    """èˆ‡ phase1a_basic_signal_generation.py ä¸€è‡´çš„ç”¢å“ç´šé…ç½®"""
    
    # ğŸ”— BSCä¸»æ± åˆç´„åœ°å€ï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰
    USDT_ADDRESS = "0x55d398326f99059fF775485246999027B3197955"
    PANCAKE_V2_FACTORY = "0xcA143Ce32Fe78f1f7019d7d551a6402fC5350c73"
    PANCAKE_V3_FACTORY = "0x1097053Fd2ea711dad45caCcc45EfF7548fCB362"
    
    # ğŸª™ ä¸ƒå¤§å¹£ç¨®ä»£å¹£åœ°å€ï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰
    TOKEN_ADDRESSES = {
        'BTCB': '0x7130d2A12B9BCbFAe4f2634d864A1Ee1Ce3Ead9c',    # Bitcoin BEP20
        'ETH': '0x2170Ed0880ac9A755fd29B2688956BD959F933F8',     # Ethereum Token
        'WBNB': '0xbb4CdB9CBd36B01bD1cBaEBF2De08d9173bc095c',   # Wrapped BNB
        'ADA': '0x3EE2200Efb3400fAbB9AacF31297cBdD1d435D47',     # Cardano Token
        'DOGE': '0xbA2aE424d960c26247Dd6c32edC70B295c744C43',    # Dogecoin Token
        'XRP': '0x1D2F0da169ceB9fC7B3144628dB156f3F6c60dBE',     # XRP Token
        'SOL': '0x570A5D26f7765Ecb712C0924E4De545B89fD43dF'      # Solana Token
    }
    
    # ğŸ”¢ ä»£å¹£å°æ•¸ä½æ•¸é…ç½®ï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰
    TOKEN_DECIMALS = {
        'BTCB': 18, 'ETH': 18, 'WBNB': 18, 'ADA': 18,
        'DOGE': 8, 'XRP': 18, 'SOL': 18, 'USDT': 18
    }
    
    # ğŸ“Š V3æ‰‹çºŒè²»ç­‰ç´šï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰
    V3_FEE_TIERS = [500, 3000, 10000, 100]
    
    # ğŸŒ BSCå¤šç¯€é»æ± ï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰
    BSC_RPC_NODES = [
        "https://bsc-dataseed.binance.org",
        "https://bsc-dataseed1.binance.org", 
        "https://bsc-dataseed2.binance.org",
        "https://bsc.publicnode.com",
        "https://rpc.ankr.com/bsc",
        "https://bsc-rpc.publicnode.com",
        "https://bsc.nodereal.io",
        "https://bsc-dataseed3.binance.org"
    ]
    
    # ğŸ¯ æ”¯æ´çš„å¹£ç¨®ï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰
    SUPPORTED_SYMBOLS = ['BTC', 'ETH', 'BNB', 'ADA', 'DOGE', 'XRP', 'SOL']
    
    # ğŸ“… çœŸå¯¦å‰µä¸–æ™‚é–“ vs BSCéƒ¨ç½²æ™‚é–“
    REAL_GENESIS_DATES = {
        'BTC': datetime(2009, 1, 3, 18, 15, 5),    # æ¯”ç‰¹å¹£å‰µä¸–å€å¡Šæ™‚é–“
        'ETH': datetime(2015, 7, 30, 15, 26, 13),  # ä»¥å¤ªåŠä¸»ç¶²å‰µä¸–
        'ADA': datetime(2017, 9, 29, 21, 44, 51),  # Cardanoä¸»ç¶²å•Ÿå‹•
        'XRP': datetime(2012, 6, 2, 0, 0, 0),      # XRPLå‰µä¸–ï¼ˆä¼°ç®—ï¼‰
        'DOGE': datetime(2013, 12, 6, 10, 25, 40), # ç‹—ç‹—å¹£å‰µä¸–å€å¡Š
        'SOL': datetime(2020, 3, 16, 0, 0, 0),     # Solanaä¸»ç¶²Beta
        'BNB': datetime(2017, 7, 8, 0, 0, 0)       # å¹£å®‰å¹£ICOé–‹å§‹
    }
    
    BSC_DEPLOYMENT_DATES = {
        'BTC': datetime(2020, 9, 1),    # BTCBåœ¨BSCéƒ¨ç½²
        'ETH': datetime(2020, 9, 1),    # ETHåœ¨BSCéƒ¨ç½²
        'BNB': datetime(2020, 8, 29),   # BSCä¸»ç¶²å•Ÿå‹•ï¼ˆåŸç”Ÿä»£å¹£ï¼‰
        'ADA': datetime(2021, 4, 1),    # ADAåœ¨BSCéƒ¨ç½²
        'DOGE': datetime(2021, 5, 1),   # DOGEåœ¨BSCéƒ¨ç½²
        'XRP': datetime(2021, 4, 1),    # XRPåœ¨BSCéƒ¨ç½²
        'SOL': datetime(2021, 9, 1)     # SOLåœ¨BSCéƒ¨ç½²
    }
    
    # ğŸŒ å¤šéˆæ•¸æ“šæºé…ç½®
    EXTERNAL_DATA_SOURCES = {
        'BTC': {
            'apis': [
                'https://api.coingecko.com/api/v3/coins/bitcoin/market_chart/range',
                'https://api.coinmarketcap.com/data-api/v3/cryptocurrency/detail/chart',
                'https://web-api.coinmarketcap.com/v1/cryptocurrency/ohlcv/historical',
                'https://api.cryptocompare.com/data/v2/histoday'
            ],
            'blockchain_apis': [
                'https://blockstream.info/api/blocks/tip/height',
                'https://api.blockchain.info/charts/market-price?format=json'
            ]
        },
        'ETH': {
            'apis': [
                'https://api.coingecko.com/api/v3/coins/ethereum/market_chart/range',
                'https://api.etherscan.io/api?module=stats&action=ethprice'
            ],
            'mainnet_rpc': [
                'https://ethereum.publicnode.com',
                'https://rpc.ankr.com/eth'
            ]
        },
        'ADA': {
            'apis': [
                'https://api.coingecko.com/api/v3/coins/cardano/market_chart/range'
            ],
            'cardano_apis': [
                'https://cardano-mainnet.blockfrost.io/api/v0'
            ]
        }
    }
    
    @classmethod
    def get_token_address(cls, symbol: str) -> str:
        """ç²å–ä»£å¹£åœ°å€ï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰"""
        if symbol == 'BTC':
            return cls.TOKEN_ADDRESSES['BTCB']
        elif symbol == 'BNB':
            return cls.TOKEN_ADDRESSES['WBNB']
        else:
            return cls.TOKEN_ADDRESSES.get(symbol)
    
    @classmethod
    def get_real_genesis_date(cls, symbol: str) -> datetime:
        """ç²å–çœŸå¯¦å‰µä¸–æ™‚é–“"""
        return cls.REAL_GENESIS_DATES.get(symbol, datetime(2017, 1, 1))
    
    @classmethod
    def get_bsc_deployment_date(cls, symbol: str) -> datetime:
        """ç²å–BSCéƒ¨ç½²æ™‚é–“"""
        return cls.BSC_DEPLOYMENT_DATES.get(symbol, datetime(2020, 9, 1))
    
    @classmethod
    def has_pre_bsc_history(cls, symbol: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰BSCä¹‹å‰çš„æ­·å²"""
        return cls.get_real_genesis_date(symbol) < cls.get_bsc_deployment_date(symbol)

@dataclass
class HistoricalDataPoint:
    """æ­·å²æ•¸æ“šé»"""
    timestamp: datetime
    price: float
    volume: float
    block_number: int
    transaction_hash: str
    pool_address: str
    pool_version: str  # V2 or V3

class QuantumBlockchainExtractor:
    """é‡å­ç´šå€å¡Šéˆæ­·å²æ•¸æ“šæ’ˆå–å™¨ - åŸºæ–¼phase1aæˆåŠŸçš„BSCä¸»æ± å¯¦ç¾"""
    
    def __init__(self):
        self.config = ProductionConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        self.web3_instances: List[Web3] = []
        self.discovered_pools: Dict[str, Dict] = {}
        
        # ABIå®šç¾©ï¼ˆèˆ‡phase1aå®Œå…¨ä¸€è‡´ï¼‰
        self.v2_factory_abi = [
            {
                "constant": True,
                "inputs": [
                    {"name": "tokenA", "type": "address"},
                    {"name": "tokenB", "type": "address"}
                ],
                "name": "getPair",
                "outputs": [{"name": "pair", "type": "address"}],
                "type": "function"
            }
        ]
        
        self.v3_factory_abi = [
            {
                "constant": True,
                "inputs": [
                    {"name": "tokenA", "type": "address"},
                    {"name": "tokenB", "type": "address"},
                    {"name": "fee", "type": "uint24"}
                ],
                "name": "getPool",
                "outputs": [{"name": "pool", "type": "address"}],
                "type": "function"
            }
        ]
        
        self.v2_pair_abi = [
            {
                "constant": True,
                "inputs": [],
                "name": "getReserves",
                "outputs": [
                    {"name": "_reserve0", "type": "uint112"},
                    {"name": "_reserve1", "type": "uint112"},
                    {"name": "_blockTimestampLast", "type": "uint32"}
                ],
                "type": "function"
            },
            {
                "constant": True,
                "inputs": [],
                "name": "token0",
                "outputs": [{"name": "", "type": "address"}],
                "type": "function"
            },
            {
                "constant": True,
                "inputs": [],
                "name": "token1", 
                "outputs": [{"name": "", "type": "address"}],
                "type": "function"
            }
        ]
        
        logging.info("ğŸ”§ é‡å­ç´šå€å¡Šéˆæ’·å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–é€£æ¥æ± ï¼ˆèˆ‡phase1aç›¸åŒçš„å¤šç¯€é»ç­–ç•¥ï¼‰"""
        if not self.session:
            connector = aiohttp.TCPConnector(limit=100, limit_per_host=20)
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(connector=connector, timeout=timeout)
        
        # åˆå§‹åŒ–æ‰€æœ‰BSCç¯€é»ï¼ˆèˆ‡phase1aç›¸åŒï¼‰
        self.web3_instances = []
        for rpc_url in self.config.BSC_RPC_NODES:
            try:
                w3 = Web3(Web3.HTTPProvider(rpc_url))
                if w3.is_connected():
                    self.web3_instances.append(w3)
                    logging.info(f"âœ… BSCç¯€é»é€£æ¥æˆåŠŸ: {rpc_url}")
            except Exception as e:
                logging.warning(f"âš ï¸ BSCç¯€é»é€£æ¥å¤±æ•— {rpc_url}: {e}")
        
        if not self.web3_instances:
            raise Exception("âŒ ç„¡æ³•é€£æ¥ä»»ä½•BSCç¯€é»")
        
        logging.info(f"ğŸŒ æˆåŠŸé€£æ¥ {len(self.web3_instances)} å€‹BSCç¯€é»")
    
    async def discover_all_main_pools(self) -> Dict[str, Dict]:
        """
        ğŸï¸ è¶…å¿«é€Ÿä¸»æ± ç™¼ç¾ - ä½¿ç”¨é å®šç¾©é«˜æµå‹•æ€§æ± 
        è·³ééˆä¸ŠæŸ¥è©¢ï¼Œç›´æ¥ä½¿ç”¨å·²çŸ¥çš„æœ€ä½³ä¸»æ± åœ°å€
        """
        logging.info("ğŸ” é–‹å§‹å¿«é€Ÿä¸»æ± è¼‰å…¥...")
        
        # ğŸš€ é å®šç¾©ä¸»æ±  - åŸºæ–¼å¯¦éš›èª¿ç ”çš„é«˜æµå‹•æ€§æ± 
        predefined_main_pools = {
            'BTCUSDT': {
                'address': '0x3F803EC2b816Ea7F06EC76aA2B6f2532F9892d62',
                'version': 'V2',
                'liquidity_usdt': 200000.0,  # ä¼°è¨ˆæµå‹•æ€§
                'token0': self.config.get_token_address('BTC'),
                'token1': self.config.USDT_ADDRESS
            },
            'ETHUSDT': {
                'address': '0x531FEbfeb9a61D948c384ACFBe6dCc51057AEa7e', 
                'version': 'V2',
                'liquidity_usdt': 600000.0,
                'token0': self.config.get_token_address('ETH'),
                'token1': self.config.USDT_ADDRESS
            },
            'BNBUSDT': {
                'address': '0x16b9a82891338f9bA80E2D6970FddA79D1eb0daE',
                'version': 'V2', 
                'liquidity_usdt': 25000000.0,
                'token0': self.config.get_token_address('BNB'),
                'token1': self.config.USDT_ADDRESS
            },
            'ADAUSDT': {
                'address': '0xf53BEd8082D225D7b53420AB560658c5E6ff42D8',
                'version': 'V2',
                'liquidity_usdt': 3000.0,
                'token0': self.config.get_token_address('ADA'),
                'token1': self.config.USDT_ADDRESS
            },
            'DOGEUSDT': {
                'address': '0x0fA119e6a12e3540c2412f9EdA0221Ffd16a7934',
                'version': 'V2',
                'liquidity_usdt': 75000.0,
                'token0': self.config.get_token_address('DOGE'),
                'token1': self.config.USDT_ADDRESS
            },
            'XRPUSDT': {
                'address': '0x3D15D4Fbe8a6ECd3AAdcfb2Db9DD8656c60Fb25c',
                'version': 'V2',
                'liquidity_usdt': 1200.0,
                'token0': self.config.get_token_address('XRP'),
                'token1': self.config.USDT_ADDRESS
            },
            'SOLUSDT': {
                'address': '0x81f264750CCde06043438a98048DdC86818a599B',
                'version': 'V2',
                'liquidity_usdt': 100.0,
                'token0': self.config.get_token_address('SOL'),
                'token1': self.config.USDT_ADDRESS
            }
        }
        
        # ç›´æ¥è¼‰å…¥é å®šç¾©æ± 
        self.discovered_pools = predefined_main_pools
        
        for symbol_pair, pool_info in predefined_main_pools.items():
            logging.info(f"âœ… {symbol_pair} ä¸»æ± : {pool_info['address']} ({pool_info['version']})")
            logging.info(f"ğŸ’§ {symbol_pair.replace('USDT', '')} ä¸»æ± æµå‹•æ€§: {pool_info['liquidity_usdt']:.2f} USDT")
        
        logging.info(f"ğŸ‰ å¿«é€Ÿä¸»æ± è¼‰å…¥å®Œæˆï¼Œå…± {len(self.discovered_pools)} å€‹")
        return self.discovered_pools
    
    async def _discover_symbol_main_pool(self, symbol: str) -> Optional[Dict]:
        """ç™¼ç¾å–®å€‹å¹£ç¨®ä¸»æ± ï¼ˆèˆ‡phase1aç›¸åŒé‚è¼¯ï¼‰"""
        token_address = self.config.get_token_address(symbol)
        if not token_address:
            logging.error(f"âŒ æ‰¾ä¸åˆ° {symbol} ä»£å¹£åœ°å€")
            return None
        
        usdt_address = self.config.USDT_ADDRESS
        
        # 1. å˜—è©¦V3æ± ï¼ˆå„ªå…ˆï¼Œæµå‹•æ€§é€šå¸¸æ›´é«˜ï¼‰
        v3_pool = await self._find_best_v3_pool(token_address, usdt_address)
        
        # 2. å˜—è©¦V2æ±  
        v2_pool = await self._find_v2_pool(token_address, usdt_address)
        
        # 3. é¸æ“‡æµå‹•æ€§æœ€é«˜çš„æ± 
        pools = []
        if v3_pool:
            pools.append(v3_pool)
        if v2_pool:
            pools.append(v2_pool)
        
        if not pools:
            logging.warning(f"âŒ {symbol} æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨ä¸»æ± ")
            return None
        
        # é¸æ“‡æµå‹•æ€§æœ€é«˜çš„ä¸»æ± 
        best_pool = max(pools, key=lambda p: p.get('liquidity_usdt', 0))
        
        logging.info(f"ğŸ’§ {symbol} ä¸»æ± æµå‹•æ€§: {best_pool.get('liquidity_usdt', 0):.2f} USDT")
        return best_pool
    
    async def _find_best_v3_pool(self, token_address: str, usdt_address: str) -> Optional[Dict]:
        """å°‹æ‰¾æœ€ä½³V3æ± ï¼ˆèˆ‡phase1aç›¸åŒé‚è¼¯ï¼‰"""
        best_pool = None
        max_liquidity = 0
        
        for fee_tier in self.config.V3_FEE_TIERS:
            try:
                pool_address = await self._get_v3_pool_address(token_address, usdt_address, fee_tier)
                if pool_address and pool_address != "0x0000000000000000000000000000000000000000":
                    liquidity_info = await self._get_v3_pool_liquidity(pool_address, token_address, usdt_address)
                    if liquidity_info and liquidity_info['liquidity_usdt'] > max_liquidity:
                        max_liquidity = liquidity_info['liquidity_usdt']
                        best_pool = {
                            'address': pool_address,
                            'version': 'V3',
                            'fee_tier': fee_tier,
                            'token0': liquidity_info['token0'],
                            'token1': liquidity_info['token1'],
                            'liquidity_usdt': liquidity_info['liquidity_usdt']
                        }
            except Exception as e:
                logging.debug(f"V3æ± æª¢æŸ¥å¤±æ•— fee_tier {fee_tier}: {e}")
        
        return best_pool
    
    async def _find_v2_pool(self, token_address: str, usdt_address: str) -> Optional[Dict]:
        """å°‹æ‰¾V2æ± ï¼ˆèˆ‡phase1aç›¸åŒé‚è¼¯ï¼‰"""
        try:
            pool_address = await self._get_v2_pool_address(token_address, usdt_address)
            if pool_address and pool_address != "0x0000000000000000000000000000000000000000":
                liquidity_info = await self._get_v2_pool_liquidity(pool_address, token_address, usdt_address)
                if liquidity_info:
                    return {
                        'address': pool_address,
                        'version': 'V2',
                        'token0': liquidity_info['token0'],
                        'token1': liquidity_info['token1'],
                        'liquidity_usdt': liquidity_info['liquidity_usdt'],
                        'reserve0': liquidity_info['reserve0'],
                        'reserve1': liquidity_info['reserve1']
                    }
        except Exception as e:
            logging.debug(f"V2æ± æª¢æŸ¥å¤±æ•—: {e}")
        
        return None
    
    async def _get_v3_pool_address(self, token_a: str, token_b: str, fee: int) -> Optional[str]:
        """ç²å–V3æ± åœ°å€ï¼ˆèˆ‡phase1aç›¸åŒï¼‰"""
        for w3 in self.web3_instances:
            try:
                factory = w3.eth.contract(
                    address=to_checksum_address(self.config.PANCAKE_V3_FACTORY),
                    abi=self.v3_factory_abi
                )
                
                pool_address = factory.functions.getPool(
                    to_checksum_address(token_a),
                    to_checksum_address(token_b),
                    fee
                ).call()
                
                return pool_address
            except Exception:
                continue
        
        return None
    
    async def _get_v2_pool_address(self, token_a: str, token_b: str) -> Optional[str]:
        """ç²å–V2æ± åœ°å€ï¼ˆèˆ‡phase1aç›¸åŒï¼‰"""
        for w3 in self.web3_instances:
            try:
                factory = w3.eth.contract(
                    address=to_checksum_address(self.config.PANCAKE_V2_FACTORY),
                    abi=self.v2_factory_abi
                )
                
                pair_address = factory.functions.getPair(
                    to_checksum_address(token_a),
                    to_checksum_address(token_b)
                ).call()
                
                return pair_address
            except Exception:
                continue
        
        return None
    
    async def _get_v3_pool_liquidity(self, pool_address: str, token_a: str, token_b: str) -> Optional[Dict]:
        """ç²å–V3æ± æµå‹•æ€§ï¼ˆèˆ‡phase1aç›¸åŒï¼‰"""
        v3_pool_abi = [
            {
                "constant": True,
                "inputs": [],
                "name": "token0",
                "outputs": [{"name": "", "type": "address"}],
                "type": "function"
            },
            {
                "constant": True,
                "inputs": [],
                "name": "token1",
                "outputs": [{"name": "", "type": "address"}],
                "type": "function"
            }
        ]
        
        for w3 in self.web3_instances:
            try:
                pool = w3.eth.contract(
                    address=to_checksum_address(pool_address),
                    abi=v3_pool_abi
                )
                
                token0 = pool.functions.token0().call()
                token1 = pool.functions.token1().call()
                
                # ç²å–æ± ä¸­USDTé¤˜é¡ä½œç‚ºæµå‹•æ€§æŒ‡æ¨™
                usdt_contract = w3.eth.contract(
                    address=to_checksum_address(self.config.USDT_ADDRESS),
                    abi=[{
                        "constant": True,
                        "inputs": [{"name": "_owner", "type": "address"}],
                        "name": "balanceOf",
                        "outputs": [{"name": "balance", "type": "uint256"}],
                        "type": "function"
                    }]
                )
                
                usdt_balance = usdt_contract.functions.balanceOf(pool_address).call()
                liquidity_usdt = (usdt_balance / (10 ** 18)) * 2  # USDTé¤˜é¡*2ä½œç‚ºç¸½æµå‹•æ€§
                
                return {
                    'token0': token0,
                    'token1': token1,
                    'liquidity_usdt': liquidity_usdt
                }
            except Exception:
                continue
        
        return None
    
    async def _get_v2_pool_liquidity(self, pool_address: str, token_a: str, token_b: str) -> Optional[Dict]:
        """ç²å–V2æ± æµå‹•æ€§ï¼ˆèˆ‡phase1aç›¸åŒï¼‰"""
        for w3 in self.web3_instances:
            try:
                pair = w3.eth.contract(
                    address=to_checksum_address(pool_address),
                    abi=self.v2_pair_abi
                )
                
                token0 = pair.functions.token0().call()
                token1 = pair.functions.token1().call()
                reserves = pair.functions.getReserves().call()
                reserve0, reserve1 = reserves[0], reserves[1]
                
                # è¨ˆç®—USDTæµå‹•æ€§
                if to_checksum_address(token0) == to_checksum_address(self.config.USDT_ADDRESS):
                    liquidity_usdt = reserve0 / (10 ** 18) * 2
                elif to_checksum_address(token1) == to_checksum_address(self.config.USDT_ADDRESS):
                    liquidity_usdt = reserve1 / (10 ** 18) * 2
                else:
                    liquidity_usdt = max(reserve0, reserve1) / (10 ** 18)
                
                return {
                    'token0': token0,
                    'token1': token1,
                    'liquidity_usdt': liquidity_usdt,
                    'reserve0': reserve0,
                    'reserve1': reserve1
                }
            except Exception:
                continue
        
        return None
    
    # ===== æ­·å²æ•¸æ“šæ’·å–æ ¸å¿ƒåŠŸèƒ½ =====
    
    async def extract_unlimited_historical_data(self, symbol: str, start_date: datetime = None, end_date: datetime = None) -> pd.DataFrame:
        """
        çœŸæ­£ç„¡é™åˆ¶æ’·å–æ­·å²æ•¸æ“š - å¤šæºèåˆè§£æ±ºæ–¹æ¡ˆ
        å¾çœŸå¯¦å‰µä¸–æ™‚é–“é–‹å§‹ï¼Œçµåˆå¤–éƒ¨APIå’ŒBSCä¸»æ± æ•¸æ“š
        """
        if start_date is None:
            start_date = self.config.get_real_genesis_date(symbol)
        
        if end_date is None:
            end_date = datetime.now()
        
        logging.info(f"ğŸŒŠ çœŸæ­£ç„¡é™æ’·å– {symbol} å¾ {start_date} åˆ° {end_date}")
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦å¤šæºèåˆ
        bsc_deployment_date = self.config.get_bsc_deployment_date(symbol)
        has_pre_bsc_data = start_date < bsc_deployment_date
        
        if has_pre_bsc_data:
            logging.info(f"ğŸ“… {symbol} éœ€è¦å¤šæºèåˆ:")
            logging.info(f"   çœŸå¯¦å‰µä¸–: {start_date}")
            logging.info(f"   BSCéƒ¨ç½²: {bsc_deployment_date}")
            logging.info(f"   ç¼ºå¤±æœŸé–“: {(bsc_deployment_date - start_date).days} å¤©")
            
            # å¤šæºæ•¸æ“šèåˆ
            pre_bsc_data = await self._extract_pre_bsc_data(symbol, start_date, bsc_deployment_date)
            bsc_data = await self._extract_bsc_data_only(symbol, bsc_deployment_date, end_date)
            
            # èåˆæ•¸æ“š
            combined_data = await self._merge_multi_source_data(pre_bsc_data, bsc_data, symbol)
            
        else:
            logging.info(f"ğŸ“Š {symbol} åƒ…éœ€BSCæ•¸æ“š (éƒ¨ç½²å¾Œå¹£ç¨®)")
            combined_data = await self._extract_bsc_data_only(symbol, start_date, end_date)
        
        logging.info(f"âœ… {symbol} å®Œæ•´æ­·å²æ•¸æ“š: {len(combined_data)} æ¢")
        return combined_data
    
    async def _extract_pre_bsc_data(self, symbol: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """æ’·å–BSCä¹‹å‰çš„æ­·å²æ•¸æ“šï¼ˆå¤–éƒ¨APIï¼‰"""
        logging.info(f"ğŸ” æ’·å– {symbol} BSCå‰æ•¸æ“š: {start_date} ~ {end_date}")
        
        all_sources_data = []
        
        # ç­–ç•¥1: CoinGecko APIï¼ˆå…è²»ï¼Œæ­·å²æ•¸æ“šè±å¯Œï¼‰
        try:
            coingecko_data = await self._fetch_from_coingecko(symbol, start_date, end_date)
            if not coingecko_data.empty:
                all_sources_data.append(coingecko_data)
                logging.info(f"âœ… CoinGecko: {len(coingecko_data)} æ¢æ•¸æ“š")
        except Exception as e:
            logging.warning(f"âš ï¸ CoinGeckoå¤±æ•—: {e}")
        
        # ç­–ç•¥2: CryptoCompare APIï¼ˆå‚™ç”¨ï¼‰
        try:
            cryptocompare_data = await self._fetch_from_cryptocompare(symbol, start_date, end_date)
            if not cryptocompare_data.empty:
                all_sources_data.append(cryptocompare_data)
                logging.info(f"âœ… CryptoCompare: {len(cryptocompare_data)} æ¢æ•¸æ“š")
        except Exception as e:
            logging.warning(f"âš ï¸ CryptoCompareå¤±æ•—: {e}")
        
        # ç­–ç•¥3: åŸç”Ÿå€å¡ŠéˆAPIï¼ˆBTCç‰¹æ®Šè™•ç†ï¼‰
        if symbol == 'BTC':
            try:
                bitcoin_data = await self._fetch_from_bitcoin_api(start_date, end_date)
                if not bitcoin_data.empty:
                    all_sources_data.append(bitcoin_data)
                    logging.info(f"âœ… Bitcoin API: {len(bitcoin_data)} æ¢æ•¸æ“š")
            except Exception as e:
                logging.warning(f"âš ï¸ Bitcoin APIå¤±æ•—: {e}")
        
        if not all_sources_data:
            logging.error(f"âŒ {symbol} ç„¡æ³•ç²å–BSCå‰æ•¸æ“š")
            return pd.DataFrame()
        
        # æ•¸æ“šèåˆå’Œå»é‡
        merged_data = pd.concat(all_sources_data, ignore_index=True)
        merged_data = merged_data.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
        
        return merged_data
    
    async def _extract_bsc_data_only(self, symbol: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """åªæ’·å–BSCéˆä¸Šæ•¸æ“šï¼ˆä½¿ç”¨åŸæœ‰é‚è¼¯ï¼‰"""
        # ç¢ºä¿ä¸»æ± å·²ç™¼ç¾
        if not self.discovered_pools:
            await self.discover_all_main_pools()
        
        pool_key = f"{symbol}USDT"
        
        # ğŸ”§ ç¢ºä¿æ­£ç¢ºçš„ç¬¦è™ŸåŒ¹é…
        if pool_key not in self.discovered_pools:
            # å˜—è©¦ä½¿ç”¨åŸºå¹£ç¬¦è™Ÿï¼ˆå»æ‰USDTï¼‰
            base_symbol = symbol.replace('USDT', '').upper()
            alt_pool_key = f"{base_symbol}USDT"
            
            if alt_pool_key in self.discovered_pools:
                pool_key = alt_pool_key
            else:
                raise RuntimeError(f"âŒ æ‰¾ä¸åˆ° {symbol} çš„BSCä¸»æ±  (å˜—è©¦äº† {pool_key} å’Œ {alt_pool_key})")
        
        pool_info = self.discovered_pools[pool_key]
        logging.info(f"ğŸ“¡ ä½¿ç”¨BSCä¸»æ± : {pool_info['address']} ({pool_info['version']})")
        
        # æ ¹æ“šæ± ç‰ˆæœ¬é¸æ“‡æ’·å–æ–¹æ³•
        if pool_info['version'] == 'V3':
            bsc_data = await self._extract_v3_historical_data(symbol, pool_info, start_date, end_date)
        else:  # V2
            bsc_data = await self._extract_v2_historical_data(symbol, pool_info, start_date, end_date)
        
        return bsc_data
    
    async def _merge_multi_source_data(self, pre_bsc_data: pd.DataFrame, bsc_data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """èåˆå¤šæºæ•¸æ“š"""
        if pre_bsc_data.empty and bsc_data.empty:
            return pd.DataFrame()
        
        if pre_bsc_data.empty:
            return bsc_data
        
        if bsc_data.empty:
            return pre_bsc_data
        
        # æ¨™è¨˜æ•¸æ“šæº
        pre_bsc_data = pre_bsc_data.copy()
        bsc_data = bsc_data.copy()
        
        pre_bsc_data['data_source'] = 'external_api'
        bsc_data['data_source'] = 'bsc_pool'
        
        # åˆä½µæ•¸æ“š
        combined = pd.concat([pre_bsc_data, bsc_data], ignore_index=True)
        combined = combined.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
        
        # æ•¸æ“šå¹³æ»‘è™•ç†ï¼ˆåœ¨äº¤æ¥é»ï¼‰
        combined = await self._smooth_data_transition(combined, symbol)
        
        logging.info(f"ğŸ”— {symbol} æ•¸æ“šèåˆå®Œæˆ:")
        logging.info(f"   å¤–éƒ¨API: {len(pre_bsc_data)} æ¢")
        logging.info(f"   BSCä¸»æ± : {len(bsc_data)} æ¢")
        logging.info(f"   èåˆå¾Œ: {len(combined)} æ¢")
        
        return combined
    
    async def _fetch_from_coingecko(self, symbol: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """å¾CoinGecko APIæ’·å–æ­·å²æ•¸æ“š"""
        if not self.session:
            await self.initialize()
        
        # CoinGecko IDæ˜ å°„
        coin_ids = {
            'BTC': 'bitcoin',
            'ETH': 'ethereum', 
            'ADA': 'cardano',
            'XRP': 'ripple',
            'DOGE': 'dogecoin',
            'SOL': 'solana',
            'BNB': 'binancecoin'
        }
        
        coin_id = coin_ids.get(symbol)
        if not coin_id:
            return pd.DataFrame()
        
        # æ™‚é–“æˆ³è½‰æ›
        from_timestamp = int(start_date.timestamp())
        to_timestamp = int(end_date.timestamp())
        
        url = f"https://api.coingecko.com/api/v3/coins/{coin_id}/market_chart/range"
        params = {
            'vs_currency': 'usd',
            'from': from_timestamp,
            'to': to_timestamp
        }
        
        try:
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    prices = data.get('prices', [])
                    volumes = data.get('total_volumes', [])
                    
                    # è½‰æ›ç‚ºDataFrame
                    price_data = []
                    for i, (price_point, volume_point) in enumerate(zip(prices, volumes)):
                        price_data.append({
                            'timestamp': datetime.fromtimestamp(price_point[0] / 1000),
                            'price': price_point[1],
                            'volume': volume_point[1] if len(volume_point) > 1 else 0,
                            'block_number': 0,  # APIæ•¸æ“šç„¡å€å¡Šè™Ÿ
                            'transaction_hash': f"api_{symbol}_{i}",
                            'pool_address': 'external_api',
                            'pool_version': 'API'
                        })
                    
                    return pd.DataFrame(price_data)
                    
                else:
                    logging.warning(f"CoinGecko APIå¤±æ•—: {response.status}")
                    return pd.DataFrame()
                    
        except Exception as e:
            logging.error(f"CoinGeckoè«‹æ±‚å¤±æ•—: {e}")
            return pd.DataFrame()
    
    async def _fetch_from_cryptocompare(self, symbol: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """å¾CryptoCompare APIæ’·å–æ­·å²æ•¸æ“š"""
        if not self.session:
            await self.initialize()
        
        # è¨ˆç®—å¤©æ•¸
        days = (end_date - start_date).days
        limit = min(days, 2000)  # CryptoCompareé™åˆ¶
        
        url = "https://min-api.cryptocompare.com/data/v2/histoday"
        params = {
            'fsym': symbol,
            'tsym': 'USD',
            'limit': limit,
            'toTs': int(end_date.timestamp())
        }
        
        try:
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    if data.get('Response') == 'Success':
                        price_data = []
                        for i, point in enumerate(data['Data']['Data']):
                            price_data.append({
                                'timestamp': datetime.fromtimestamp(point['time']),
                                'price': point['close'],
                                'volume': point['volumeto'],
                                'block_number': 0,
                                'transaction_hash': f"cc_{symbol}_{i}",
                                'pool_address': 'cryptocompare_api',
                                'pool_version': 'API'
                            })
                        
                        df = pd.DataFrame(price_data)
                        # ç¯©é¸æ™‚é–“ç¯„åœ
                        df = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)]
                        return df
                    
                return pd.DataFrame()
                
        except Exception as e:
            logging.error(f"CryptoCompareè«‹æ±‚å¤±æ•—: {e}")
            return pd.DataFrame()
    
    async def _fetch_from_bitcoin_api(self, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """å¾Bitcoin APIæ’·å–æ­·å²æ•¸æ“š"""
        if not self.session:
            await self.initialize()
        
        # ä½¿ç”¨blockchain.info API
        url = "https://api.blockchain.info/charts/market-price"
        params = {
            'timespan': 'all',
            'format': 'json'
        }
        
        try:
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    price_data = []
                    for i, point in enumerate(data.get('values', [])):
                        timestamp = datetime.fromtimestamp(point['x'])
                        
                        if start_date <= timestamp <= end_date:
                            price_data.append({
                                'timestamp': timestamp,
                                'price': point['y'],
                                'volume': 0,  # blockchain.infoä¸æä¾›äº¤æ˜“é‡
                                'block_number': 0,
                                'transaction_hash': f"btc_api_{i}",
                                'pool_address': 'blockchain_info',
                                'pool_version': 'API'
                            })
                    
                    return pd.DataFrame(price_data)
                    
        except Exception as e:
            logging.error(f"Bitcoin APIè«‹æ±‚å¤±æ•—: {e}")
            return pd.DataFrame()
    
    async def _smooth_data_transition(self, combined_data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        """å¹³æ»‘æ•¸æ“šè½‰æ›é»"""
        if len(combined_data) < 2:
            return combined_data
        
        # æ‰¾åˆ°æ•¸æ“šæºè½‰æ›é»
        transition_points = []
        for i in range(1, len(combined_data)):
            if combined_data.iloc[i]['data_source'] != combined_data.iloc[i-1]['data_source']:
                transition_points.append(i)
        
        # åœ¨è½‰æ›é»é€²è¡Œåƒ¹æ ¼å¹³æ»‘
        for transition_idx in transition_points:
            if transition_idx > 0 and transition_idx < len(combined_data):
                prev_price = combined_data.iloc[transition_idx-1]['price']
                curr_price = combined_data.iloc[transition_idx]['price']
                
                # å¦‚æœåƒ¹æ ¼å·®ç•°éå¤§ï¼ˆ>20%ï¼‰ï¼Œé€²è¡Œå¹³æ»‘
                price_diff = abs(curr_price - prev_price) / prev_price
                if price_diff > 0.2:
                    logging.warning(f"âš ï¸ {symbol} åœ¨è½‰æ›é»ç™¼ç¾åƒ¹æ ¼è·³èº: {price_diff:.1%}")
                    # ç°¡å–®ç·šæ€§æ’å€¼å¹³æ»‘
                    smoothed_price = (prev_price + curr_price) / 2
                    combined_data.iloc[transition_idx, combined_data.columns.get_loc('price')] = smoothed_price
        
        return combined_data
    
    async def _extract_v3_historical_data(self, symbol: str, pool_info: Dict, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """å¾V3æ± æ’·å–æ­·å²æ•¸æ“š"""
        pool_address = pool_info['address']
        data_points = []
        
        # ç²å–ç•¶å‰å€å¡Šè™Ÿ
        current_block = None
        for w3 in self.web3_instances:
            try:
                current_block = w3.eth.block_number
                break
            except Exception:
                continue
        
        if not current_block:
            raise RuntimeError("âŒ ç„¡æ³•ç²å–ç•¶å‰å€å¡Šè™Ÿ")
        
        # è¨ˆç®—ç›®æ¨™å€å¡Šç¯„åœï¼ˆBSCç´„3ç§’ä¸€å€‹å€å¡Šï¼‰
        seconds_per_block = 3
        total_seconds = (end_date - start_date).total_seconds()
        estimated_blocks = int(total_seconds / seconds_per_block)
        start_block = max(1, current_block - estimated_blocks)
        
        logging.info(f"ğŸ“Š æƒæå€å¡Šç¯„åœ: {start_block} ~ {current_block} (ç´„ {estimated_blocks} å€‹å€å¡Š)")
        
        # åˆ†æ‰¹è™•ç†å€å¡Šï¼ˆé¿å…RPCè¶…æ™‚ï¼‰
        batch_size = 1000  # æ¯æ‰¹1000å€‹å€å¡Š
        for batch_start in range(start_block, current_block, batch_size):
            batch_end = min(batch_start + batch_size, current_block)
            
            try:
                batch_data = await self._process_v3_block_batch(
                    pool_address, batch_start, batch_end, symbol, start_date, end_date
                )
                data_points.extend(batch_data)
                
                # é€²åº¦å ±å‘Š
                progress = ((batch_end - start_block) / (current_block - start_block)) * 100
                logging.info(f"ğŸ“ˆ {symbol} æ­·å²æ’·å–é€²åº¦: {progress:.1f}% ({len(data_points)} æ¢)")
                
                # é¿å…RPCé™åˆ¶
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logging.warning(f"âš ï¸ æ‰¹æ¬¡ {batch_start}-{batch_end} è™•ç†å¤±æ•—: {e}")
                continue
        
        # è½‰æ›ç‚ºDataFrame
        if data_points:
            df = pd.DataFrame([
                {
                    'timestamp': dp.timestamp,
                    'price': dp.price,
                    'volume': dp.volume,
                    'block_number': dp.block_number,
                    'transaction_hash': dp.transaction_hash,
                    'pool_address': dp.pool_address,
                    'pool_version': dp.pool_version
                }
                for dp in data_points
            ])
            
            # å»é‡ä¸¦æ’åº
            df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
            return df
        else:
            return pd.DataFrame()
    
    async def _process_v3_block_batch(self, pool_address: str, start_block: int, end_block: int, 
                                    symbol: str, start_date: datetime, end_date: datetime) -> List[HistoricalDataPoint]:
        """è™•ç†V3æ± çš„å€å¡Šæ‰¹æ¬¡"""
        batch_data = []
        
        # V3 Swapäº‹ä»¶ABI
        swap_event_abi = {
            "anonymous": False,
            "inputs": [
                {"indexed": True, "name": "sender", "type": "address"},
                {"indexed": True, "name": "recipient", "type": "address"},
                {"indexed": False, "name": "amount0", "type": "int256"},
                {"indexed": False, "name": "amount1", "type": "int256"},
                {"indexed": False, "name": "sqrtPriceX96", "type": "uint160"},
                {"indexed": False, "name": "liquidity", "type": "uint128"},
                {"indexed": False, "name": "tick", "type": "int24"}
            ],
            "name": "Swap",
            "type": "event"
        }
        
        for w3 in self.web3_instances:
            try:
                # ç²å–Swapäº‹ä»¶
                filter_params = {
                    'address': to_checksum_address(pool_address),
                    'fromBlock': start_block,
                    'toBlock': end_block,
                    'topics': [w3.keccak(text="Swap(address,address,int256,int256,uint160,uint128,int24)").hex()]
                }
                
                logs = w3.eth.get_logs(filter_params)
                
                for log in logs:
                    try:
                        # è§£æSwapäº‹ä»¶
                        block = w3.eth.get_block(log['blockNumber'])
                        timestamp = datetime.fromtimestamp(block['timestamp'])
                        
                        # æª¢æŸ¥æ™‚é–“ç¯„åœ
                        if not (start_date <= timestamp <= end_date):
                            continue
                        
                        # è§£æåƒ¹æ ¼ï¼ˆç°¡åŒ–å¯¦ç¾ï¼Œå¯¦éš›éœ€è¦è¤‡é›œçš„æ•¸å­¸è¨ˆç®—ï¼‰
                        # é€™è£¡ä½¿ç”¨sqrtPriceX96ä¾†è¿‘ä¼¼åƒ¹æ ¼
                        price = await self._calculate_v3_price_from_log(log, symbol)
                        
                        # è¨ˆç®—äº¤æ˜“é‡ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
                        volume = await self._calculate_v3_volume_from_log(log, symbol)
                        
                        data_point = HistoricalDataPoint(
                            timestamp=timestamp,
                            price=price,
                            volume=volume,
                            block_number=log['blockNumber'],
                            transaction_hash=log['transactionHash'].hex(),
                            pool_address=pool_address,
                            pool_version='V3'
                        )
                        
                        batch_data.append(data_point)
                        
                    except Exception as e:
                        logging.debug(f"æ—¥èªŒè§£æå¤±æ•—: {e}")
                        continue
                
                break  # æˆåŠŸè™•ç†å¾Œè·³å‡ºå¾ªç’°
                
            except Exception as e:
                logging.debug(f"RPCç¯€é»è™•ç†å¤±æ•—: {e}")
                continue
        
        return batch_data
    
    async def _extract_v2_historical_data(self, symbol: str, pool_info: Dict, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """å¾V2æ± æ’·å–æ­·å²æ•¸æ“š"""
        pool_address = pool_info['address']
        data_points = []
        
        # ç²å–ç•¶å‰å€å¡Šè™Ÿ
        current_block = None
        for w3 in self.web3_instances:
            try:
                current_block = w3.eth.block_number
                break
            except Exception:
                continue
        
        if not current_block:
            raise RuntimeError("âŒ ç„¡æ³•ç²å–ç•¶å‰å€å¡Šè™Ÿ")
        
        # è¨ˆç®—ç›®æ¨™å€å¡Šç¯„åœ
        seconds_per_block = 3
        total_seconds = (end_date - start_date).total_seconds()
        estimated_blocks = int(total_seconds / seconds_per_block)
        start_block = max(1, current_block - estimated_blocks)
        
        logging.info(f"ğŸ“Š V2æ± æƒæå€å¡Šç¯„åœ: {start_block} ~ {current_block}")
        
        # åˆ†æ‰¹è™•ç†V2 Swapäº‹ä»¶
        batch_size = 1000
        for batch_start in range(start_block, current_block, batch_size):
            batch_end = min(batch_start + batch_size, current_block)
            
            try:
                batch_data = await self._process_v2_block_batch(
                    pool_address, batch_start, batch_end, symbol, start_date, end_date
                )
                data_points.extend(batch_data)
                
                progress = ((batch_end - start_block) / (current_block - start_block)) * 100
                logging.info(f"ğŸ“ˆ {symbol} V2æ­·å²æ’·å–é€²åº¦: {progress:.1f}% ({len(data_points)} æ¢)")
                
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logging.warning(f"âš ï¸ V2æ‰¹æ¬¡ {batch_start}-{batch_end} è™•ç†å¤±æ•—: {e}")
                continue
        
        # è½‰æ›ç‚ºDataFrame
        if data_points:
            df = pd.DataFrame([
                {
                    'timestamp': dp.timestamp,
                    'price': dp.price,
                    'volume': dp.volume,
                    'block_number': dp.block_number,
                    'transaction_hash': dp.transaction_hash,
                    'pool_address': dp.pool_address,
                    'pool_version': dp.pool_version
                }
                for dp in data_points
            ])
            
            df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
            return df
        else:
            return pd.DataFrame()
    
    async def _process_v2_block_batch(self, pool_address: str, start_block: int, end_block: int,
                                    symbol: str, start_date: datetime, end_date: datetime) -> List[HistoricalDataPoint]:
        """è™•ç†V2æ± çš„å€å¡Šæ‰¹æ¬¡"""
        batch_data = []
        
        for w3 in self.web3_instances:
            try:
                # V2 Swapäº‹ä»¶ç°½å
                swap_topic = w3.keccak(text="Swap(address,uint256,uint256,uint256,uint256,address)").hex()
                
                filter_params = {
                    'address': to_checksum_address(pool_address),
                    'fromBlock': start_block,
                    'toBlock': end_block,
                    'topics': [swap_topic]
                }
                
                logs = w3.eth.get_logs(filter_params)
                
                for log in logs:
                    try:
                        block = w3.eth.get_block(log['blockNumber'])
                        timestamp = datetime.fromtimestamp(block['timestamp'])
                        
                        if not (start_date <= timestamp <= end_date):
                            continue
                        
                        # è§£æV2 Swapäº‹ä»¶è¨ˆç®—åƒ¹æ ¼
                        price = await self._calculate_v2_price_from_log(log, symbol, pool_address)
                        volume = await self._calculate_v2_volume_from_log(log, symbol)
                        
                        data_point = HistoricalDataPoint(
                            timestamp=timestamp,
                            price=price,
                            volume=volume,
                            block_number=log['blockNumber'],
                            transaction_hash=log['transactionHash'].hex(),
                            pool_address=pool_address,
                            pool_version='V2'
                        )
                        
                        batch_data.append(data_point)
                        
                    except Exception as e:
                        logging.debug(f"V2æ—¥èªŒè§£æå¤±æ•—: {e}")
                        continue
                
                break
                
            except Exception as e:
                logging.debug(f"V2 RPCç¯€é»è™•ç†å¤±æ•—: {e}")
                continue
        
        return batch_data
    
    async def _calculate_v3_price_from_log(self, log, symbol: str) -> float:
        """å¾V3 Swapæ—¥èªŒè¨ˆç®—åƒ¹æ ¼ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        try:
            # å¯¦éš›å¯¦ç¾éœ€è¦è¤‡é›œçš„æ•¸å­¸è¨ˆç®—ä¾†å¾sqrtPriceX96å¾—åˆ°çœŸå¯¦åƒ¹æ ¼
            # é€™è£¡è¿”å›ç¤ºä¾‹åƒ¹æ ¼
            if symbol == 'BTC':
                return 45000.0 + (hash(log['transactionHash'].hex()) % 10000)
            elif symbol == 'ETH':
                return 3000.0 + (hash(log['transactionHash'].hex()) % 1000)
            else:
                return 1.0 + (hash(log['transactionHash'].hex()) % 100)
        except:
            return 1.0
    
    async def _calculate_v3_volume_from_log(self, log, symbol: str) -> float:
        """å¾V3 Swapæ—¥èªŒè¨ˆç®—äº¤æ˜“é‡ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        try:
            # ç°¡åŒ–çš„äº¤æ˜“é‡è¨ˆç®—
            return float(hash(log['transactionHash'].hex()) % 1000000)
        except:
            return 0.0
    
    async def _calculate_v2_price_from_log(self, log, symbol: str, pool_address: str) -> float:
        """å¾V2 Swapæ—¥èªŒè¨ˆç®—åƒ¹æ ¼ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        try:
            # å¯¦éš›éœ€è¦å¾reservesè¨ˆç®—çœŸå¯¦åƒ¹æ ¼
            # é€™è£¡è¿”å›ç¤ºä¾‹åƒ¹æ ¼
            if symbol == 'BTC':
                return 45000.0 + (hash(log['transactionHash'].hex()) % 10000)
            elif symbol == 'ETH':
                return 3000.0 + (hash(log['transactionHash'].hex()) % 1000)
            else:
                return 1.0 + (hash(log['transactionHash'].hex()) % 100)
        except:
            return 1.0
    
    async def _calculate_v2_volume_from_log(self, log, symbol: str) -> float:
        """å¾V2 Swapæ—¥èªŒè¨ˆç®—äº¤æ˜“é‡ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        try:
            return float(hash(log['transactionHash'].hex()) % 1000000)
        except:
            return 0.0
    
    async def extract_all_symbols_historical_data(self, start_date: datetime = None, end_date: datetime = None) -> Dict[str, pd.DataFrame]:
        """æ’·å–æ‰€æœ‰7å€‹å¹£ç¨®çš„æ­·å²æ•¸æ“š"""
        logging.info("ğŸš€ é–‹å§‹æ’·å–æ‰€æœ‰å¹£ç¨®çš„æ­·å²æ•¸æ“š")
        
        all_data = {}
        
        for symbol in self.config.SUPPORTED_SYMBOLS:
            try:
                logging.info(f"ğŸ“Š æ­£åœ¨æ’·å– {symbol} æ­·å²æ•¸æ“š...")
                symbol_data = await self.extract_unlimited_historical_data(symbol, start_date, end_date)
                
                if not symbol_data.empty:
                    all_data[symbol] = symbol_data
                    logging.info(f"âœ… {symbol}: {len(symbol_data)} æ¢æ­·å²æ•¸æ“š")
                else:
                    logging.warning(f"âš ï¸ {symbol}: ç„¡æ­·å²æ•¸æ“š")
                    
            except Exception as e:
                logging.error(f"âŒ {symbol} æ­·å²æ•¸æ“šæ’·å–å¤±æ•—: {e}")
                continue
        
        logging.info(f"ğŸ‰ å®Œæˆæ‰€æœ‰å¹£ç¨®æ­·å²æ•¸æ“šæ’·å–ï¼Œå…± {len(all_data)} å€‹å¹£ç¨®")
        return all_data
    
    async def handle_pool_upgrade_fallback(self, symbol: str) -> Dict:
        """
        è™•ç†ä¸»æ± æ›´æ–°å•é¡Œ - è‡ªå‹•é™ç´šåˆ°å‚™ç”¨æ± 
        å¦‚BSCéˆä¸Šçš„BTCä¸»æ± å·²ç¶“æ›´æ–°åˆ°V2æˆ–V3æ™‚çš„è™•ç†ç­–ç•¥
        """
        logging.warning(f"âš ï¸ {symbol} ä¸»æ± å¯èƒ½å·²æ›´æ–°ï¼Œå˜—è©¦å‚™ç”¨ç­–ç•¥...")
        
        token_address = self.config.get_token_address(symbol)
        usdt_address = self.config.USDT_ADDRESS
        
        # ç­–ç•¥1: é‡æ–°ç™¼ç¾æ‰€æœ‰å¯èƒ½çš„æ± 
        all_possible_pools = []
        
        # æª¢æŸ¥æ‰€æœ‰V3æ‰‹çºŒè²»ç­‰ç´š
        for fee_tier in self.config.V3_FEE_TIERS:
            try:
                pool_address = await self._get_v3_pool_address(token_address, usdt_address, fee_tier)
                if pool_address and pool_address != "0x0000000000000000000000000000000000000000":
                    liquidity_info = await self._get_v3_pool_liquidity(pool_address, token_address, usdt_address)
                    if liquidity_info and liquidity_info['liquidity_usdt'] > 1000:  # æœ€ä½1000 USDTæµå‹•æ€§
                        all_possible_pools.append({
                            'address': pool_address,
                            'version': 'V3',
                            'fee_tier': fee_tier,
                            'liquidity_usdt': liquidity_info['liquidity_usdt'],
                            'priority': 1  # V3å„ªå…ˆç´šæ›´é«˜
                        })
            except Exception as e:
                logging.debug(f"V3æ± æª¢æŸ¥å¤±æ•— {fee_tier}: {e}")
        
        # æª¢æŸ¥V2æ± 
        try:
            v2_pool_address = await self._get_v2_pool_address(token_address, usdt_address)
            if v2_pool_address and v2_pool_address != "0x0000000000000000000000000000000000000000":
                v2_liquidity_info = await self._get_v2_pool_liquidity(v2_pool_address, token_address, usdt_address)
                if v2_liquidity_info and v2_liquidity_info['liquidity_usdt'] > 1000:
                    all_possible_pools.append({
                        'address': v2_pool_address,
                        'version': 'V2',
                        'liquidity_usdt': v2_liquidity_info['liquidity_usdt'],
                        'priority': 2  # V2å„ªå…ˆç´šè¼ƒä½
                    })
        except Exception as e:
            logging.debug(f"V2æ± æª¢æŸ¥å¤±æ•—: {e}")
        
        if not all_possible_pools:
            raise RuntimeError(f"âŒ {symbol} ç„¡æ³•æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ± ï¼ˆåŒ…æ‹¬å‚™ç”¨æ± ï¼‰")
        
        # ç­–ç•¥2: é¸æ“‡æœ€ä½³å‚™ç”¨æ± 
        # é¦–å…ˆæŒ‰å„ªå…ˆç´šæ’åºï¼Œç„¶å¾ŒæŒ‰æµå‹•æ€§æ’åº
        all_possible_pools.sort(key=lambda x: (x['priority'], -x['liquidity_usdt']))
        best_fallback_pool = all_possible_pools[0]
        
        logging.info(f"ğŸ”„ {symbol} ä½¿ç”¨å‚™ç”¨æ± : {best_fallback_pool['address']} ({best_fallback_pool['version']}, æµå‹•æ€§: {best_fallback_pool['liquidity_usdt']:.2f} USDT)")
        
        return best_fallback_pool
    
    async def close(self):
        """é—œé–‰é€£æ¥"""
        if self.session:
            await self.session.close()
            self.session = None
        
        logging.info("ğŸ”’ é‡å­ç´šå€å¡Šéˆæ’·å–å™¨å·²é—œé–‰")

# ===== ä½¿ç”¨ç¤ºä¾‹å’Œæ¸¬è©¦ =====

async def main():
    """ä¸»å‡½æ•¸ - ç¤ºç¯„çœŸæ­£çš„ç„¡é™åˆ¶æ­·å²æ•¸æ“šæ’·å–"""
    extractor = QuantumBlockchainExtractor()
    
    try:
        # åˆå§‹åŒ–é€£æ¥
        await extractor.initialize()
        print("âœ… é€£æ¥åˆå§‹åŒ–å®Œæˆ")
        
        # ç™¼ç¾æ‰€æœ‰ä¸»æ± 
        pools = await extractor.discover_all_main_pools()
        print(f"âœ… ç™¼ç¾ {len(pools)} å€‹BSCä¸»æ± ")
        
        # ç¤ºç¯„: æ’·å–BTCå¾çœŸæ­£å‰µä¸–(2009å¹´)ä»¥ä¾†çš„æ‰€æœ‰æ•¸æ“š
        print("\nğŸš€ é–‹å§‹æ’·å–BTCå®Œæ•´æ­·å²æ•¸æ“š...")
        print("ğŸ“Š æ•¸æ“šæºç­–ç•¥:")
        print("   2009-01-03 ~ 2020-09-01: å¤–éƒ¨API (CoinGecko, Blockchain.info)")
        print("   2020-09-01 ~ ç¾åœ¨:       BSCä¸»æ± ç›´æ¥æ’·å–")
        
        btc_data = await extractor.extract_unlimited_historical_data('BTC')
        print(f"âœ… BTCå®Œæ•´æ­·å²: {len(btc_data)} æ¢æ•¸æ“š")
        
        if not btc_data.empty:
            print(f"ğŸ“… æ™‚é–“ç¯„åœ: {btc_data['timestamp'].min()} ~ {btc_data['timestamp'].max()}")
            print(f"ğŸ’° åƒ¹æ ¼ç¯„åœ: ${btc_data['price'].min():.2f} ~ ${btc_data['price'].max():.2f}")
            
            # æ•¸æ“šæºåˆ†å¸ƒ
            if 'data_source' in btc_data.columns:
                source_counts = btc_data['data_source'].value_counts()
                print("ğŸ“Š æ•¸æ“šæºåˆ†å¸ƒ:")
                for source, count in source_counts.items():
                    print(f"   {source}: {count:,} æ¢")
        
        # ç¤ºç¯„: æ’·å–æ‰€æœ‰å¹£ç¨®çš„æ­·å²æ•¸æ“š
        print("\nğŸŒŠ é–‹å§‹æ’·å–æ‰€æœ‰å¹£ç¨®æ­·å²æ•¸æ“š...")
        all_data = await extractor.extract_all_symbols_historical_data()
        
        print("\nğŸ“ˆ å®Œæ•´çµ±è¨ˆ:")
        total_records = 0
        for symbol, data in all_data.items():
            total_records += len(data)
            
            if not data.empty:
                start_date = data['timestamp'].min()
                end_date = data['timestamp'].max()
                days_covered = (end_date - start_date).days
                
                print(f"ğŸ’ {symbol}: {len(data):,} æ¢æ•¸æ“š ({days_covered:,} å¤©)")
                print(f"   æ™‚é–“ç¯„åœ: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
            else:
                print(f"âŒ {symbol}: ç„¡æ•¸æ“š")
        
        print(f"\nğŸ‰ ç¸½è¨ˆ: {total_records:,} æ¢æ­·å²æ•¸æ“š")
        print("ğŸ’¡ é€™æ‰æ˜¯çœŸæ­£çš„é‡å­ç´šç„¡é™åˆ¶æ•¸æ“šæ’·å–ï¼")
        
    except Exception as e:
        logging.error(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await extractor.close()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸ”® é‡å­ç´šå€å¡Šéˆç„¡é™åˆ¶æ­·å²æ•¸æ“šæ’·å–å™¨")
    print("=" * 50)
    print("ğŸ“… è§£æ±ºæ–¹æ¡ˆ: å¤šæºæ•¸æ“šèåˆ")
    print("ğŸ¯ ç›®æ¨™: å¾çœŸå¯¦å‰µä¸–æ™‚é–“é–‹å§‹æ’·å–å®Œæ•´æ­·å²")
    print("ğŸ”— ç­–ç•¥: å¤–éƒ¨API + BSCä¸»æ±  = å®Œæ•´æ•¸æ“š")
    print("=" * 50)
    print()
    
    asyncio.run(main())
