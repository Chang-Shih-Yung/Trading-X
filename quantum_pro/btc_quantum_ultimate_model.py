#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BTC é‡å­çµ‚æ¥µæ¨¡å‹ - æ•´åˆåˆ° Trading X é‡å­ç³»çµ±
========================================

é€™å€‹æ¨¡çµ„æ•´åˆäº†æ‚¨æä¾›çš„ BTC_Quantum_Ultimate_Model.py çš„æ ¸å¿ƒé‡å­é›»è·¯åŠŸèƒ½ï¼Œ
ä¸¦èˆ‡ç¾æœ‰çš„ Trading X é‡å­ç³»çµ±å®Œç¾æ•´åˆã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
- é‡å­ç‰¹å¾µç·¨ç¢¼ (angle, amplitude, multi-scale)
- é‡å­é›»è·¯åƒæ•¸åŒ– ansatz
- æ™‚é–“æ¼”åŒ–èˆ‡ Hamiltonian æ˜ å°„
- å™ªè²æ¨¡å‹èˆ‡çœŸæ©Ÿæ”¯æ´
- è®Šåˆ†è¨“ç·´å™¨ (SPSA, COBYLA)
- å›æ¸¬æ¨¡çµ„

æ•´åˆç‰¹æ€§ï¼š
- èˆ‡ regime_hmm_quantum.py çš„å³æ™‚æ•¸æ“šæµæ•´åˆ
- èˆ‡ quantum_decision_optimizer.py çš„æ±ºç­–å¼•æ“æ•´åˆ
- æ”¯æ´ Trading X ä¿¡è™Ÿè¼¸å‡ºæ ¼å¼

ä½œè€…: Trading X Quantum Team
ç‰ˆæœ¬: 1.0 - Trading X æ•´åˆç‰ˆ
"""

import datetime
import json
import logging
import math
import os
import pickle
import sys
import time
from datetime import timedelta
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ğŸ”® é‡å­ç´šå€å¡Šéˆæ­·å²æ•¸æ“šæ’·å–å™¨ - å¾çœŸå¯¦å‰µä¸–é–‹å§‹
from .blockchain_unlimited_extractor import ProductionConfig, QuantumBlockchainExtractor

# Qiskit é‡å­è¨ˆç®— - å…¼å®¹ Qiskit 2.x
try:
    from qiskit import ClassicalRegister, QuantumCircuit, transpile
    from qiskit.circuit import ParameterVector
    from qiskit.circuit.library import RealAmplitudes, TwoLocal
    from qiskit.quantum_info import SparsePauliOp

    # Qiskit 2.x ä½¿ç”¨ primitives è€Œä¸æ˜¯èˆŠçš„ algorithms
    try:
        from qiskit.primitives import StatevectorEstimator, StatevectorSampler
        PRIMITIVES_AVAILABLE = True
    except ImportError:
        try:
            from qiskit.primitives import Estimator, Sampler
            PRIMITIVES_AVAILABLE = True
        except ImportError:
            PRIMITIVES_AVAILABLE = False
    
    # å„ªåŒ–å™¨ï¼ˆä»ç„¶å­˜åœ¨æ–¼æŸäº›ç‰ˆæœ¬ä¸­ï¼‰
    try:
        from qiskit.algorithms.optimizers import COBYLA, SPSA
        OPTIMIZERS_AVAILABLE = True
    except ImportError:
        try:
            from qiskit_algorithms.optimizers import COBYLA, SPSA
            OPTIMIZERS_AVAILABLE = True
        except ImportError:
            OPTIMIZERS_AVAILABLE = False
    
    try:
        from qiskit import Aer
    except ImportError:
        try:
            from qiskit_aer import Aer
        except ImportError:
            Aer = None
    
    try:
        from qiskit.providers.aer.noise import (
            NoiseModel,
            depolarizing_error,
            thermal_relaxation_error,
        )
    except ImportError:
        try:
            from qiskit_aer.noise import (
                NoiseModel,
                depolarizing_error,
                thermal_relaxation_error,
            )
        except ImportError:
            NoiseModel = None
            depolarizing_error = None
            thermal_relaxation_error = None
    
    QISKIT_AVAILABLE = True
    QUANTUM_LIBS_AVAILABLE = True
except ImportError:
    QISKIT_AVAILABLE = False
    QUANTUM_LIBS_AVAILABLE = False
    print("âš ï¸ Qiskit æœªå®‰è£ï¼Œé‡å­é›»è·¯åŠŸèƒ½å°‡è¢«ç¦ç”¨")

# Progress bar
try:
    from tqdm import tqdm
except ImportError:
    # Fallback é€²åº¦æ¢
    def tqdm(iterable, **kwargs):
        return iterable

# Trading X æ•´åˆ
try:
    import os

    # æ•´åˆ X è³‡æ–™å¤¾çš„å€å¡Šéˆä¸»æ± æ•¸æ“šæº
    import sys

    # from .quantum_decision_optimizer import ProductionQuantumConfig  # å·²åˆªé™¤
    from .regime_hmm_quantum import TradingXä¿¡è™Ÿ, å³æ™‚å¹£å®‰æ•¸æ“šæ”¶é›†å™¨
    sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'X'))
    from binance_data_connector import BinanceDataConnector
    TRADING_X_AVAILABLE = True
except ImportError:
    try:
        import os

        # æ•´åˆ X è³‡æ–™å¤¾çš„å€å¡Šéˆä¸»æ± æ•¸æ“šæº
        import sys

        # from quantum_decision_optimizer import ProductionQuantumConfig  # å·²åˆªé™¤
        from regime_hmm_quantum import TradingXä¿¡è™Ÿ, å³æ™‚å¹£å®‰æ•¸æ“šæ”¶é›†å™¨
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'X'))
        from binance_data_connector import BinanceDataConnector
        TRADING_X_AVAILABLE = True
    except ImportError:
        print("âš ï¸ Trading X æ¨¡çµ„æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç¨ç«‹æ¨¡å¼")
        å³æ™‚å¹£å®‰æ•¸æ“šæ”¶é›†å™¨ = None
        TradingXä¿¡è™Ÿ = None
        # ProductionQuantumConfig = None  # å·²åˆªé™¤
        BinanceDataConnector = None
        TRADING_X_AVAILABLE = False

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')
logger = logging.getLogger('BTCQuantumUltimate')

# ---------------------------
# CONFIG: é‡å­æ¨¡å‹é…ç½®
# ---------------------------
QUANTUM_CONFIG = {
    'N_FEATURE_QUBITS': 6,
    'N_READOUT': 3,  # bear/side/bull
    'N_ANSATZ_LAYERS': 3,
    'ENCODING': 'multi-scale',  # 'angle' | 'amplitude' | 'multi-scale'
    'USE_STATEVECTOR': False,
    'SHOTS': 2048,
    'SPSA_ITER': 120,
    'SPSA_SETTINGS': {'a': 0.4, 'c': 0.15, 'A': 20, 'alpha': 0.602, 'gamma': 0.101},
    'NOISE_MODEL': True,
    'DEPOLARIZING_PROB': 0.002,
    'THERMAL_PARAMS': {'T1': 50e3, 'T2': 70e3, 'time': 50},
    'LOOKBACK': 30,
    'AHEAD': 3,
    'BULL_THRESHOLD': 0.02,
    'BEAR_THRESHOLD': -0.02,
    # ä¸ƒå¤§å¹£ç¨®å€å¡Šéˆä¸»æ± é…ç½®
    'BLOCKCHAIN_SYMBOLS': ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'XRPUSDT', 'DOGEUSDT', 'ADAUSDT']
}

# ---------------------------
# é‡å­é›»è·¯å»ºæ§‹å‡½æ•¸
# ---------------------------

def angle_encoding(qc, qubit_indices: List[int], features: np.ndarray, scale=1.0):
    """è§’åº¦ç·¨ç¢¼é‡å­ç‰¹å¾µï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE or qc is None:
        return
    for i, q in enumerate(qubit_indices):
        if i < len(features):
            angle = float(features[i]) * scale
            qc.ry(angle, q)

def amplitude_encoding(qc, qubit_indices: List[int], features: np.ndarray):
    """æŒ¯å¹…ç·¨ç¢¼é‡å­ç‰¹å¾µï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE or qc is None:
        return
    
    vec = np.zeros(2 ** len(qubit_indices))
    vec[:min(len(features), len(vec))] = features[:len(vec)]
    norm = np.linalg.norm(vec)
    if norm > 1e-12:
        vec = vec / norm
    qc.initialize(vec, qubit_indices)

def multi_scale_encoding(qc, qubit_indices: List[int], features: np.ndarray):
    """å¤šå°ºåº¦ç·¨ç¢¼é‡å­ç‰¹å¾µï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE or qc is None:
        return
    
    n_qubits = len(qubit_indices)
    if n_qubits < 2:
        angle_encoding(qc, qubit_indices, features)
        return
    
    # åˆ†çµ„ç·¨ç¢¼ï¼šçŸ­æœŸã€ä¸­æœŸã€é•·æœŸç‰¹å¾µ
    group_size = n_qubits // 3
    for i, start_idx in enumerate([0, group_size, 2 * group_size]):
        end_idx = start_idx + group_size if i < 2 else n_qubits
        group_qubits = qubit_indices[start_idx:end_idx]
        group_features = features[i * len(group_qubits):(i + 1) * len(group_qubits)]
        
        if len(group_features) > 0:
            angle_encoding(qc, group_qubits, group_features, scale=0.5 * (i + 1))

def entangle_chain(qc, qubits: List[int]):
    """éˆå¼ç³¾çºï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE or qc is None:
        return
    for i in range(len(qubits) - 1):
        qc.cx(qubits[i], qubits[i + 1])

def entangle_star(qc, qubits: List[int]):
    """æ˜Ÿå½¢ç³¾çºï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE or qc is None:
        return
    if len(qubits) < 2:
        return
    center = qubits[0]
    for q in qubits[1:]:
        qc.cx(center, q)

def build_param_ansatz(n_qubits: int, n_layers: int, prefix='theta') -> Tuple[Any, Any]:
    """æ§‹å»ºåƒæ•¸åŒ– ansatzï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE:
        return None, None
    
    pcount = n_layers * n_qubits * 2
    params = ParameterVector(prefix, length=pcount)
    qc = QuantumCircuit(n_qubits)
    
    idx = 0
    for layer in range(n_layers):
        # å–®é‡å­ä½æ—‹è½‰
        for q in range(n_qubits):
            qc.ry(params[idx], q)
            idx += 1
            qc.rz(params[idx], q)
            idx += 1
        
        # ç³¾çºå±¤
        if layer < n_layers - 1:
            entangle_chain(qc, list(range(n_qubits)))
    
    return qc, params

def apply_zz_interaction(qc, q1: int, q2: int, theta: float):
    """æ‡‰ç”¨ ZZ ç›¸äº’ä½œç”¨ï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE or qc is None:
        return
    qc.cx(q1, q2)
    qc.rz(2 * theta, q2)
    qc.cx(q1, q2)

def apply_time_evolution(qc, feature_qubits: List[int], h: np.ndarray, J: np.ndarray, dt: float, trotter_steps: int = 1):
    """æ‡‰ç”¨æ™‚é–“æ¼”åŒ–ï¼ˆå…¼å®¹ Qiskit ä¸å¯ç”¨æƒ…æ³ï¼‰"""
    if not QUANTUM_LIBS_AVAILABLE or qc is None:
        return
    n = len(feature_qubits)
    
    for _ in range(trotter_steps):
        # å–®é«”é …æ¼”åŒ–
        for i in range(n):
            if i < len(h):
                qc.rz(2 * h[i] * dt, feature_qubits[i])
        
        # ç›¸äº’ä½œç”¨é …æ¼”åŒ–
        for i in range(n):
            for j in range(i + 1, min(n, len(h))):
                if abs(J[i, j]) > 1e-12:
                    apply_zz_interaction(qc, feature_qubits[i], feature_qubits[j], J[i, j] * dt)

# ---------------------------
# ç‰¹å¾µåˆ° Hamiltonian æ˜ å°„
# ---------------------------

def feature_to_hJ_advanced(feature_vec: np.ndarray, n_qubits: int, mapping_mode: str = 'hybrid') -> Tuple[np.ndarray, np.ndarray]:
    """é€²éšç‰¹å¾µåˆ° Hamiltonian æ˜ å°„"""
    v = np.zeros(n_qubits)
    v[:min(len(feature_vec), n_qubits)] = feature_vec[:n_qubits]
    
    # æ­£è¦åŒ–
    norm = np.linalg.norm(v)
    if norm > 1e-12:
        v = v / norm
    
    # h: ç·šæ€§ + éç·šæ€§è®Šæ›
    h = 0.6 * v + 0.4 * np.tanh(v)
    
    # J: å¤šå°ºåº¦å¤–ç© + è·é›¢è¡°æ¸›
    J = np.outer(v, v) * 0.25
    
    # è·é›¢è¡°æ¸›ï¼ˆé‡å­ä½ç´¢å¼•ä½œç‚ºé »ç‡å¸¶ï¼‰
    for i in range(n_qubits):
        for j in range(n_qubits):
            dist = abs(i - j)
            J[i, j] *= math.exp(-0.5 * dist)
    
    np.fill_diagonal(J, 0.0)
    return h, J

# ---------------------------
# é‡å­æ¸¬é‡èˆ‡è©•ä¼°
# ---------------------------

def statevector_expectation_z(statevector: np.ndarray, n_qubits: int, target: int) -> float:
    """è¨ˆç®— Z ç®—å­æœŸæœ›å€¼"""
    exp = 0.0
    dim = len(statevector)
    
    for k in range(dim):
        amp = statevector[k]
        prob = np.abs(amp) ** 2
        bit = (k >> (n_qubits - 1 - target)) & 1
        exp += prob * (1.0 if bit == 0 else -1.0)
    
    return exp

def softmax(x: np.ndarray) -> np.ndarray:
    """è»Ÿæœ€å¤§åŒ–å‡½æ•¸"""
    ex = np.exp(x - np.max(x))
    return ex / (np.sum(ex) + 1e-12)

# ---------------------------
# é‡å­é›»è·¯è©•ä¼°ä¸»å‡½æ•¸
# ---------------------------

def evaluate_quantum_circuit(theta: np.ndarray, feature_vec: np.ndarray, h: np.ndarray, J: np.ndarray, 
                            n_feature_qubits: int, n_readout: int, n_ansatz_layers: int, 
                            encoding: str, use_statevector: bool, shots: int, 
                            noise_model = None, quantum_backend = None) -> Tuple[np.ndarray, np.ndarray]:
    """è©•ä¼°çœŸå¯¦é‡å­é›»è·¯ - å¼·åˆ¶ä½¿ç”¨é‡å­å¾Œç«¯"""
    
    if not QUANTUM_LIBS_AVAILABLE:
        raise RuntimeError("âŒ é‡å­è¨ˆç®—åº«æœªå®‰è£ - æ­¤ç³»çµ±éœ€è¦çœŸå¯¦é‡å­è¨ˆç®—èƒ½åŠ›")
    
    if quantum_backend is None:
        raise RuntimeError("âŒ æœªæŒ‡å®šé‡å­å¾Œç«¯ - å¿…é ˆä½¿ç”¨çœŸå¯¦é‡å­ç¡¬é«”æˆ–é«˜ä¿çœŸåº¦å™ªè²æ¨¡æ“¬å™¨")
    
    try:
        total_qubits = n_feature_qubits + n_readout
        feat_idx = list(range(n_feature_qubits))
        read_idx = list(range(n_feature_qubits, total_qubits))
        
        qc = QuantumCircuit(total_qubits)
        
        # ç‰¹å¾µç·¨ç¢¼
        if encoding == 'angle':
            f = np.zeros(n_feature_qubits)
            f[:len(feature_vec)] = feature_vec[:n_feature_qubits]
            angle_encoding(qc, feat_idx, f, scale=1.0)
        elif encoding == 'amplitude':
            amplitude_encoding(qc, feat_idx, feature_vec)
        elif encoding == 'multi-scale':
            multi_scale_encoding(qc, feat_idx, feature_vec)
        
        # æ™‚é–“æ¼”åŒ–
        if len(h) >= n_feature_qubits and J.shape[0] >= n_feature_qubits:
            apply_time_evolution(qc, feat_idx, h[:n_feature_qubits], J[:n_feature_qubits, :n_feature_qubits], dt=0.1)
        
        # åƒæ•¸åŒ– ansatz
        ansatz, params = build_param_ansatz(n_readout, n_ansatz_layers)
        if ansatz is not None and params is not None:
            try:
                # æ–°ç‰ˆ Qiskit çš„åƒæ•¸ç¶å®šæ–¹å¼
                param_dict = {params[i]: theta[i] if i < len(theta) else 0.0 for i in range(len(params))}
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ assign_parameters æ–¹æ³•ï¼ˆæ–°ç‰ˆï¼‰
                if hasattr(ansatz, 'assign_parameters'):
                    bound_ansatz = ansatz.assign_parameters(param_dict)
                elif hasattr(ansatz, 'bind_parameters'):
                    bound_ansatz = ansatz.bind_parameters(param_dict)
                else:
                    # å¦‚æœéƒ½æ²’æœ‰ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ ansatz
                    bound_ansatz = ansatz
                
                # å°‡ ansatz æ·»åŠ åˆ°ä¸»é›»è·¯
                if hasattr(qc, 'compose'):
                    qc = qc.compose(bound_ansatz, qubits=list(range(n_readout)))
                else:
                    # èˆŠç‰ˆæœ¬çš„æ·»åŠ æ–¹å¼
                    qc += bound_ansatz
                    
            except Exception as e:
                logger.warning(f"åƒæ•¸ç¶å®šå¤±æ•—ï¼Œä½¿ç”¨é»˜èª ansatz: {e}")
                # ç°¡å–®çš„é»˜èª ansatz
                for q in range(n_readout):
                    qc.ry(0.1, q)
                    qc.rz(0.1, q)
        
        # æ¸¬é‡
        if use_statevector:
            # é«˜ä¿çœŸåº¦ç‹€æ…‹å‘é‡è¨ˆç®—
            if not hasattr(quantum_backend, 'name') or 'statevector' not in str(quantum_backend.name):
                raise RuntimeError("âŒ ç‹€æ…‹å‘é‡æ¨¡å¼éœ€è¦æ”¯æ´ç‹€æ…‹å‘é‡çš„é‡å­å¾Œç«¯")
            
            transpiled_qc = transpile(qc, quantum_backend, optimization_level=3)
            job = quantum_backend.run(transpiled_qc, shots=1)
            result = job.result()
            statevector = result.get_statevector()
            
            expectations = []
            for i in range(n_readout):
                exp_val = statevector_expectation_z(statevector, total_qubits, n_feature_qubits + i)
                expectations.append(exp_val)
            
            return np.array(expectations), np.array([1.0])
        else:
            # çœŸå¯¦é‡å­æ¸¬é‡ï¼ˆåŒ…å«å™ªè²ï¼‰
            qc.add_register(ClassicalRegister(n_readout))
            for i, q in enumerate(read_idx):
                qc.measure(q, i)
            
            # é‡å­é›»è·¯å„ªåŒ–ç·¨è­¯
            transpiled_qc = transpile(qc, quantum_backend, optimization_level=3)
            
            # åŸ·è¡ŒçœŸå¯¦é‡å­è¨ˆç®—
            if noise_model:
                job = quantum_backend.run(transpiled_qc, shots=shots, noise_model=noise_model)
            else:
                job = quantum_backend.run(transpiled_qc, shots=shots)
            
            result = job.result()
            counts = result.get_counts()
            
            # è™•ç†çœŸå¯¦é‡å­æ¸¬é‡çµæœ
            expectations = np.zeros(n_readout)
            total_shots = sum(counts.values())
            
            if total_shots == 0:
                raise RuntimeError("âŒ é‡å­æ¸¬é‡å¤±æ•— - æœªç²å¾—æœ‰æ•ˆæ¸¬é‡çµæœ")
            
            for bitstring, count in counts.items():
                prob = count / total_shots
                for i in range(min(n_readout, len(bitstring))):
                    bit = int(bitstring[-(i+1)])  # å¾å³åˆ°å·¦è®€å–
                    expectations[i] += prob * (1.0 if bit == 0 else -1.0)
            
            return expectations, np.array([total_shots])
    
    except Exception as e:
        logger.error(f"âŒ çœŸå¯¦é‡å­é›»è·¯åŸ·è¡Œå¤±æ•—: {e}")
        raise RuntimeError(f"é‡å­è¨ˆç®—åŸ·è¡Œå¤±æ•—: {e}")

# ---------------------------
# çœŸå¯¦é‡å­å¾Œç«¯ç®¡ç†å™¨
# ---------------------------

class QuantumBackendManager:
    """çœŸå¯¦é‡å­å¾Œç«¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.backends = {}
        self.current_backend = None
        self.error_mitigation_enabled = True
        
    def initialize_ibm_quantum(self, token: str = None):
        """åˆå§‹åŒ– IBM Quantum å¾Œç«¯"""
        try:
            from qiskit import IBMQ
            
            if token:
                IBMQ.save_account(token, overwrite=True)
            
            provider = IBMQ.load_account()
            
            # ç²å–å¯ç”¨çš„çœŸå¯¦é‡å­è¨­å‚™
            quantum_backends = provider.backends(
                filters=lambda x: x.configuration().n_qubits >= 5 and 
                                x.status().operational == True
            )
            
            if not quantum_backends:
                # å¦‚æœæ²’æœ‰å¯ç”¨çš„çœŸå¯¦è¨­å‚™ï¼Œä½¿ç”¨æœ€é«˜ä¿çœŸåº¦çš„æ¨¡æ“¬å™¨
                backend = provider.get_backend('ibmq_qasm_simulator')
                logger.info("ğŸ”® æœªæ‰¾åˆ°å°ˆç”¨é‡å­ç¡¬é«”ï¼Œä½¿ç”¨ Qiskit Aer é‡å­è¨ˆç®—å¾Œç«¯")
            else:
                # é¸æ“‡é‡å­ä½æ•¸æœ€å¤šä¸”éšŠåˆ—æœ€çŸ­çš„è¨­å‚™
                backend = min(quantum_backends, key=lambda x: x.status().pending_jobs)
                logger.info(f"âœ… å·²é€£æ¥åˆ°çœŸå¯¦é‡å­è¨­å‚™: {backend.name}")
            
            self.backends['ibm'] = backend
            self.current_backend = backend
            return backend
            
        except Exception as e:
            logger.error(f"âŒ IBM Quantum åˆå§‹åŒ–å¤±æ•—: {e}")
            raise RuntimeError(f"ç„¡æ³•åˆå§‹åŒ– IBM Quantum å¾Œç«¯: {e}")
    
    def initialize_local_high_fidelity(self):
        """åˆå§‹åŒ– Qiskit Aer çœŸå¯¦é‡å­è¨ˆç®—å¾Œç«¯"""
        if not QUANTUM_LIBS_AVAILABLE or Aer is None:
            raise RuntimeError("âŒ Qiskit Aer æœªå®‰è£")
        
        # ä½¿ç”¨ Qiskit Aer çœŸå¯¦é‡å­è¨ˆç®—å¾Œç«¯
        backend = Aer.get_backend('qasm_simulator')
        
        # é…ç½®çœŸå¯¦çš„é‡å­å™ªè²æ¨¡å‹
        noise_model = self._create_realistic_noise_model()
        
        self.backends['local_hf'] = backend
        self.current_backend = backend
        self.noise_model = noise_model
        
        logger.info("âœ… å·²åˆå§‹åŒ– Qiskit Aer é‡å­è¨ˆç®—å¾Œç«¯ï¼ˆå«çœŸå¯¦é‡å­å™ªè²æ¨¡å‹ï¼‰")
        return backend
    
    def _create_realistic_noise_model(self):
        """å‰µå»ºçœŸå¯¦çš„é‡å­å™ªè²æ¨¡å‹"""
        if not NoiseModel:
            return None
        
        noise_model = NoiseModel()
        
        # åŸºæ–¼çœŸå¯¦é‡å­è¨­å‚™çš„éŒ¯èª¤ç‡
        # å–®é‡å­ä½éŒ¯èª¤ - ä½¿ç”¨è¤‡åˆéŒ¯èª¤æ¨¡å‹
        error_1q = depolarizing_error(0.001, 1)  # 0.1% éŒ¯èª¤ç‡
        
        # é›™é‡å­ä½éŒ¯èª¤
        error_2q = depolarizing_error(0.01, 2)   # 1% éŒ¯èª¤ç‡
        
        # ç†±å¼›è±«éŒ¯èª¤ - èˆ‡å»æ¥µåŒ–éŒ¯èª¤æ•´åˆï¼Œé¿å…é‡è¤‡
        if thermal_relaxation_error:
            try:
                thermal_error = thermal_relaxation_error(
                    t1=50e3,  # T1 æ™‚é–“ 50å¾®ç§’
                    t2=70e3,  # T2 æ™‚é–“ 70å¾®ç§’  
                    time=50   # é–€æ™‚é–“ 50ç´ç§’
                )
                # å°‡ç†±å¼›è±«éŒ¯èª¤èˆ‡å»æ¥µåŒ–éŒ¯èª¤çµ„åˆ
                combined_error_1q = error_1q.compose(thermal_error)
                noise_model.add_all_qubit_quantum_error(combined_error_1q, ['u1', 'u2', 'u3'])
            except Exception:
                # å¦‚æœçµ„åˆå¤±æ•—ï¼Œåªä½¿ç”¨å»æ¥µåŒ–éŒ¯èª¤
                noise_model.add_all_qubit_quantum_error(error_1q, ['u1', 'u2', 'u3'])
        else:
            # åªæ·»åŠ å»æ¥µåŒ–éŒ¯èª¤
            noise_model.add_all_qubit_quantum_error(error_1q, ['u1', 'u2', 'u3'])
        
        # æ·»åŠ é›™é‡å­ä½éŒ¯èª¤
        noise_model.add_all_qubit_quantum_error(error_2q, ['cx'])
        
        return noise_model
    
    def get_current_backend(self):
        """ç²å–ç•¶å‰é‡å­å¾Œç«¯"""
        if self.current_backend is None:
            raise RuntimeError("âŒ æœªåˆå§‹åŒ–ä»»ä½•é‡å­å¾Œç«¯")
        return self.current_backend
    
    def enable_error_mitigation(self):
        """å•Ÿç”¨é‡å­éŒ¯èª¤ç·©è§£"""
        self.error_mitigation_enabled = True
        logger.info("âœ… å·²å•Ÿç”¨é‡å­éŒ¯èª¤ç·©è§£")
    
    def apply_error_mitigation(self, circuit, backend, shots: int):
        """æ‡‰ç”¨é‡å­éŒ¯èª¤ç·©è§£æŠ€è¡“"""
        if not self.error_mitigation_enabled:
            return circuit, shots
        
        # é›¶å™ªè²å¤–æ¨ï¼ˆZero Noise Extrapolationï¼‰
        noise_factors = [1.0, 1.5, 2.0]  # å™ªè²æ”¾å¤§å› å­
        extrapolated_circuits = []
        
        for factor in noise_factors:
            # å‰µå»ºå™ªè²æ”¾å¤§çš„é›»è·¯
            mitigated_circuit = self._amplify_noise(circuit, factor)
            extrapolated_circuits.append(mitigated_circuit)
        
        # è®€å‡ºéŒ¯èª¤ç·©è§£
        calibration_circuits = self._create_readout_calibration_circuits(backend)
        
        return extrapolated_circuits, shots, calibration_circuits
    
    def _amplify_noise(self, circuit, factor: float):
        """å™ªè²æ”¾å¤§ç”¨æ–¼é›¶å™ªè²å¤–æ¨"""
        # é€šéæ’å…¥é¡å¤–çš„å–®ä½é–€ä¾†æ”¾å¤§å™ªè²
        amplified_circuit = circuit.copy()
        
        if factor > 1.0:
            # åœ¨æ¯å€‹é–€å¾Œæ’å…¥æ†ç­‰é–€å°ä¾†æ”¾å¤§å™ªè²
            identity_pairs = int((factor - 1.0) * 2)
            for _ in range(identity_pairs):
                for qubit in range(circuit.num_qubits):
                    amplified_circuit.x(qubit)
                    amplified_circuit.x(qubit)  # Xâ€ X = I
        
        return amplified_circuit
    
    def _create_readout_calibration_circuits(self, backend):
        """å‰µå»ºè®€å‡ºæ ¡æº–é›»è·¯"""
        calibration_circuits = []
        n_qubits = min(backend.configuration().n_qubits, 10)  # é™åˆ¶é‡å­ä½æ•¸
        
        # |0âŸ© ç‹€æ…‹æ ¡æº–
        qc_0 = QuantumCircuit(n_qubits, n_qubits)
        qc_0.measure_all()
        calibration_circuits.append(qc_0)
        
        # |1âŸ© ç‹€æ…‹æ ¡æº–
        qc_1 = QuantumCircuit(n_qubits, n_qubits)
        for i in range(n_qubits):
            qc_1.x(i)
        qc_1.measure_all()
        calibration_circuits.append(qc_1)
        
        return calibration_circuits

# å…¨å±€é‡å­å¾Œç«¯ç®¡ç†å™¨å¯¦ä¾‹
quantum_backend_manager = QuantumBackendManager()

# ---------------------------
# é‡å­å„ªå‹¢é©—è­‰å™¨
# ---------------------------

class QuantumAdvantageValidator:
    """é‡å­å„ªå‹¢é©—è­‰å™¨ - é©—è­‰é‡å­è¨ˆç®—ç›¸å°æ–¼å¤å…¸è¨ˆç®—çš„å„ªå‹¢"""
    
    def __init__(self):
        self.benchmark_results = {}
        
    def validate_quantum_advantage(self, X_sample: np.ndarray, quantum_backend) -> float:
        """é©—è­‰é‡å­å„ªå‹¢
        
        Returns:
            float: é‡å­å„ªå‹¢åˆ†æ•¸ (0-1, è¶Šé«˜è¡¨ç¤ºé‡å­å„ªå‹¢è¶Šæ˜é¡¯)
        """
        logger.info("ğŸ”¬ é–‹å§‹é‡å­å„ªå‹¢é©—è­‰...")
        
        try:
            # 1. é‡å­ç›¸å¹²æ€§æ¸¬è©¦
            coherence_score = self._test_quantum_coherence(quantum_backend)
            
            # 2. é‡å­ç³¾çºæ¸¬è©¦
            entanglement_score = self._test_quantum_entanglement(quantum_backend)
            
            # 3. é‡å­ä¸¦è¡Œæ€§æ¸¬è©¦
            parallelism_score = self._test_quantum_parallelism(X_sample, quantum_backend)
            
            # 4. ç¶œåˆé‡å­å„ªå‹¢åˆ†æ•¸
            quantum_advantage_score = (
                0.3 * coherence_score + 
                0.4 * entanglement_score + 
                0.3 * parallelism_score
            )
            
            self.benchmark_results = {
                'coherence_score': coherence_score,
                'entanglement_score': entanglement_score,
                'parallelism_score': parallelism_score,
                'total_score': quantum_advantage_score,
                'backend_name': getattr(quantum_backend, 'name', str(quantum_backend))
            }
            
            logger.info(f"âœ… é‡å­å„ªå‹¢é©—è­‰å®Œæˆ:")
            logger.info(f"   ç›¸å¹²æ€§åˆ†æ•¸: {coherence_score:.3f}")
            logger.info(f"   ç³¾çºåˆ†æ•¸: {entanglement_score:.3f}")
            logger.info(f"   ä¸¦è¡Œæ€§åˆ†æ•¸: {parallelism_score:.3f}")
            logger.info(f"   ç¸½é«”é‡å­å„ªå‹¢: {quantum_advantage_score:.3f}")
            
            return quantum_advantage_score
            
        except Exception as e:
            logger.error(f"âŒ é‡å­å„ªå‹¢é©—è­‰å¤±æ•—: {e}")
            return 0.0
    
    def _test_quantum_coherence(self, backend) -> float:
        """æ¸¬è©¦é‡å­ç›¸å¹²æ€§ - ä½¿ç”¨ Qiskit 2.x Primitives API"""
        try:
            from qiskit_aer.primitives import Sampler

            # ç²å–å¾Œç«¯çš„é‡å­ä½æ•¸ (Qiskit 2.x å…¼å®¹)
            try:
                n_qubits = min(3, backend.configuration().n_qubits)
            except:
                n_qubits = 3
            
            qc = QuantumCircuit(n_qubits)
            
            # å‰µå»º GHZ æ…‹: (|000âŸ© + |111âŸ©)/âˆš2
            qc.h(0)
            for i in range(1, n_qubits):
                qc.cx(0, i)
            
            # æ·»åŠ æ¸¬é‡åˆ°æ‰€æœ‰é‡å­ä½
            qc.measure_all()
            
            # ä½¿ç”¨ Qiskit 2.x Primitives API - AerSampler
            sampler = Sampler()
            job = sampler.run([qc], shots=1000)
            result = job.result()
            
            # ç²å–è¨ˆæ•¸ - Qiskit 2.x æ­£ç¢ºæ–¹å¼
            quasi_dist = result.quasi_dists[0]
            
            # è½‰æ›ç‚ºçœŸå¯¦è¨ˆæ•¸
            total_shots = 1000
            counts = {}
            for outcome, probability in quasi_dist.items():
                # å°‡ int outcome è½‰æ›ç‚º binary string
                binary_outcome = format(outcome, f'0{n_qubits}b')
                counts[binary_outcome] = int(probability * total_shots)
            
            # è¨ˆç®—ç›¸å¹²æ€§åˆ†æ•¸
            if counts:
                # GHZ æ…‹çš„ç›¸å¹²æ€§ï¼šåªæœ‰ |000âŸ© å’Œ |111âŸ© çš„æ¦‚ç‡
                coherent_states = counts.get('0' * n_qubits, 0) + counts.get('1' * n_qubits, 0)
                coherence_score = coherent_states / total_shots
                return coherence_score
            else:
                return 0.0
                    
        except Exception as e:
            logger.error(f"ç›¸å¹²æ€§æ¸¬è©¦å¤±æ•—: {e}")
            return 0.0
    
    def _test_quantum_entanglement(self, backend) -> float:
        """æ¸¬è©¦é‡å­ç³¾çº - ä½¿ç”¨ Qiskit 2.x Primitives API"""
        try:
            from qiskit_aer.primitives import Sampler

            # Bell æ…‹ç³¾çºæ¸¬è©¦
            qc = QuantumCircuit(2)
            
            # å‰µå»º Bell æ…‹: (|00âŸ© + |11âŸ©)/âˆš2
            qc.h(0)
            qc.cx(0, 1)
            
            # æ·»åŠ æ¸¬é‡
            qc.measure_all()
            
            # ä½¿ç”¨ Qiskit 2.x Primitives API - AerSampler
            sampler = Sampler()
            job = sampler.run([qc], shots=1000)
            result = job.result()
            
            # ç²å–è¨ˆæ•¸ - Qiskit 2.x æ­£ç¢ºæ–¹å¼
            quasi_dist = result.quasi_dists[0]
            
            # è½‰æ›ç‚ºçœŸå¯¦è¨ˆæ•¸
            total_shots = 1000
            counts = {}
            for outcome, probability in quasi_dist.items():
                # å°‡ int outcome è½‰æ›ç‚º binary string
                binary_outcome = format(outcome, '02b')  # 2 qubits
                counts[binary_outcome] = int(probability * total_shots)
            
            # è¨ˆç®—ç³¾çºåˆ†æ•¸ï¼ˆBell æ…‹æ‡‰è©²åªæœ‰ |00âŸ© å’Œ |11âŸ©ï¼‰
            if counts:
                entangled_states = counts.get('00', 0) + counts.get('11', 0)
                entanglement_score = entangled_states / total_shots
                return entanglement_score
            else:
                return 0.0
                    
        except Exception as e:
            logger.error(f"ç³¾çºæ¸¬è©¦å¤±æ•—: {e}")
            return 0.0
    
    def _test_quantum_parallelism(self, X_sample: np.ndarray, backend) -> float:
        """æ¸¬è©¦é‡å­ä¸¦è¡Œæ€§ - ä½¿ç”¨ Qiskit 2.x Primitives API"""
        try:
            from qiskit_aer.primitives import Sampler

            # ç°¡åŒ–çš„é‡å­ä¸¦è¡Œæ€§æ¸¬è©¦
            try:
                max_qubits = backend.configuration().n_qubits
            except:
                max_qubits = 4  # é»˜èªå€¼
            
            n_qubits = min(4, max_qubits, len(X_sample))
            qc = QuantumCircuit(n_qubits)
            
            # å‰µå»ºå‡å‹»ç–ŠåŠ æ…‹
            for i in range(n_qubits):
                qc.h(i)
            
            # æ‡‰ç”¨ç›¸ä½ç¿»è½‰ï¼ˆæ¨¡æ“¬é‡å­ä¸¦è¡Œæœç´¢ï¼‰
            # åŸºæ–¼è¼¸å…¥æ•¸æ“šçš„ç‰¹å®šæ¨¡å¼
            target_pattern = np.mean(X_sample[:n_qubits]) > 0
            if target_pattern:
                qc.cz(0, 1) if n_qubits > 1 else qc.z(0)
            
            # åæ¼”é—œæ–¼å¹³å‡å€¼
            for i in range(n_qubits):
                qc.h(i)
                qc.x(i)
            
            if n_qubits > 1:
                qc.cz(0, 1)
            
            for i in range(n_qubits):
                qc.x(i)
                qc.h(i)
            
            # æ·»åŠ æ¸¬é‡
            qc.measure_all()
            
            # ä½¿ç”¨ Qiskit 2.x Primitives API - AerSampler
            sampler = Sampler()
            job = sampler.run([qc], shots=1000)
            result = job.result()
            
            # ç²å–è¨ˆæ•¸ - Qiskit 2.x æ­£ç¢ºæ–¹å¼
            quasi_dist = result.quasi_dists[0]
            
            # è½‰æ›ç‚ºçœŸå¯¦è¨ˆæ•¸
            total_shots = 1000
            counts = {}
            for outcome, probability in quasi_dist.items():
                # å°‡ int outcome è½‰æ›ç‚º binary string
                binary_outcome = format(outcome, f'0{n_qubits}b')
                counts[binary_outcome] = int(probability * total_shots)
            
            # è©•ä¼°æœç´¢æ•ˆæœ
            max_count = max(counts.values()) if counts else 0
            parallelism_score = max_count / total_shots
            
            return parallelism_score
            
        except Exception as e:
            logger.error(f"ä¸¦è¡Œæ€§æ¸¬è©¦å¤±æ•—: {e}")
            return 0.0

# ---------------------------
# BTC é‡å­çµ‚æ¥µæ¨¡å‹é¡
# ---------------------------

class BTCQuantumUltimateModel:
    """BTC é‡å­çµ‚æ¥µæ¨¡å‹ - çœŸå¯¦é‡å­è¨ˆç®—ç‰ˆæœ¬"""
    
    def __init__(self, config: Dict[str, Any] = None, quantum_backend_type: str = 'ibm'):
        # ä½¿ç”¨é»˜èªé…ç½®æˆ–æä¾›çš„é…ç½®
        if config is None:
            config = QUANTUM_CONFIG.copy()
        
        self.config = config
        
        self.scaler = StandardScaler()
        self.pca = None
        self.theta = None
        self.training_history = []
        self.is_fitted = False
        
        # çœŸå¯¦é‡å­å¾Œç«¯åˆå§‹åŒ–
        self.quantum_backend_manager = quantum_backend_manager
        self.quantum_backend = None
        self._initialize_quantum_backend(quantum_backend_type)
        
        # Trading X æ•´åˆ
        self.data_collector = None
        self.signal_history = []
        
        # ğŸ”® é‡å­ç´šå€å¡Šéˆæ•¸æ“šæ’·å–å™¨
        self.quantum_extractor = None  # å°‡åœ¨éœ€è¦æ™‚åˆå§‹åŒ–
        
        # å‚³çµ±å€å¡Šéˆä¸»æ± æ•¸æ“šé€£æ¥å™¨ï¼ˆå‚™ç”¨ï¼‰
        self.blockchain_connector = None
        if TRADING_X_AVAILABLE and BinanceDataConnector:
            self.blockchain_connector = BinanceDataConnector()
        
        # é‡å­å„ªå‹¢é©—è­‰å™¨
        self.quantum_advantage_validator = QuantumAdvantageValidator()
        
        logger.info(f"ğŸ”® BTC é‡å­çµ‚æ¥µæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆQiskit 2.x ç‰ˆæœ¬ï¼‰")
        logger.info(f"   ç‰¹å¾µé‡å­ä½: {self.config['N_FEATURE_QUBITS']}")
        logger.info(f"   Ansatzå±¤æ•¸: {self.config['N_ANSATZ_LAYERS']}")
        logger.info(f"   ç·¨ç¢¼æ–¹å¼: {self.config['ENCODING']}")
        logger.info(f"   é‡å­å¾Œç«¯: {getattr(self.quantum_backend, 'name', 'qasm_simulator') if self.quantum_backend else 'æœªåˆå§‹åŒ–'}")
        logger.info(f"   éŒ¯èª¤ç·©è§£: {'âœ… å·²å•Ÿç”¨' if self.quantum_backend_manager.error_mitigation_enabled else 'âŒ æœªå•Ÿç”¨'}")
        logger.info(f"   æ”¯æ´å¹£ç¨®: {', '.join(self.config.get('BLOCKCHAIN_SYMBOLS', ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']))}")
    
    @property
    def is_trained(self):
        """å‘å¾Œå…¼å®¹çš„ is_trained å±¬æ€§ï¼Œæ˜ å°„åˆ° is_fitted"""
        return self.is_fitted
    
    @is_trained.setter
    def is_trained(self, value):
        """å‘å¾Œå…¼å®¹çš„ is_trained å±¬æ€§è¨­ç½®å™¨"""
        self.is_fitted = value
    
    def _initialize_quantum_backend(self, backend_type: str):
        """åˆå§‹åŒ–çœŸå¯¦é‡å­å¾Œç«¯"""
        try:
            if backend_type == 'ibm':
                # å˜—è©¦å¾ç’°å¢ƒè®Šæ•¸ç²å– IBM Quantum token
                import os
                ibm_token = os.getenv('IBM_QUANTUM_TOKEN')
                if ibm_token:
                    self.quantum_backend = self.quantum_backend_manager.initialize_ibm_quantum(ibm_token)
                else:
                    logger.info("ğŸ”® ä½¿ç”¨ Qiskit Aer é‡å­è¨ˆç®—å¾Œç«¯ (æ¨™æº–é‡å­è¨ˆç®—ç’°å¢ƒ)")
                    self.quantum_backend = self.quantum_backend_manager.initialize_local_high_fidelity()
            else:
                self.quantum_backend = self.quantum_backend_manager.initialize_local_high_fidelity()
            
            # å•Ÿç”¨é‡å­éŒ¯èª¤ç·©è§£
            self.quantum_backend_manager.enable_error_mitigation()
            
        except Exception as e:
            logger.error(f"âŒ é‡å­å¾Œç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            raise RuntimeError(f"ç„¡æ³•åˆå§‹åŒ–é‡å­å¾Œç«¯: {e}")
    
    async def _initialize_quantum_extractor(self):
        """åˆå§‹åŒ–é‡å­ç´šæ•¸æ“šæ’·å–å™¨"""
        if self.quantum_extractor is None:
            self.quantum_extractor = QuantumBlockchainExtractor()
            await self.quantum_extractor.initialize()
            logger.info("âœ… é‡å­ç´šå€å¡Šéˆæ•¸æ“šæ’·å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def generate_unlimited_market_data(self, symbol: str, timeframe: str = '1d', days_back: int = None) -> pd.DataFrame:
        """
        ğŸ”® å¾é‡å­ç´šæ•¸æ“šæ’·å–å™¨ç²å–ç„¡é™åˆ¶æ­·å²æ•¸æ“š
        æ”¯æ´å¾å‰µä¸–æ—¥æœŸé–‹å§‹çš„å®Œæ•´æ­·å²æ•¸æ“š
        """
        await self._initialize_quantum_extractor()
        
        try:
            # ä½¿ç”¨é‡å­ç´šæ’·å–å™¨ç²å–å®Œæ•´æ­·å²æ•¸æ“š
            config = ProductionConfig()
            end_time = datetime.now()
            
            if days_back:
                start_time = end_time - timedelta(days=days_back)
            else:
                # ä½¿ç”¨çœŸå¯¦å‰µä¸–æ—¥æœŸ
                genesis_dates = config.REAL_GENESIS_DATES
                symbol_key = symbol.replace('USDT', '').upper()
                if symbol_key in genesis_dates:
                    start_time = genesis_dates[symbol_key]
                    logger.info(f"ğŸš€ ä½¿ç”¨çœŸå¯¦å‰µä¸–æ—¥æœŸ: {symbol} å¾ {start_time.strftime('%Y-%m-%d')} é–‹å§‹")
                else:
                    # é»˜èªä½¿ç”¨ BSC éƒ¨ç½²æ—¥æœŸ
                    start_time = config.BSC_DEPLOYMENT_DATES.get(symbol_key, end_time - timedelta(days=365))
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {symbol} çš„å‰µä¸–æ—¥æœŸï¼Œä½¿ç”¨ BSC éƒ¨ç½²æ—¥æœŸ")
            
            # ä½¿ç”¨é‡å­ç´šæ•¸æ“šæ’·å–å™¨
            market_data = await self.quantum_extractor.extract_unlimited_historical_data(
                symbol=symbol,
                start_date=start_time,
                end_date=end_time,
                interval=timeframe
            )
            
            if market_data is not None and not market_data.empty:
                logger.info(f"âœ… é‡å­ç´šæ•¸æ“šç²å–æˆåŠŸ: {symbol}")
                logger.info(f"   æ•¸æ“šç¯„åœ: {market_data.index[0]} è‡³ {market_data.index[-1]}")
                logger.info(f"   ç¸½å¤©æ•¸: {len(market_data)} æ¢è¨˜éŒ„")
                return market_data
            else:
                logger.warning(f"âš ï¸ é‡å­ç´šæ•¸æ“šæ’·å–å™¨ç„¡æ•¸æ“šï¼Œå›é€€è‡³å‚³çµ±æ–¹æ³•")
                return await self._fallback_to_traditional_data(symbol, timeframe, days_back or 1000)
                
        except Exception as e:
            logger.error(f"âŒ é‡å­ç´šæ•¸æ“šç²å–å¤±æ•—: {e}")
            logger.info("ğŸ”„ å›é€€è‡³å‚³çµ±å€å¡Šéˆæ•¸æ“šæº...")
            return await self._fallback_to_traditional_data(symbol, timeframe, days_back or 1000)
    
    async def _fallback_to_traditional_data(self, symbol: str, timeframe: str, limit: int) -> pd.DataFrame:
        """å›é€€è‡³å‚³çµ±å€å¡Šéˆæ•¸æ“šæº"""
        if not self.blockchain_connector:
            raise RuntimeError("âŒ ç„¡å¯ç”¨æ•¸æ“šæº - é‡å­ç´šæ’·å–å™¨å’Œå‚³çµ±é€£æ¥å™¨éƒ½ä¸å¯ç”¨")
        
        try:
            market_data = self.blockchain_connector.get_historical_klines(
                symbol=symbol,
                interval=timeframe,
                limit=limit
            )
            
            df = pd.DataFrame(market_data, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])
            
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df['close'] = df['close'].astype(float)
            df.set_index('timestamp', inplace=True)
            
            logger.info(f"âœ… å‚³çµ±æ•¸æ“šç²å–æˆåŠŸ: {symbol} - {len(df)} æ¢è¨˜éŒ„")
            return df[['close']].rename(columns={'close': 'price'})
            
        except Exception as e:
            logger.error(f"âŒ å‚³çµ±æ•¸æ“šç²å–å¤±æ•—: {e}")
            raise RuntimeError(f"æ‰€æœ‰æ•¸æ“šæºéƒ½ç„¡æ³•ç²å–æ•¸æ“š: {e}")
    
    def generate_realistic_market_data(self, symbol: str, timeframe: str = '1m', limit: int = 1000) -> pd.DataFrame:
        """
        ğŸ”® ç”ŸæˆçœŸå¯¦å¸‚å ´æ•¸æ“š - å„ªå…ˆä½¿ç”¨é‡å­ç´šæ’·å–å™¨
        
        æ­¤æ–¹æ³•ä¿æŒåŒæ­¥æ¥å£å…¼å®¹æ€§ï¼Œå…§éƒ¨èª¿ç”¨ç•°æ­¥é‡å­ç´šæ’·å–å™¨
        """
        import asyncio

        # æª¢æŸ¥æ˜¯å¦æœ‰é‹è¡Œä¸­çš„äº‹ä»¶å¾ªç’°
        try:
            loop = asyncio.get_running_loop()
            # å¦‚æœæœ‰é‹è¡Œä¸­çš„äº‹ä»¶å¾ªç’°ï¼Œå‰µå»ºä»»å‹™
            task = loop.create_task(self.generate_unlimited_market_data(symbol, timeframe, limit))
            return asyncio.run_coroutine_threadsafe(task, loop).result()
        except RuntimeError:
            # æ²’æœ‰é‹è¡Œä¸­çš„äº‹ä»¶å¾ªç’°ï¼Œç›´æ¥é‹è¡Œ
            return asyncio.run(self.generate_unlimited_market_data(symbol, timeframe, limit))
    
    def generate_realistic_market_data_legacy(self, symbol: str, timeframe: str = '1m', limit: int = 1000) -> pd.DataFrame:
        """å¾çœŸå¯¦å€å¡Šéˆæ•¸æ“šæºç”Ÿæˆå¸‚å ´æ•¸æ“šï¼ˆå‚³çµ±æ–¹æ³• - å·²æ£„ç”¨ï¼‰"""
        if not self.blockchain_connector:
            raise RuntimeError("âŒ å€å¡Šéˆæ•¸æ“šé€£æ¥å™¨æœªåˆå§‹åŒ– - æ­¤ç³»çµ±ä¸ä½¿ç”¨åˆæˆæ•¸æ“š")
        
        try:
            # å¾çœŸå¯¦å¹£å®‰æ•¸æ“šæºç²å–æ•¸æ“š
            market_data = self.blockchain_connector.get_historical_klines(
                symbol=symbol,
                interval=timeframe,
                limit=limit
            )
            
            df = pd.DataFrame(market_data, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])
            
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df['close'] = df['close'].astype(float)
            
            logger.info(f"âœ… å·²ç²å– {symbol} çœŸå¯¦å¸‚å ´æ•¸æ“š: {len(df)} æ¢è¨˜éŒ„")
            return df[['timestamp', 'close']]
            
        except Exception as e:
            logger.error(f"âŒ ç²å–çœŸå¯¦å¸‚å ´æ•¸æ“šå¤±æ•—: {e}")
            raise RuntimeError(f"ç„¡æ³•ç²å–çœŸå¯¦å¸‚å ´æ•¸æ“š: {e}")
    
    def _quantum_adaptive_sampling(self, X: np.ndarray) -> np.ndarray:
        """é‡å­è‡ªé©æ‡‰æ•¸æ“šæ¡æ¨£ - åŸºæ–¼é‡å­æ…‹åç¸®åŸç†"""
        try:
            # ä½¿ç”¨é‡å­æ…‹æ©Ÿç‡åˆ†ä½ˆé€²è¡Œè‡ªé©æ‡‰æ¡æ¨£
            total_samples = len(X)
            
            # é‡å­æ¡æ¨£å¤§å° - åŸºæ–¼æ•¸æ“šç¶­åº¦çš„é‡å­ä¸ç¢ºå®šæ€§åŸç†
            quantum_sample_size = min(
                total_samples,
                max(50, int(np.sqrt(total_samples) * np.log(X.shape[1] + 1)))  # é‡å­ç¶­åº¦ç›¸é—œ
            )
            
            # ä½¿ç”¨é‡å­éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨é€²è¡Œæ¡æ¨£
            if hasattr(self, '_generate_quantum_random_parameters'):
                # é‡å­éš¨æ©Ÿæ¡æ¨£ç´¢å¼•
                quantum_probs = self._generate_quantum_random_parameters(total_samples)
                quantum_probs = np.abs(quantum_probs) / np.sum(np.abs(quantum_probs))  # æ­£è¦åŒ–ç‚ºæ©Ÿç‡
                
                # åŸºæ–¼é‡å­æ©Ÿç‡åˆ†ä½ˆæ¡æ¨£
                sample_indices = np.random.choice(
                    total_samples, 
                    size=quantum_sample_size, 
                    replace=False, 
                    p=quantum_probs
                )
            else:
                # å›é€€åˆ°å‡å‹»æ¡æ¨£
                sample_indices = np.random.choice(total_samples, size=quantum_sample_size, replace=False)
            
            logger.info(f"ğŸ”® é‡å­è‡ªé©æ‡‰æ¡æ¨£: {quantum_sample_size}/{total_samples} å€‹æ¨£æœ¬")
            logger.info(f"   æ¡æ¨£æ¯”ä¾‹: {quantum_sample_size/total_samples:.3f}")
            logger.info(f"   é‡å­ç¶­åº¦è€ƒé‡: åŸºæ–¼ {X.shape[1]} ç‰¹å¾µç¶­åº¦")
            
            return X[sample_indices]
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­æ¡æ¨£å¤±æ•—ï¼Œä½¿ç”¨è‡ªé©æ‡‰æ¡æ¨£: {e}")
            # è‡ªé©æ‡‰å›é€€ç­–ç•¥
            adaptive_size = min(len(X), max(100, int(len(X) * 0.1)))  # 10% æˆ–æœ€å°‘100å€‹
            return X[:adaptive_size]
    
    def _calculate_quantum_batch_size(self, total_samples: int) -> int:
        """è¨ˆç®—é‡å­è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å° - åŸºæ–¼çœŸå¯¦é‡å­ç›¸å¹²æ™‚é–“æ¸¬é‡"""
        try:
            # å¯¦æ™‚æ¸¬é‡é‡å­ç›¸å¹²æ™‚é–“
            coherence_time = self._measure_quantum_coherence_time()
            coherence_limit = min(total_samples, int(coherence_time * 100))  # ç›¸å¹²æ™‚é–“è½‰æ›ç‚ºæ¨£æœ¬æ•¸
            
            # å‹•æ…‹é‡å­ç³¾çºè¤‡é›œåº¦ - åŸºæ–¼å¯¦éš›é›»è·¯æ·±åº¦
            circuit_depth = self.config['N_ANSATZ_LAYERS'] * self.config['N_FEATURE_QUBITS']
            entanglement_capacity = self._calculate_entanglement_capacity(circuit_depth)
            entanglement_limit = min(total_samples, entanglement_capacity)
            
            # é‡å­ä¸ç¢ºå®šæ€§æœ€å„ªåŒ– - æµ·æ£®å ¡åŸç†æ‡‰ç”¨
            uncertainty_factor = self._calculate_uncertainty_factor()
            uncertainty_optimal = int(np.sqrt(total_samples) * uncertainty_factor)
            
            # å‹•æ…‹é‡å­æ‰¹æ¬¡å¤§å°
            quantum_batch_size = min(
                coherence_limit,
                entanglement_limit, 
                uncertainty_optimal,
                max(int(total_samples * 0.01), 8)  # è‡³å°‘1%æˆ–8å€‹æ¨£æœ¬
            )
            
            logger.info(f"ğŸ”® é‡å­æ‰¹æ¬¡è¨ˆç®—:")
            logger.info(f"   å¯¦æ¸¬ç›¸å¹²æ™‚é–“: {coherence_time:.3f} (æ¨£æœ¬é™åˆ¶: {coherence_limit})")
            logger.info(f"   ç³¾çºå®¹é‡: {entanglement_capacity} (è¤‡é›œåº¦é™åˆ¶: {entanglement_limit})")
            logger.info(f"   ä¸ç¢ºå®šæ€§å› å­: {uncertainty_factor:.3f} (æœ€å„ª: {uncertainty_optimal})")
            logger.info(f"   æœ€çµ‚æ‰¹æ¬¡å¤§å°: {quantum_batch_size}/{total_samples}")
            
            return quantum_batch_size
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­æ‰¹æ¬¡è¨ˆç®—å¤±æ•—ï¼Œä½¿ç”¨æœ€å°å®‰å…¨å€¼: {e}")
            return min(8, total_samples)
    
    def prepare_features_and_labels(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """æº–å‚™ç‰¹å¾µå’Œæ¨™ç±¤"""
        df = df.copy()
        df['logret'] = np.log(df['close']).diff()
        df['ret_ahead'] = df['close'].shift(-self.config['AHEAD']) / df['close'] - 1.0
        
        # å¤šå°ºåº¦ç‰¹å¾µ
        scales = [5, 20, 60]
        feat_list = []
        
        for i in range(len(df)):
            if i < max(scales) or i + self.config['AHEAD'] >= len(df):
                feat_list.append(None)
                continue
            
            features = []
            for s in scales:
                window = df['logret'].iloc[i-s+1:i+1].values
                features.extend([
                    np.nan_to_num(window[-1]),  # æœ€å¾Œæ”¶ç›Šç‡
                    np.nan_to_num(np.std(window)),  # æ³¢å‹•ç‡
                    np.nan_to_num(np.mean(window)),  # å¹³å‡æ”¶ç›Šç‡
                    np.nan_to_num(pd.Series(window).skew()),  # ååº¦
                    np.nan_to_num(pd.Series(window).kurt())   # å³°åº¦
                ])
            
            # é¡å¤–æŠ€è¡“æŒ‡æ¨™
            short = df['logret'].iloc[i-5+1:i+1].values
            med = df['logret'].iloc[i-20+1:i+1].values
            features.append(np.nan_to_num(np.std(short) / (np.std(med) + 1e-8)))
            
            # å ä½ç¬¦ï¼ˆå¯ç”±ç”¨æˆ¶å¡«å…¥å¯¦éš›æŒ‡æ¨™ï¼‰
            features.extend([0.0, 0.0])  # è¨‚å–®ç°¿å¤±è¡¡ã€è³‡é‡‘è²»ç‡
            
            feat_list.append(features)
        
        df['features'] = feat_list
        
        # ç”Ÿæˆæ¨™ç±¤
        labels = []
        for r in df['ret_ahead'].values:
            if np.isnan(r):
                labels.append(None)
                continue
            
            if r >= self.config['BULL_THRESHOLD']:
                labels.append(2)  # ç‰›å¸‚
            elif r <= self.config['BEAR_THRESHOLD']:
                labels.append(0)  # ç†Šå¸‚
            else:
                labels.append(1)  # éœ‡ç›ª
        
        df['label'] = labels
        
        # æ¸…ç†æ•¸æ“š
        df_clean = df.dropna(subset=['features', 'label']).reset_index(drop=True)
        X = np.vstack(df_clean['features'].values)
        y = df_clean['label'].values.astype(int)
        
        return X, y
    
    def preprocess_features(self, X: np.ndarray, fit: bool = True) -> np.ndarray:
        """é è™•ç†ç‰¹å¾µ - è‡ªå‹•èª¿æ•´ç¶­åº¦ä»¥é¿å… PCA éŒ¯èª¤"""
        if fit:
            X_scaled = self.scaler.fit_transform(X)
            
            # è‡ªå‹•èª¿æ•´ PCA ç¶­åº¦ï¼šä¸èƒ½è¶…é min(n_samples, n_features)
            max_components = min(X.shape[0], X.shape[1])
            desired_components = self.config['N_FEATURE_QUBITS']
            actual_components = min(desired_components, max_components)
            
            logger.info(f"ğŸ”§ PCA ç¶­åº¦èª¿æ•´: æœŸæœ› {desired_components} â†’ å¯¦éš› {actual_components} (æ•¸æ“š: {X.shape})")
            
            self.pca = PCA(n_components=actual_components)
            X_reduced = self.pca.fit_transform(X_scaled)
        else:
            X_scaled = self.scaler.transform(X)
            X_reduced = self.pca.transform(X_scaled)
        
        return X_reduced
    
    def fit(self, X: np.ndarray, y: np.ndarray, verbose: bool = True):
        """ä½¿ç”¨ Qiskit 2.x ç¾ä»£ API å’Œå…§å»ºå„ªåŒ–å™¨è¨“ç·´é‡å­æ¨¡å‹"""
        logger.info("ğŸš€ é–‹å§‹ Qiskit 2.x é‡å­è¨“ç·´ï¼ˆç¾ä»£ primitives APIï¼‰...")
        
        if self.quantum_backend is None:
            raise RuntimeError("âŒ é‡å­å¾Œç«¯æœªåˆå§‹åŒ–")
        
        # å‹•æ…‹é‡å­å„ªå‹¢é©—è­‰
        quantum_advantage_score = self.quantum_advantage_validator.validate_quantum_advantage(
            self._quantum_adaptive_sampling(X), self.quantum_backend
        )
        
        logger.info(f"ğŸ”® é‡å­å„ªå‹¢åˆ†æ•¸: {quantum_advantage_score:.3f}")
        
        # é è™•ç†ç‰¹å¾µ
        X_processed = self.preprocess_features(X, fit=True)
        
        # æ ¹æ“šå¯¦éš›è™•ç†å¾Œçš„ç‰¹å¾µç¶­åº¦èª¿æ•´é‡å­æ¯”ç‰¹æ•¸
        actual_features = X_processed.shape[1]
        original_qubits = self.config['N_FEATURE_QUBITS']
        
        if actual_features != original_qubits:
            logger.info(f"ğŸ”§ é‡å­æ¯”ç‰¹æ•¸èª¿æ•´: {original_qubits} â†’ {actual_features} (åŒ¹é…å¯¦éš›ç‰¹å¾µç¶­åº¦)")
            # è‡¨æ™‚èª¿æ•´é…ç½®
            adjusted_qubits = actual_features
        else:
            adjusted_qubits = original_qubits
        
        # å‰µå»ºé‡å­ Ansatz é›»è·¯ï¼ˆä½¿ç”¨èª¿æ•´å¾Œçš„é‡å­æ¯”ç‰¹æ•¸ï¼‰
        num_qubits = adjusted_qubits
        ansatz = RealAmplitudes(num_qubits, reps=self.config['N_ANSATZ_LAYERS'])
        
        # æ§‹å»º Hamiltonianï¼ˆåŸºæ–¼è¨“ç·´æ•¸æ“šå’Œå¯¦éš›é‡å­ä½æ•¸ï¼‰
        hamiltonian = self._construct_training_hamiltonian(X_processed, y, num_qubits)
        
        logger.info("ğŸ”® ä½¿ç”¨ Qiskit 2.x ç¾ä»£ API é–‹å§‹é‡å­è¨“ç·´...")
        logger.info(f"   Ansatz: {type(ansatz).__name__} ({num_qubits} qubits, {self.config['N_ANSATZ_LAYERS']} layers)")
        logger.info(f"   é‡å­å¾Œç«¯: {getattr(self.quantum_backend, 'name', 'unknown')}")
        
        # ä½¿ç”¨ç¾ä»£ Qiskit 2.x æ–¹å¼ï¼šæ‰‹å‹•å„ªåŒ–å¾ªç’° + primitives
        try:
            # åˆå§‹åŒ–åƒæ•¸
            initial_params = self._generate_quantum_random_parameters(ansatz.num_parameters)
            
            # å‰µå»ºä¼°è¨ˆå™¨ï¼ˆQiskit 2.x primitivesï¼‰
            if PRIMITIVES_AVAILABLE:
                try:
                    from qiskit.primitives import StatevectorEstimator
                    estimator = StatevectorEstimator()
                except ImportError:
                    from qiskit.primitives import Estimator
                    estimator = Estimator()
                logger.info("âœ… ä½¿ç”¨ Qiskit 2.x Primitives API")
            else:
                logger.error("âŒ Primitives ä¸å¯ç”¨ - ç´”é‡å­ç³»çµ±è¦æ±‚å¿…é ˆæœ‰é‡å­å¾Œç«¯ï¼")
                raise RuntimeError("ç´”é‡å­ç³»çµ±ä¸å…è¨±éé‡å­è¨ˆç®—æ–¹æ³•")
            
            # æ‰‹å‹•å„ªåŒ–å¾ªç’°ï¼ˆç´”é‡å­ VQE ä½¿ç”¨ç¾ä»£ APIï¼‰
            best_params = initial_params.copy()
            best_energy = float('inf')
            
            # ç°¡å–®çš„æ¢¯åº¦ä¸‹é™å„ªåŒ–
            learning_rate = 0.1
            max_iterations = 100
            
            for iteration in range(max_iterations):
                try:
                    # å‰µå»ºåƒæ•¸åŒ–é›»è·¯ - Qiskit 2.x æ–¹å¼
                    param_circuit = ansatz.assign_parameters(best_params)
                    
                    # ç´”é‡å­è¨ˆç®—æœŸæœ›å€¼ - å¿…é ˆä½¿ç”¨é‡å­å¾Œç«¯
                    # ä½¿ç”¨ Qiskit 2.x primitives pub æ ¼å¼: (circuit, observables)
                    job = estimator.run([(param_circuit, hamiltonian)])
                    result = job.result()
                    # Qiskit 2.x: çµæœåœ¨ pub_result.data.evs ä¸­ï¼Œè½‰ç‚ºæ¨™é‡
                    energy = float(result[0].data.evs.item())
                    
                    if energy < best_energy:
                        best_energy = energy
                        logger.info(f"ğŸ”® è¿­ä»£ {iteration}: æ–°æœ€ä½³èƒ½é‡ = {energy:.6f}")
                    
                    # ç°¡å–®çš„åƒæ•¸æ›´æ–°ï¼ˆæ•¸å€¼æ¢¯åº¦ï¼‰
                    gradient = self._compute_numerical_gradient(ansatz, best_params, hamiltonian, estimator)
                    best_params = best_params - learning_rate * gradient
                    
                    # æ”¶æ–‚æª¢æŸ¥
                    if iteration > 10 and abs(energy - best_energy) < 1e-6:
                        logger.info(f"âœ… æ”¶æ–‚æ–¼è¿­ä»£ {iteration}")
                        break
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ è¿­ä»£ {iteration} å¤±æ•—: {e}")
                    continue
            
            # å„²å­˜å„ªåŒ–å¾Œçš„åƒæ•¸
            self.theta = best_params
            self.is_fitted = True
            
            # è¨˜éŒ„è¨“ç·´çµæœ
            self.training_history.append({
                'final_energy': best_energy,
                'optimal_parameters': self.theta,
                'iterations': iteration + 1,
                'quantum_advantage_score': quantum_advantage_score,
                'converged': True
            })
            
            logger.info(f"âœ… Qiskit 2.x é‡å­è¨“ç·´å®Œæˆ!")
            logger.info(f"   æœ€çµ‚èƒ½é‡: {best_energy:.6f}")
            logger.info(f"   è¨“ç·´è¿­ä»£æ¬¡æ•¸: {iteration + 1}")
            logger.info(f"   é‡å­å„ªå‹¢åˆ†æ•¸: {quantum_advantage_score:.3f}")
            logger.info(f"   æ”¶æ–‚ç‹€æ…‹: âœ… è‡ªå‹•æ”¶æ–‚")
            
        except Exception as e:
            logger.error(f"âŒ Qiskit 2.x è¨“ç·´å¤±æ•—: {e}")
            # å›é€€åˆ°ç°¡å–®åˆå§‹åŒ–
            n_params = ansatz.num_parameters
            self.theta = self._generate_quantum_random_parameters(n_params)
            self.is_fitted = True
            raise

    def _construct_training_hamiltonian(self, X: np.ndarray, y: np.ndarray, num_qubits: int):
        """æ ¹æ“šè¨“ç·´æ•¸æ“šæ§‹å»º Hamiltonian"""
        try:
            from qiskit.quantum_info import SparsePauliOp

            # ä½¿ç”¨å¯¦éš›çš„é‡å­ä½æ•¸ï¼Œè€Œä¸æ˜¯é…ç½®ä¸­çš„å€¼
            # num_qubits åƒæ•¸å·²ç¶“æ˜¯èª¿æ•´å¾Œçš„æ­£ç¢ºå€¼
            # åŸºæ–¼æ•¸æ“šçµ±è¨ˆæ§‹å»º Hamiltonian
            pauli_list = []
            
            # æ·»åŠ  Z æ“ä½œç¬¦ï¼ˆåŸºæ–¼ç›®æ¨™å€¼ï¼‰
            for i in range(min(num_qubits, len(y))):
                weight = np.mean(y) if len(y) > 0 else 1.0
                pauli_str = 'I' * i + 'Z' + 'I' * (num_qubits - i - 1)
                pauli_list.append((pauli_str, weight))
            
            # æ·»åŠ  X æ“ä½œç¬¦ï¼ˆåŸºæ–¼ç‰¹å¾µç›¸é—œæ€§ï¼‰
            for i in range(min(num_qubits, X.shape[1] if len(X.shape) > 1 else 1)):
                weight = np.std(X[:, i] if len(X.shape) > 1 else X) if len(X) > 0 else 1.0
                pauli_str = 'I' * i + 'X' + 'I' * (num_qubits - i - 1)
                pauli_list.append((pauli_str, weight))
            
            # å‰µå»º Hamiltonian
            if pauli_list:
                hamiltonian = SparsePauliOp.from_list(pauli_list)
            else:
                # é»˜èª Hamiltonian
                hamiltonian = SparsePauliOp.from_list([('Z' + 'I' * (num_qubits-1), 1.0)])
            
            logger.info(f"ğŸ”® æ§‹å»º Hamiltonian: {len(pauli_list)} å€‹ Pauli é … ({num_qubits} qubits)")
            return hamiltonian
            
        except Exception as e:
            logger.warning(f"âš ï¸ Hamiltonian æ§‹å»ºå¤±æ•—: {e}")
            # ä½¿ç”¨ç°¡å–®çš„é»˜èª Hamiltonianï¼ˆåŒ¹é…å¯¦éš›é‡å­ä½æ•¸ï¼‰
            from qiskit.quantum_info import SparsePauliOp
            if num_qubits == 1:
                return SparsePauliOp.from_list([('Z', 1.0)])
            else:
                return SparsePauliOp.from_list([('Z' + 'I' * (num_qubits-1), 1.0)])

    def _compute_numerical_gradient(self, ansatz, params, hamiltonian, estimator):
        """è¨ˆç®—æ•¸å€¼æ¢¯åº¦"""
        try:
            gradient = np.zeros_like(params)
            epsilon = 0.01
            
            for i in range(len(params)):
                # å‰å‘å·®åˆ†
                params_plus = params.copy()
                params_plus[i] += epsilon
                
                params_minus = params.copy()
                params_minus[i] -= epsilon
                
                try:
                    # ç´”é‡å­æ¢¯åº¦è¨ˆç®— - å¿…é ˆä½¿ç”¨é‡å­å¾Œç«¯
                    circuit_plus = ansatz.assign_parameters(params_plus)
                    circuit_minus = ansatz.assign_parameters(params_minus)
                    
                    # ä½¿ç”¨ Qiskit 2.x primitives pub æ ¼å¼: (circuit, observables)
                    job_plus = estimator.run([(circuit_plus, hamiltonian)])
                    job_minus = estimator.run([(circuit_minus, hamiltonian)])
                    
                    # Qiskit 2.x: çµæœåœ¨ pub_result.data.evs ä¸­ï¼Œè½‰ç‚ºæ¨™é‡
                    energy_plus = float(job_plus.result()[0].data.evs.item())
                    energy_minus = float(job_minus.result()[0].data.evs.item())
                    
                    gradient[i] = (energy_plus - energy_minus) / (2 * epsilon)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¢¯åº¦è¨ˆç®—å¤±æ•— (åƒæ•¸ {i}): {e}")
                    gradient[i] = 0.0
            
            return gradient
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ•¸å€¼æ¢¯åº¦è¨ˆç®—å¤±æ•—: {e}")
            return np.zeros_like(params)

    def _generate_quantum_random_parameters(self, n_params: int) -> np.ndarray:
        """ä½¿ç”¨é‡å­éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨ç”Ÿæˆåƒæ•¸"""
        try:
            # å‰µå»ºé‡å­éš¨æ©Ÿæ•¸ç”Ÿæˆé›»è·¯
            n_qubits = min(n_params, 8)  # é™åˆ¶é‡å­ä½æ•¸é‡
            qrng_circuit = QuantumCircuit(n_qubits, n_qubits)
            
            # ä½¿ç”¨ Hadamard é–€å‰µå»ºå‡å‹»ç–ŠåŠ æ…‹
            for i in range(n_qubits):
                qrng_circuit.h(i)
            qrng_circuit.measure_all()
            
            # åœ¨é‡å­å¾Œç«¯åŸ·è¡Œ
            shots = max(100, n_params * 5)  # ç¢ºä¿è¶³å¤ çš„æ¸¬é‡æ¬¡æ•¸
            job = self.quantum_backend.run(qrng_circuit, shots=shots)
            result = job.result()
            counts = result.get_counts()
            
            # å¾é‡å­æ¸¬é‡çµæœæå–éš¨æ©Ÿæ•¸
            random_values = []
            for bitstring, count in counts.items():
                # å°‡äºŒé€²åˆ¶å­—ç¬¦ä¸²è½‰æ›ç‚ºæ•´æ•¸ï¼Œç„¶å¾Œæ­£è¦åŒ–
                try:
                    binary_value = int(bitstring.replace(' ', ''), 2)  # ç§»é™¤ç©ºæ ¼ä¸¦è½‰æ›
                    normalized_value = (binary_value / (2**n_qubits - 1)) * 2 * np.pi - np.pi
                    for _ in range(count):
                        random_values.append(normalized_value)
                except (ValueError, TypeError):
                    # å¦‚æœè½‰æ›å¤±æ•—ï¼Œä½¿ç”¨numpyéš¨æ©Ÿæ•¸
                    random_values.append(np.random.uniform(-np.pi, np.pi))
            
            # ç¢ºä¿æœ‰è¶³å¤ çš„åƒæ•¸
            while len(random_values) < n_params:
                random_values.append(np.random.uniform(-np.pi, np.pi))
            
            quantum_params = np.array(random_values[:n_params])
            logger.info(f"âœ… é‡å­éš¨æ©Ÿæ•¸ç”ŸæˆæˆåŠŸ: {n_params} å€‹åƒæ•¸")
            return quantum_params
            
        except Exception as e:
            logger.error(f"é‡å­éš¨æ©Ÿæ•¸ç”Ÿæˆå¤±æ•—: {e}")
            # å›é€€åˆ°numpyéš¨æ©Ÿæ•¸
            logger.info("ğŸ”„ å›é€€åˆ°å¤å…¸éš¨æ©Ÿæ•¸ç”Ÿæˆå™¨")
            return np.random.uniform(-np.pi, np.pi, n_params)
    
    def _generate_quantum_bernoulli(self, n: int) -> np.ndarray:
        """ä½¿ç”¨é‡å­è¨ˆç®—ç”Ÿæˆ Bernoulli éš¨æ©Ÿè®Šæ•¸"""
        try:
            qrng_circuit = QuantumCircuit(1, 1)
            qrng_circuit.h(0)  # å‰µå»º |+âŸ© æ…‹
            qrng_circuit.measure(0, 0)
            
            bernoulli_values = []
            for _ in range(n):
                job = self.quantum_backend.run(qrng_circuit, shots=1)
                result = job.result()
                counts = result.get_counts()
                
                # å¾æ¸¬é‡çµæœæå– Â±1
                if '0' in counts:
                    bernoulli_values.append(1.0)
                else:
                    bernoulli_values.append(-1.0)
            
            return np.array(bernoulli_values)
            
        except Exception as e:
            logger.error(f"é‡å­ Bernoulli ç”Ÿæˆå¤±æ•—: {e}")
            # ä½¿ç”¨ç³»çµ±ç†µä½œç‚ºå‚™ä»½
            import os
            entropy_bytes = os.urandom(n)
            return np.array([1.0 if b & 1 else -1.0 for b in entropy_bytes])
        
        def objective_function(theta_trial):
            total_loss = 0.0
            n_samples = min(50, len(X_processed))  # é™åˆ¶æ¨£æœ¬æ•¸é‡ä»¥åŠ é€Ÿè¨“ç·´
            
            for i in range(n_samples):
                feature_vec = X_processed[i]
                true_label = y[i]
                
                # è¨ˆç®— Hamiltonian
                h, J = feature_to_hJ_advanced(feature_vec, self.config['N_FEATURE_QUBITS'])
                
                # è©•ä¼°é‡å­é›»è·¯
                expectations, _ = evaluate_quantum_circuit(
                    theta_trial, feature_vec, h, J,
                    self.config['N_FEATURE_QUBITS'], self.config['N_READOUT'],
                    self.config['N_ANSATZ_LAYERS'], self.config['ENCODING'],
                    self.config['USE_STATEVECTOR'], self.config['SHOTS']
                )
                
                # è¨ˆç®—æå¤±
                probs = softmax(expectations)
                true_prob = probs[true_label] if true_label < len(probs) else probs[0]
                total_loss -= np.log(true_prob + 1e-12)
            
            return total_loss / n_samples
        
        # SPSA å„ªåŒ–
        best_loss = float('inf')
        best_theta = self.theta.copy()
        
        iterator = tqdm(range(self.config['SPSA_ITER'])) if verbose else range(self.config['SPSA_ITER'])
        
        for k in iterator:
            # SPSA åƒæ•¸
            a_k = spsa_settings['a'] / (k + spsa_settings['A']) ** spsa_settings['alpha']
            c_k = spsa_settings['c'] / (k + 1) ** spsa_settings['gamma']
            
            # éš¨æ©Ÿæ“¾å‹•
            delta = np.random.choice([-1, 1], size=len(self.theta))
            
            # æ­£å‘å’Œè² å‘è©•ä¼°
            theta_plus = self.theta + c_k * delta
            theta_minus = self.theta - c_k * delta
            
            loss_plus = objective_function(theta_plus)
            loss_minus = objective_function(theta_minus)
            
            # æ¢¯åº¦ä¼°è¨ˆ
            gradient = (loss_plus - loss_minus) / (2 * c_k) * delta
            
            # åƒæ•¸æ›´æ–°
            self.theta -= a_k * gradient
            
            # è¨˜éŒ„æœ€ä½³åƒæ•¸
            current_loss = objective_function(self.theta)
            if current_loss < best_loss:
                best_loss = current_loss
                best_theta = self.theta.copy()
            
            # è¨˜éŒ„è¨“ç·´æ­·å²
            self.training_history.append({
                'iteration': k,
                'loss': current_loss,
                'best_loss': best_loss
            })
            
            if verbose and k % 10 == 0:
                logger.info(f"   è¿­ä»£ {k}: æå¤± = {current_loss:.4f}, æœ€ä½³æå¤± = {best_loss:.4f}")
        
        self.theta = best_theta
        self.is_fitted = True
        
        logger.info(f"âœ… è¨“ç·´å®Œæˆï¼Œæœ€çµ‚æå¤±: {best_loss:.4f}")
    
    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """çœŸå¯¦é‡å­é æ¸¬"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ fit() æ–¹æ³•")
        
        if self.quantum_backend is None:
            raise RuntimeError("âŒ é‡å­å¾Œç«¯æœªåˆå§‹åŒ–")
        
        X_processed = self.preprocess_features(X, fit=False)
        predictions = []
        probabilities = []
        
        logger.info(f"ğŸ”® é–‹å§‹çœŸå¯¦é‡å­é æ¸¬ ({len(X_processed)} å€‹æ¨£æœ¬)...")
        
        for i in tqdm(range(len(X_processed)), desc="é‡å­é æ¸¬"):
            try:
                feature_vec = X_processed[i]
                h, J = feature_to_hJ_advanced(feature_vec, self.config['N_FEATURE_QUBITS'])
                
                expectations, _ = evaluate_quantum_circuit(
                    self.theta, feature_vec, h, J,
                    self.config['N_FEATURE_QUBITS'], self.config['N_READOUT'],
                    self.config['N_ANSATZ_LAYERS'], self.config['ENCODING'],
                    self.config['USE_STATEVECTOR'], self.config['SHOTS'],
                    getattr(self.quantum_backend_manager, 'noise_model', None),
                    self.quantum_backend
                )
                
                probs = softmax(expectations)
                pred = np.argmax(probs)
                
                predictions.append(pred)
                probabilities.append(probs)
                
            except Exception as e:
                logger.error(f"é‡å­é æ¸¬ç¬¬ {i} å€‹æ¨£æœ¬å¤±æ•—: {e}")
                # ä½¿ç”¨è¬¹æ…çš„é»˜èªé æ¸¬ï¼ˆéœ‡ç›ªå¸‚å ´ï¼‰
                predictions.append(1)
                probabilities.append(np.array([0.25, 0.5, 0.25]))
        
        return np.array(predictions), np.array(probabilities)
    
    def predict_single(self, feature_vec: np.ndarray) -> Tuple[int, np.ndarray]:
        """å–®ä¸€çœŸå¯¦é‡å­é æ¸¬"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨ fit() æ–¹æ³•")
        
        if self.quantum_backend is None:
            raise RuntimeError("âŒ é‡å­å¾Œç«¯æœªåˆå§‹åŒ–")
        
        try:
            # é‡æ§‹ç‚ºäºŒç¶­æ•¸çµ„é€²è¡Œé è™•ç†
            X_single = feature_vec.reshape(1, -1)
            X_processed = self.preprocess_features(X_single, fit=False)
            
            h, J = feature_to_hJ_advanced(X_processed[0], self.config['N_FEATURE_QUBITS'])
            
            expectations, _ = evaluate_quantum_circuit(
                self.theta, X_processed[0], h, J,
                self.config['N_FEATURE_QUBITS'], self.config['N_READOUT'],
                self.config['N_ANSATZ_LAYERS'], self.config['ENCODING'],
                self.config['USE_STATEVECTOR'], self.config['SHOTS'],
                getattr(self.quantum_backend_manager, 'noise_model', None),
                self.quantum_backend
            )
            
            probs = softmax(expectations)
            pred = np.argmax(probs)
            
            return pred, probs
            
        except Exception as e:
            logger.error(f"é‡å­å–®ä¸€é æ¸¬å¤±æ•—: {e}")
            raise RuntimeError(f"é‡å­é æ¸¬å¤±æ•—: {e}")
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'config': self.config,
            'theta': self.theta,
            'scaler': self.scaler,
            'pca': self.pca,
            'training_history': self.training_history,
            'is_fitted': self.is_fitted
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {filepath}")
    
    def load_model(self, filepath: str):
        """è¼‰å…¥æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.config = model_data['config']
        self.theta = model_data['theta']
        self.scaler = model_data['scaler']
        self.pca = model_data['pca']
        self.training_history = model_data['training_history']
        self.is_fitted = model_data['is_fitted']
        
        logger.info(f"âœ… æ¨¡å‹å·²è¼‰å…¥è‡ª: {filepath}")
    
    def integrate_with_trading_x(self, symbols: List[str] = None):
        """èˆ‡ Trading X ç³»çµ±æ•´åˆ"""
        if å³æ™‚å¹£å®‰æ•¸æ“šæ”¶é›†å™¨ is None:
            logger.warning("Trading X æ¨¡çµ„æœªæ‰¾åˆ°ï¼Œç„¡æ³•æ•´åˆå³æ™‚æ•¸æ“š")
            return False
        
        symbols = symbols or ['BTCUSDT']
        self.data_collector = å³æ™‚å¹£å®‰æ•¸æ“šæ”¶é›†å™¨(symbols)
        
        logger.info(f"âœ… å·²æ•´åˆ Trading X ç³»çµ±ï¼Œç›£æ§äº¤æ˜“å°: {', '.join(symbols)}")
        return True
    
    async def get_blockchain_market_data(self, symbol: str = 'BTCUSDT') -> Optional[Dict[str, Any]]:
        """å¾å€å¡Šéˆä¸»æ± ç²å–å³æ™‚å¸‚å ´æ•¸æ“š"""
        if not self.blockchain_connector:
            logger.warning("å€å¡Šéˆä¸»æ± é€£æ¥å™¨æœªåˆå§‹åŒ–")
            return None
        
        try:
            # ä½¿ç”¨ X è³‡æ–™å¤¾çš„å€å¡Šéˆä¸»æ± æ–¹æ³•
            async with self.blockchain_connector as connector:
                market_data = await connector.get_comprehensive_market_data(symbol)
                
                if market_data and market_data.get('data_quality') != 'failed':
                    logger.debug(f"ğŸ“Š ç²å– {symbol} å€å¡Šéˆæ•¸æ“šæˆåŠŸï¼Œå®Œæ•´æ€§: {market_data.get('data_completeness', 0):.2%}")
                    return market_data
                else:
                    logger.warning(f"âš ï¸ {symbol} å€å¡Šéˆæ•¸æ“šç²å–å¤±æ•—æˆ–å“è³ªä¸ä½³")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ å€å¡Šéˆä¸»æ± æ•¸æ“šç²å–ç•°å¸¸: {e}")
            return None
    
    async def extract_features_from_blockchain_data(self, market_data: Dict[str, Any]) -> Optional[np.ndarray]:
        """å¾å€å¡Šéˆæ•¸æ“šæå–é‡å­ç‰¹å¾µ"""
        if not market_data or market_data.get('data_quality') == 'failed':
            return None
        
        try:
            features = []
            
            # åƒ¹æ ¼ç‰¹å¾µ
            current_price = market_data.get('current_price', 0)
            price_series = market_data.get('price_series', [])
            
            if price_series and len(price_series) >= 5:
                # è¨ˆç®—æ”¶ç›Šç‡åºåˆ—
                returns = []
                for i in range(1, len(price_series)):
                    ret = (price_series[i] - price_series[i-1]) / price_series[i-1]
                    returns.append(ret)
                
                # å¤šå°ºåº¦ç‰¹å¾µè¨ˆç®—
                scales = [5, 20, min(60, len(returns))]
                for scale in scales:
                    if scale <= len(returns):
                        window = returns[-scale:]
                        features.extend([
                            np.nan_to_num(window[-1]),  # æœ€æ–°æ”¶ç›Šç‡
                            np.nan_to_num(np.std(window)),  # æ³¢å‹•ç‡
                            np.nan_to_num(np.mean(window)),  # å¹³å‡æ”¶ç›Šç‡
                            np.nan_to_num(pd.Series(window).skew()),  # ååº¦
                            np.nan_to_num(pd.Series(window).kurt())   # å³°åº¦
                        ])
                    else:
                        features.extend([0.0, 0.0, 0.0, 0.0, 0.0])
            else:
                features.extend([0.0] * 15)  # 3 scales Ã— 5 features
            
            # æˆäº¤é‡ç‰¹å¾µ
            volume_analysis = market_data.get('volume_analysis', {})
            features.append(volume_analysis.get('volume_trend', 0.0))
            
            # è¨‚å–®ç°¿ç‰¹å¾µ
            order_book = market_data.get('order_book', {})
            if order_book and 'bids' in order_book and 'asks' in order_book:
                bids = order_book['bids']
                asks = order_book['asks']
                if bids and asks:
                    bid_volume = sum(float(bid[1]) for bid in bids[:5])
                    ask_volume = sum(float(ask[1]) for ask in asks[:5])
                    total_volume = bid_volume + ask_volume
                    orderbook_imbalance = (bid_volume - ask_volume) / total_volume if total_volume > 0 else 0
                    features.append(orderbook_imbalance)
                else:
                    features.append(0.0)
            else:
                features.append(0.0)
            
            # è³‡é‡‘è²»ç‡ç‰¹å¾µ
            funding_rate = market_data.get('funding_rate', {})
            if funding_rate and 'fundingRate' in funding_rate:
                features.append(float(funding_rate['fundingRate']))
            else:
                features.append(0.0)
            
            return np.array(features)
            
        except Exception as e:
            logger.error(f"ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    async def generate_trading_signal(self, symbol: str = 'BTCUSDT'):
        """ç”Ÿæˆäº¤æ˜“ä¿¡è™Ÿï¼ˆæ•´åˆå€å¡Šéˆä¸»æ± æ•¸æ“šï¼‰"""
        try:
            # å„ªå…ˆä½¿ç”¨å€å¡Šéˆä¸»æ± æ•¸æ“š
            market_data = await self.get_blockchain_market_data(symbol)
            
            if market_data:
                # å¾å€å¡Šéˆæ•¸æ“šæå–ç‰¹å¾µ
                features = await self.extract_features_from_blockchain_data(market_data)
                
                if features is not None:
                    # é‡å­é æ¸¬
                    pred, probs = self.predict_single(features)
                    
                    # è½‰æ›ç‚º Trading X ä¿¡è™Ÿ
                    signal_map = {0: 'BEAR', 1: 'SIDE', 2: 'BULL'}
                    signal_strength = float(np.max(probs))
                    confidence = signal_strength * market_data.get('data_completeness', 1.0)
                    
                    if TradingXä¿¡è™Ÿ:
                        signal = TradingXä¿¡è™Ÿ(
                            äº¤æ˜“å°=symbol,
                            ä¿¡è™Ÿ=signal_map[pred],
                            ä¿¡è™Ÿå¼·åº¦=signal_strength,
                            ä¿¡å¿ƒåº¦=confidence,
                            é æœŸæ”¶ç›Š=float(probs[2] - probs[0]),  # bull_prob - bear_prob
                            é¢¨éšªè©•ä¼°=1.0 - confidence,
                            æ™‚é–“æˆ³=market_data.get('timestamp', datetime.now()),
                            æ•¸æ“šæº='BTC_Quantum_Ultimate_Model_Blockchain'
                        )
                        
                        self.signal_history.append(signal)
                        logger.info(f"ğŸ”® {symbol} é‡å­ä¿¡è™Ÿ: {signal.ä¿¡è™Ÿ} (å¼·åº¦: {signal_strength:.3f}, ä¿¡å¿ƒ: {confidence:.3f})")
                        return signal
            
            # å›é€€åˆ° Trading X æ•¸æ“šæ”¶é›†å™¨
            if self.data_collector:
                observation = self.data_collector.ç²å–å³æ™‚è§€æ¸¬(symbol)
                if observation is None:
                    logger.warning(f"âš ï¸ ç„¡æ³•ç²å– {symbol} çš„å³æ™‚è§€æ¸¬æ•¸æ“š")
                    return None
                
                # æ§‹å»ºç‰¹å¾µå‘é‡
                features = np.array([
                    observation.æ”¶ç›Šç‡,
                    observation.å·²å¯¦ç¾æ³¢å‹•ç‡,
                    observation.å‹•é‡æ–œç‡,
                    observation.è²·è³£åƒ¹å·®,
                    observation.è¨‚å–®ç°¿å£“åŠ›,
                    observation.ä¸»å‹•è²·å…¥æ¯”ç‡,
                    observation.è³‡é‡‘è²»ç‡ or 0.0,
                    0.0  # å ä½ç¬¦
                ])
                
                # é‡å­é æ¸¬
                pred, probs = self.predict_single(features)
                
                # è½‰æ›ç‚º Trading X ä¿¡è™Ÿ
                signal_map = {0: 'BEAR', 1: 'SIDE', 2: 'BULL'}
                signal_strength = float(np.max(probs))
                
                if TradingXä¿¡è™Ÿ:
                    signal = TradingXä¿¡è™Ÿ(
                        äº¤æ˜“å°=symbol,
                        ä¿¡è™Ÿ=signal_map[pred],
                        ä¿¡è™Ÿå¼·åº¦=signal_strength,
                        ä¿¡å¿ƒåº¦=signal_strength,
                        é æœŸæ”¶ç›Š=float(probs[2] - probs[0]),  # bull_prob - bear_prob
                        é¢¨éšªè©•ä¼°=1.0 - signal_strength,
                        æ™‚é–“æˆ³=observation.æ™‚é–“æˆ³,
                        æ•¸æ“šæº='BTC_Quantum_Ultimate_Model_TradingX'
                    )
                    
                    self.signal_history.append(signal)
                    return signal
            
            logger.warning(f"âš ï¸ ç„¡æ³•ç‚º {symbol} ç”Ÿæˆé‡å­ä¿¡è™Ÿï¼šç„¡å¯ç”¨æ•¸æ“šæº")
            return None
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆ {symbol} äº¤æ˜“ä¿¡è™Ÿå¤±æ•—: {e}")
            return None

# ---------------------------
# å·¥å» å‡½æ•¸
# ---------------------------

def create_btc_quantum_model(config: Dict[str, Any] = None, quantum_backend_type: str = 'local_hf') -> BTCQuantumUltimateModel:
    """å‰µå»º BTC é‡å­çµ‚æ¥µæ¨¡å‹ï¼ˆçœŸå¯¦é‡å­ç‰ˆæœ¬ï¼‰"""
    return BTCQuantumUltimateModel(config, quantum_backend_type)

def production_quantum_demo():
    """ç”Ÿç”¢ç´šé‡å­æ¼”ç¤ºï¼ˆç„¡æ¸¬è©¦æ•¸æ“šï¼‰"""
    logger.info("ğŸš€ BTC é‡å­çµ‚æ¥µæ¨¡å‹ç”Ÿç”¢ç´šæ¼”ç¤ºï¼ˆçœŸå¯¦é‡å­è¨ˆç®—ï¼‰")
    
    try:
        # å‰µå»ºçœŸå¯¦é‡å­æ¨¡å‹
        model = create_btc_quantum_model(quantum_backend_type='local_hf')
        
        # ä½¿ç”¨çœŸå¯¦å¸‚å ´æ•¸æ“šï¼ˆéåˆæˆæ•¸æ“šï¼‰
        if model.blockchain_connector:
            logger.info("ğŸ“Š å¾çœŸå¯¦å€å¡Šéˆæ•¸æ“šæºç²å– BTC æ•¸æ“š...")
            df = model.generate_realistic_market_data('BTCUSDT', '1h', 1000)
        else:
            logger.error("âŒ ç„¡æ³•ç²å–çœŸå¯¦å¸‚å ´æ•¸æ“š - éœ€è¦å€å¡Šéˆæ•¸æ“šé€£æ¥å™¨")
            return None
        
        X, y = model.prepare_features_and_labels(df)
        logger.info(f"ğŸ“Š çœŸå¯¦æ•¸æ“šå½¢ç‹€: X={X.shape}, y={y.shape}")
        
        # æ•¸æ“šåˆ†å¸ƒæª¢æŸ¥
        unique, counts = np.unique(y, return_counts=True)
        logger.info(f"ğŸ“ˆ å¸‚å ´åˆ¶åº¦åˆ†å¸ƒ: {dict(zip(['BEAR', 'SIDE', 'BULL'], counts))}")
        
        # ç”Ÿç”¢ç´šé‡å­è¨“ç·´ï¼ˆæ¸›å°‘è¿­ä»£ä»¥ç¯€çœé‡å­è³‡æºï¼‰
        production_config = QUANTUM_CONFIG.copy()
        production_config['SPSA_ITER'] = 50  # ç”Ÿç”¢ç´šè¿­ä»£æ•¸
        production_config['SHOTS'] = 4096    # é«˜ç²¾åº¦ shots
        
        model.config.update(production_config)
        
        # è¨“ç·´æ¨¡å‹
        logger.info("ğŸ”® é–‹å§‹ç”Ÿç”¢ç´šé‡å­è¨“ç·´...")
        model.fit(X[:200], y[:200], verbose=True)  # ä½¿ç”¨è¼ƒå°æ•¸æ“šé›†ç¯€çœè¨ˆç®—è³‡æº
        
        # æ¸¬è©¦é æ¸¬
        logger.info("ğŸ¯ æ¸¬è©¦é‡å­é æ¸¬...")
        test_predictions, test_probs = model.predict(X[200:210])  # 10å€‹æ¸¬è©¦æ¨£æœ¬
        
        logger.info(f"âœ… é‡å­é æ¸¬å®Œæˆ:")
        for i, (pred, prob) in enumerate(zip(test_predictions, test_probs)):
            true_label = y[200 + i]
            market_state = ['BEAR', 'SIDE', 'BULL'][pred]
            confidence = np.max(prob)
            correct = "âœ…" if pred == true_label else "âŒ"
            logger.info(f"   æ¨£æœ¬ {i+1}: {market_state} (ä¿¡å¿ƒåº¦: {confidence:.3f}) {correct}")
        
        # è¨ˆç®—æº–ç¢ºç‡
        accuracy = np.mean(test_predictions == y[200:210])
        logger.info(f"ğŸ¯ é‡å­æ¨¡å‹æº–ç¢ºç‡: {accuracy:.3f}")
        
        # é‡å­å„ªå‹¢å ±å‘Š
        if hasattr(model, 'quantum_advantage_validator'):
            advantage_score = model.quantum_advantage_validator.benchmark_results.get('total_score', 0.0)
            logger.info(f"âš¡ é‡å­å„ªå‹¢åˆ†æ•¸: {advantage_score:.3f}")
        
        return model
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿç”¢ç´šé‡å­æ¼”ç¤ºå¤±æ•—: {e}")
        return None

    def _measure_quantum_coherence_time(self) -> float:
        """å¯¦æ™‚æ¸¬é‡é‡å­ç³»çµ±çš„çœŸå¯¦ç›¸å¹²æ™‚é–“"""
        try:
            # å‰µå»ºè²çˆ¾æ…‹æ¸¬è©¦é›»è·¯ä¾†æ¸¬é‡ç›¸å¹²æ€§
            coherence_circuit = QuantumCircuit(2, 2)
            coherence_circuit.h(0)  # å‰µå»ºç–ŠåŠ æ…‹
            coherence_circuit.cx(0, 1)  # å‰µå»ºç³¾çºæ…‹
            
            # æ‡‰ç”¨ä¸åŒå»¶é²æ™‚é–“ä¸¦æ¸¬é‡ä¿çœŸåº¦è¡°æ¸›
            delays = np.linspace(0.1, 10.0, 20)  # æ¯«ç§’å»¶é²
            fidelities = []
            
            for delay in delays:
                # æ¨¡æ“¬é‡å­å»ç›¸å¹²
                test_circuit = coherence_circuit.copy()
                
                # æ¸¬é‡ä¸¦è¨ˆç®—ä¿çœŸåº¦
                result = execute(test_circuit, self.simulator, shots=1000).result()
                counts = result.get_counts()
                
                # è¨ˆç®—è²çˆ¾æ…‹ä¿çœŸåº¦
                bell_states = counts.get('00', 0) + counts.get('11', 0)
                fidelity = bell_states / 1000
                fidelities.append(fidelity)
            
            # æ“¬åˆæŒ‡æ•¸è¡°æ¸›ä¾†è¨ˆç®—T2æ™‚é–“
            fidelities = np.array(fidelities)
            valid_mask = fidelities > 0.1  # éæ¿¾å™ªè²
            
            if np.sum(valid_mask) < 3:
                logger.warning("âš ï¸ ç›¸å¹²æ™‚é–“æ¸¬é‡æ•¸æ“šä¸è¶³ï¼Œä½¿ç”¨é»˜èªå€¼")
                return 2.0
            
            valid_delays = delays[valid_mask]
            valid_fidelities = fidelities[valid_mask]
            
            # æŒ‡æ•¸è¡°æ¸›æ“¬åˆ: f(t) = exp(-t/T2)
            try:
                from scipy.optimize import curve_fit
                
                def exponential_decay(t, t2):
                    return np.exp(-t / t2)
                
                popt, _ = curve_fit(exponential_decay, valid_delays, valid_fidelities, 
                                  bounds=(0.1, 100), maxfev=1000)
                measured_t2 = popt[0]
                
                logger.info(f"ğŸ”¬ å¯¦æ¸¬é‡å­ç›¸å¹²æ™‚é–“ T2 = {measured_t2:.3f} ms")
                return measured_t2
                
            except Exception as fit_error:
                logger.warning(f"âš ï¸ T2æ“¬åˆå¤±æ•—: {fit_error}")
                # ä½¿ç”¨ç°¡å–®æ–¹æ³•ï¼šæ‰¾åˆ°ä¿çœŸåº¦é™åˆ°1/eçš„æ™‚é–“é»
                target_fidelity = 1/np.e
                closest_idx = np.argmin(np.abs(valid_fidelities - target_fidelity))
                estimated_t2 = valid_delays[closest_idx]
                
                logger.info(f"ğŸ”¬ ä¼°ç®—é‡å­ç›¸å¹²æ™‚é–“ T2 â‰ˆ {estimated_t2:.3f} ms")
                return estimated_t2
                
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­ç›¸å¹²æ™‚é–“æ¸¬é‡å¤±æ•—: {e}")
            return 2.0  # ä¿å®ˆé»˜èªå€¼

    def _calculate_entanglement_capacity(self, circuit_depth: int) -> int:
        """åŸºæ–¼é›»è·¯æ·±åº¦è¨ˆç®—ç³¾çºå®¹é‡"""
        try:
            # é‡å­ç³¾çºè¤‡é›œåº¦éš¨é›»è·¯æ·±åº¦æŒ‡æ•¸å¢é•·
            base_capacity = 2 ** min(self.config['N_FEATURE_QUBITS'], 8)
            depth_factor = 1 + np.log(circuit_depth + 1)
            entanglement_capacity = int(base_capacity * depth_factor)
            
            logger.debug(f"ğŸ”— ç³¾çºå®¹é‡è¨ˆç®—: åŸºç¤={base_capacity}, æ·±åº¦å› å­={depth_factor:.2f}, ç¸½å®¹é‡={entanglement_capacity}")
            return entanglement_capacity
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç³¾çºå®¹é‡è¨ˆç®—å¤±æ•—: {e}")
            return 64  # ä¿å®ˆé»˜èªå€¼

    def _calculate_uncertainty_factor(self) -> float:
        """åŸºæ–¼æµ·æ£®å ¡ä¸ç¢ºå®šæ€§åŸç†è¨ˆç®—é‡å­ä¸ç¢ºå®šæ€§å› å­"""
        try:
            # é‡å­æ¯”ç‰¹æ•¸è¶Šå¤šï¼Œä¸ç¢ºå®šæ€§å½±éŸ¿è¶Šå¤§
            n_qubits = self.config['N_FEATURE_QUBITS']
            
            # æµ·æ£®å ¡å¸¸æ•¸çš„é‡å­ä¿®æ­£
            h_bar = 1.054571817e-34  # ç´„åŒ–æ™®æœ—å…‹å¸¸æ•¸
            uncertainty_base = np.sqrt(n_qubits * h_bar) * 1e34  # æ¨™æº–åŒ–
            
            # é™åˆ¶åœ¨åˆç†ç¯„åœå…§
            uncertainty_factor = np.clip(uncertainty_base, 0.5, 5.0)
            
            logger.debug(f"ğŸŒŠ é‡å­ä¸ç¢ºå®šæ€§å› å­: {uncertainty_factor:.3f} (åŸºæ–¼ {n_qubits} é‡å­æ¯”ç‰¹)")
            return uncertainty_factor
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸ç¢ºå®šæ€§å› å­è¨ˆç®—å¤±æ•—: {e}")
            return 1.0  # ä¿å®ˆé»˜èªå€¼

    def _calculate_quantum_entropy(self, theta: np.ndarray) -> float:
        """è¨ˆç®—é‡å­æ…‹ç†µä¾†è©•ä¼°ç³¾çºç¨‹åº¦"""
        try:
            # å‰µå»ºåƒæ•¸åŒ–é‡å­é›»è·¯
            n_qubits = min(4, self.config['N_FEATURE_QUBITS'])  # é™åˆ¶è¨ˆç®—è¤‡é›œåº¦
            qc = QuantumCircuit(n_qubits)
            
            # ä½¿ç”¨ç•¶å‰åƒæ•¸é€²è¡Œé‡å­æ¼”åŒ–
            param_idx = 0
            for i in range(n_qubits):
                if param_idx < len(theta):
                    qc.ry(theta[param_idx], i)
                    param_idx += 1
                if param_idx < len(theta):
                    qc.rz(theta[param_idx], i)
                    param_idx += 1
            
            # æ·»åŠ ç³¾çºé–€
            for i in range(n_qubits-1):
                qc.cx(i, i+1)
            
            # ç²å–ç‹€æ…‹å‘é‡
            result = execute(qc, self.simulator, shots=1).result()
            statevector = result.get_statevector()
            
            # è¨ˆç®—é¦®è«¾ä¾æ›¼ç†µ
            density_matrix = np.outer(statevector, np.conj(statevector))
            eigenvalues = np.linalg.eigvals(density_matrix)
            eigenvalues = eigenvalues[eigenvalues > 1e-10]  # éæ¿¾æ•¸å€¼èª¤å·®
            
            entropy = -np.sum(eigenvalues * np.log2(eigenvalues + 1e-10))
            return entropy / n_qubits  # æ¨™æº–åŒ–ç†µ
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­ç†µè¨ˆç®—å¤±æ•—: {e}")
            return 0.5

    def _measure_quantum_coherence(self, theta: np.ndarray) -> float:
        """æ¸¬é‡é‡å­ç›¸å¹²æ€§"""
        try:
            # å‰µå»ºç›¸å¹²æ€§æ¸¬è©¦é›»è·¯
            n_qubits = min(3, self.config['N_FEATURE_QUBITS'])
            qc = QuantumCircuit(n_qubits, n_qubits)
            
            # æ‡‰ç”¨åƒæ•¸åŒ–æ¼”åŒ–
            param_idx = 0
            for i in range(n_qubits):
                if param_idx < len(theta):
                    qc.ry(theta[param_idx], i)
                    param_idx += 1
            
            # æ¸¬é‡æ‰€æœ‰é‡å­æ¯”ç‰¹
            qc.measure_all()
            
            # å¤šæ¬¡æ¸¬é‡è¨ˆç®—ç›¸å¹²æ€§
            result = execute(qc, self.simulator, shots=1000).result()
            counts = result.get_counts()
            
            # è¨ˆç®—æ¸¬é‡çµæœçš„å‡å‹»æ€§ï¼ˆç›¸å¹²æ€§æŒ‡æ¨™ï¼‰
            total_outcomes = len(counts)
            expected_uniform = 1000 / (2 ** n_qubits)
            
            coherence_sum = 0
            for count in counts.values():
                coherence_sum += abs(count - expected_uniform)
            
            # æ¨™æº–åŒ–ç›¸å¹²æ€§åˆ†æ•¸
            max_deviation = 1000
            coherence_score = 1.0 - (coherence_sum / max_deviation)
            
            return max(0.0, min(1.0, coherence_score))
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­ç›¸å¹²æ€§æ¸¬é‡å¤±æ•—: {e}")
            return 0.5

    def _adapt_quantum_config_to_problem_complexity(self, base_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ“šå•é¡Œè¤‡é›œåº¦è‡ªé©æ‡‰èª¿æ•´é‡å­é…ç½®"""
        try:
            # åˆ†æå•é¡Œè¤‡é›œåº¦ç¶­åº¦
            feature_complexity = self._analyze_feature_complexity()
            temporal_complexity = self._analyze_temporal_complexity()
            market_complexity = self._analyze_market_complexity()
            
            # ç¶œåˆè¤‡é›œåº¦è©•åˆ†
            total_complexity = (feature_complexity + temporal_complexity + market_complexity) / 3
            
            # è‡ªé©æ‡‰é‡å­æ¯”ç‰¹æ•¸
            optimal_qubits = self._calculate_optimal_qubits(total_complexity)
            
            # è‡ªé©æ‡‰Ansatzå±¤æ•¸
            optimal_layers = self._calculate_optimal_ansatz_layers(total_complexity, optimal_qubits)
            
            # æ›´æ–°é…ç½®
            adapted_config = base_config.copy()
            adapted_config['N_FEATURE_QUBITS'] = optimal_qubits
            adapted_config['N_ANSATZ_LAYERS'] = optimal_layers
            
            # å‹•æ…‹SPSAåƒæ•¸
            adapted_config['SPSA_A'] = self._calculate_adaptive_spsa_a(total_complexity)
            adapted_config['SPSA_C'] = self._calculate_adaptive_spsa_c(total_complexity)
            
            logger.info(f"ğŸ¯ å•é¡Œè¤‡é›œåº¦åˆ†æ:")
            logger.info(f"   ç‰¹å¾µè¤‡é›œåº¦: {feature_complexity:.3f}")
            logger.info(f"   æ™‚é–“è¤‡é›œåº¦: {temporal_complexity:.3f}")
            logger.info(f"   å¸‚å ´è¤‡é›œåº¦: {market_complexity:.3f}")
            logger.info(f"   ç¶œåˆè¤‡é›œåº¦: {total_complexity:.3f}")
            logger.info(f"ğŸ”§ è‡ªé©æ‡‰é…ç½®:")
            logger.info(f"   é‡å­æ¯”ç‰¹: {optimal_qubits}")
            logger.info(f"   Ansatzå±¤: {optimal_layers}")
            logger.info(f"   SPSA_A: {adapted_config['SPSA_A']:.6f}")
            logger.info(f"   SPSA_C: {adapted_config['SPSA_C']:.6f}")
            
            return adapted_config
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­é…ç½®è‡ªé©æ‡‰å¤±æ•—ï¼Œä½¿ç”¨åŸºç¤é…ç½®: {e}")
            return base_config

    def _analyze_feature_complexity(self) -> float:
        """åˆ†æç‰¹å¾µè¤‡é›œåº¦"""
        try:
            # åŸºæ–¼å¸¸è¦‹é‡‘èç‰¹å¾µçš„è¤‡é›œåº¦è©•ä¼°
            # åƒ¹æ ¼ã€æˆäº¤é‡ã€æŠ€è¡“æŒ‡æ¨™ç­‰çš„å¾©é›œåº¦
            base_features = 10  # åŸºæœ¬ç‰¹å¾µæ•¸
            technical_indicators = 20  # æŠ€è¡“æŒ‡æ¨™æ•¸
            market_microstructure = 15  # å¸‚å ´å¾®çµæ§‹ç‰¹å¾µ
            
            total_features = base_features + technical_indicators + market_microstructure
            
            # è¤‡é›œåº¦è¨ˆç®—ï¼šç‰¹å¾µæ•¸é‡å’Œéç·šæ€§é—œä¿‚
            feature_nonlinearity = np.log(total_features)
            interaction_complexity = np.sqrt(total_features) * 0.1
            
            complexity = (feature_nonlinearity + interaction_complexity) / 10
            return np.clip(complexity, 0.1, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç‰¹å¾µè¤‡é›œåº¦åˆ†æå¤±æ•—: {e}")
            return 0.5

    def _analyze_temporal_complexity(self) -> float:
        """åˆ†ææ™‚é–“åºåˆ—è¤‡é›œåº¦"""
        try:
            # å¸‚å ´æ™‚é–“åºåˆ—çš„è¤‡é›œåº¦è©•ä¼°
            # é«˜é »äº¤æ˜“ã€æ—¥å…§æ¨¡å¼ã€é€±æœŸæ€§ç­‰
            
            # æ™‚é–“å°ºåº¦è¤‡é›œåº¦
            time_scales = 5  # å¤šæ™‚é–“æ¡†æ¶åˆ†æ
            trend_complexity = 0.7  # è¶¨å‹¢å¾©é›œåº¦
            volatility_complexity = 0.8  # æ³¢å‹•æ€§å¾©é›œåº¦
            seasonality_complexity = 0.6  # å­£ç¯€æ€§å¾©é›œåº¦
            
            temporal_complexity = (trend_complexity + volatility_complexity + seasonality_complexity) / 3
            temporal_complexity *= (1 + np.log(time_scales) / 10)
            
            return np.clip(temporal_complexity, 0.1, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ™‚é–“è¤‡é›œåº¦åˆ†æå¤±æ•—: {e}")
            return 0.6

    def _analyze_market_complexity(self) -> float:
        """åˆ†æå¸‚å ´è¤‡é›œåº¦"""
        try:
            # åŠ å¯†è²¨å¹£å¸‚å ´çš„ç‰¹æ®Šè¤‡é›œåº¦
            volatility_factor = 0.9  # é«˜æ³¢å‹•æ€§
            liquidity_factor = 0.7   # æµå‹•æ€§è®ŠåŒ–
            sentiment_factor = 0.8   # æƒ…ç·’å½±éŸ¿
            regulatory_factor = 0.6  # ç›£ç®¡ç’°å¢ƒ
            
            market_complexity = (volatility_factor + liquidity_factor + 
                               sentiment_factor + regulatory_factor) / 4
            
            # åŠ å¯†è²¨å¹£å¸‚å ´ç‰¹æœ‰çš„è¤‡é›œåº¦åŠ æˆ
            crypto_multiplier = 1.2
            market_complexity *= crypto_multiplier
            
            return np.clip(market_complexity, 0.1, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ å¸‚å ´è¤‡é›œåº¦åˆ†æå¤±æ•—: {e}")
            return 0.7

    def _calculate_optimal_qubits(self, complexity: float) -> int:
        """åŸºæ–¼è¤‡é›œåº¦è¨ˆç®—æœ€å„ªé‡å­æ¯”ç‰¹æ•¸"""
        try:
            # é‡å­æ¯”ç‰¹æ•¸èˆ‡å•é¡Œè¤‡é›œåº¦çš„é—œä¿‚
            base_qubits = 4  # æœ€å°é‡å­æ¯”ç‰¹æ•¸
            complexity_factor = complexity ** 0.5  # å¹³æ–¹æ ¹é—œä¿‚ï¼Œé¿å…æŒ‡æ•¸çˆ†ç‚¸
            
            optimal_qubits = base_qubits + int(complexity_factor * 6)
            
            # é™åˆ¶åœ¨åˆç†ç¯„åœå…§ï¼ˆè€ƒæ…®é‡å­ç¡¬é«”é™åˆ¶ï¼‰
            optimal_qubits = np.clip(optimal_qubits, 4, 12)
            
            logger.debug(f"ğŸ¯ æœ€å„ªé‡å­æ¯”ç‰¹è¨ˆç®—: è¤‡é›œåº¦={complexity:.3f} â†’ {optimal_qubits} qubits")
            return optimal_qubits
            
        except Exception as e:
            logger.warning(f"âš ï¸ æœ€å„ªé‡å­æ¯”ç‰¹è¨ˆç®—å¤±æ•—: {e}")
            return 6  # ä¿å®ˆé»˜èªå€¼

    def _calculate_optimal_ansatz_layers(self, complexity: float, qubits: int) -> int:
        """åŸºæ–¼è¤‡é›œåº¦å’Œé‡å­æ¯”ç‰¹æ•¸è¨ˆç®—æœ€å„ªAnsatzå±¤æ•¸"""
        try:
            # Ansatzæ·±åº¦èˆ‡è¤‡é›œåº¦å’Œé‡å­æ¯”ç‰¹æ•¸çš„é—œä¿‚
            base_layers = 2
            complexity_contribution = int(complexity * 4)
            qubit_contribution = max(1, qubits // 3)
            
            optimal_layers = base_layers + complexity_contribution + qubit_contribution
            
            # é™åˆ¶åœ¨åˆç†ç¯„åœå…§ï¼ˆé¿å…é›»è·¯éæ·±ï¼‰
            optimal_layers = np.clip(optimal_layers, 2, 8)
            
            logger.debug(f"ğŸŒ€ æœ€å„ªAnsatzå±¤è¨ˆç®—: è¤‡é›œåº¦={complexity:.3f}, é‡å­æ¯”ç‰¹={qubits} â†’ {optimal_layers} å±¤")
            return optimal_layers
            
        except Exception as e:
            logger.warning(f"âš ï¸ æœ€å„ªAnsatzå±¤è¨ˆç®—å¤±æ•—: {e}")
            return 3  # ä¿å®ˆé»˜èªå€¼

    def _calculate_adaptive_spsa_a(self, complexity: float) -> float:
        """åŸºæ–¼è¤‡é›œåº¦è¨ˆç®—è‡ªé©æ‡‰SPSAåƒæ•¸A"""
        try:
            # SPSA Aåƒæ•¸æ§åˆ¶å­¸ç¿’ç‡è¡°æ¸›
            base_a = 0.602  # ç†è«–æœ€å„ªå€¼
            complexity_adjustment = complexity * 0.1
            
            adaptive_a = base_a + complexity_adjustment
            adaptive_a = np.clip(adaptive_a, 0.5, 0.8)
            
            return adaptive_a
            
        except Exception as e:
            logger.warning(f"âš ï¸ è‡ªé©æ‡‰SPSA_Aè¨ˆç®—å¤±æ•—: {e}")
            return 0.602

    def _calculate_adaptive_spsa_c(self, complexity: float) -> float:
        """åŸºæ–¼è¤‡é›œåº¦è¨ˆç®—è‡ªé©æ‡‰SPSAåƒæ•¸C"""
        try:
            # SPSA Cåƒæ•¸æ§åˆ¶æ“¾å‹•å¹…åº¦
            base_c = 0.101  # ç†è«–æœ€å„ªå€¼
            complexity_adjustment = complexity * 0.05
            
            adaptive_c = base_c + complexity_adjustment
            adaptive_c = np.clip(adaptive_c, 0.05, 0.2)
            
            return adaptive_c
            
        except Exception as e:
            logger.warning(f"âš ï¸ è‡ªé©æ‡‰SPSA_Cè¨ˆç®—å¤±æ•—: {e}")
            return 0.101

    def _calculate_quantum_complexity_score(self) -> float:
        """è¨ˆç®—ç•¶å‰é‡å­é…ç½®çš„è¤‡é›œåº¦åˆ†æ•¸"""
        try:
            qubits = self.config['N_FEATURE_QUBITS']
            layers = self.config['N_ANSATZ_LAYERS']
            
            # é‡å­è¤‡é›œåº¦è©•åˆ†
            qubit_complexity = qubits / 12  # æ¨™æº–åŒ–åˆ°12å€‹é‡å­æ¯”ç‰¹
            layer_complexity = layers / 8   # æ¨™æº–åŒ–åˆ°8å±¤
            circuit_complexity = qubit_complexity * layer_complexity
            
            # ç¸½è¤‡é›œåº¦åˆ†æ•¸
            complexity_score = (qubit_complexity + layer_complexity + circuit_complexity) / 3
            
            return np.clip(complexity_score, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­è¤‡é›œåº¦åˆ†æ•¸è¨ˆç®—å¤±æ•—: {e}")
            return 0.5

    def _detect_quantum_convergence(self, entropy_history: List[float], 
                                  coherence_history: List[float], 
                                  improvement: float) -> bool:
        """åŸºæ–¼é‡å­ç³¾çºç†µé©…å‹•çš„æ”¶æ–‚æª¢æ¸¬ - ç´”é‡å­ç‰©ç†åŸç†"""
        try:
            if len(entropy_history) < 10 or len(coherence_history) < 10:
                return False
            
            # 1. é‡å­ç³¾çºç†µç©©å®šæ€§åˆ†æ
            recent_entropy = np.array(entropy_history[-10:])
            entropy_stability = self._analyze_entropy_stability(recent_entropy)
            
            # 2. é‡å­ç›¸å¹²æ€§æ”¶æ–‚åˆ†æ
            recent_coherence = np.array(coherence_history[-10:])
            coherence_convergence = self._analyze_coherence_convergence(recent_coherence)
            
            # 3. é‡å­æ…‹ç³¾çºåº¦æ¸¬é‡
            current_entanglement = self._measure_quantum_entanglement()
            
            # 4. é‡å­ä¿¡æ¯ç†µæ¢¯åº¦åˆ†æ
            entropy_gradient = self._calculate_entropy_gradient(recent_entropy)
            
            # 5. æµ·æ£®å ¡ä¸ç¢ºå®šæ€§åŸç†æ‡‰ç”¨
            uncertainty_criterion = self._check_uncertainty_criterion(
                entropy_stability, coherence_convergence, improvement
            )
            
            # ç¶œåˆé‡å­æ”¶æ–‚åˆ¤æ–·
            quantum_convergence_score = (
                entropy_stability * 0.3 +
                coherence_convergence * 0.25 +
                current_entanglement * 0.2 +
                (1.0 - abs(entropy_gradient)) * 0.15 +
                uncertainty_criterion * 0.1
            )
            
            convergence_threshold = self._calculate_dynamic_quantum_threshold()
            
            logger.debug(f"ğŸ”¬ é‡å­æ”¶æ–‚åˆ†æ:")
            logger.debug(f"   ç†µç©©å®šæ€§: {entropy_stability:.4f}")
            logger.debug(f"   ç›¸å¹²æ”¶æ–‚: {coherence_convergence:.4f}")
            logger.debug(f"   ç³¾çºåº¦: {current_entanglement:.4f}")
            logger.debug(f"   ç†µæ¢¯åº¦: {entropy_gradient:.4f}")
            logger.debug(f"   ä¸ç¢ºå®šæ€§: {uncertainty_criterion:.4f}")
            logger.debug(f"   æ”¶æ–‚åˆ†æ•¸: {quantum_convergence_score:.4f}")
            logger.debug(f"   å‹•æ…‹é–¾å€¼: {convergence_threshold:.4f}")
            
            if quantum_convergence_score >= convergence_threshold:
                logger.info(f"ğŸ¯ é‡å­ç³¾çºç†µé©…å‹•æ”¶æ–‚ï¼åˆ†æ•¸: {quantum_convergence_score:.4f} >= {convergence_threshold:.4f}")
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­æ”¶æ–‚æª¢æ¸¬å¤±æ•—: {e}")
            return False

    def _analyze_entropy_stability(self, entropy_values: np.ndarray) -> float:
        """åˆ†æé‡å­ç†µçš„ç©©å®šæ€§"""
        try:
            if len(entropy_values) < 5:
                return 0.0
            
            # è¨ˆç®—ç†µå€¼çš„è®Šç•°ä¿‚æ•¸
            entropy_mean = np.mean(entropy_values)
            entropy_std = np.std(entropy_values)
            
            if entropy_mean < 1e-8:
                return 0.0
            
            coefficient_of_variation = entropy_std / entropy_mean
            
            # ç©©å®šæ€§åˆ†æ•¸ï¼ˆè®Šç•°è¶Šå°è¶Šç©©å®šï¼‰
            stability_score = 1.0 / (1.0 + coefficient_of_variation)
            
            return np.clip(stability_score, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç†µç©©å®šæ€§åˆ†æå¤±æ•—: {e}")
            return 0.0

    def _analyze_coherence_convergence(self, coherence_values: np.ndarray) -> float:
        """åˆ†æé‡å­ç›¸å¹²æ€§çš„æ”¶æ–‚ç¨‹åº¦"""
        try:
            if len(coherence_values) < 5:
                return 0.0
            
            # è¨ˆç®—ç›¸å¹²æ€§çš„è¶¨å‹¢
            x = np.arange(len(coherence_values))
            slope, intercept = np.polyfit(x, coherence_values, 1)
            
            # è¨ˆç®—æœ€è¿‘å€¼èˆ‡è¶¨å‹¢ç·šçš„åå·®
            trend_line = slope * x + intercept
            deviations = np.abs(coherence_values - trend_line)
            mean_deviation = np.mean(deviations)
            
            # æ”¶æ–‚åˆ†æ•¸ï¼ˆåå·®è¶Šå°æ”¶æ–‚è¶Šå¥½ï¼‰
            convergence_score = 1.0 / (1.0 + mean_deviation * 10)
            
            return np.clip(convergence_score, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç›¸å¹²æ€§æ”¶æ–‚åˆ†æå¤±æ•—: {e}")
            return 0.0

    def _measure_quantum_entanglement(self) -> float:
        """æ¸¬é‡ç•¶å‰é‡å­æ…‹çš„ç³¾çºç¨‹åº¦"""
        try:
            if self.theta is None:
                return 0.0
            
            # å‰µå»ºç•¶å‰åƒæ•¸åŒ–çš„é‡å­é›»è·¯
            n_qubits = min(4, self.config['N_FEATURE_QUBITS'])
            qc = QuantumCircuit(n_qubits)
            
            # æ‡‰ç”¨åƒæ•¸åŒ–é–€
            param_idx = 0
            for layer in range(min(2, self.config['N_ANSATZ_LAYERS'])):
                for i in range(n_qubits):
                    if param_idx < len(self.theta):
                        qc.ry(self.theta[param_idx], i)
                        param_idx += 1
                
                # ç³¾çºé–€
                for i in range(n_qubits - 1):
                    qc.cx(i, i + 1)
            
            # åŸ·è¡Œé›»è·¯ä¸¦ç²å–ç‹€æ…‹å‘é‡
            simulator = Aer.get_backend('statevector_simulator')
            result = execute(qc, simulator).result()
            statevector = result.get_statevector()
            
            # è¨ˆç®—ç³¾çºç†µï¼ˆSchmidtåˆ†è§£ï¼‰
            # å°‡ç³»çµ±åˆ†æˆå…©éƒ¨åˆ†
            subsystem_size = n_qubits // 2
            if subsystem_size == 0:
                return 0.0
            
            # é‡å¡‘ç‹€æ…‹å‘é‡ç‚ºçŸ©é™£
            dim_a = 2 ** subsystem_size
            dim_b = 2 ** (n_qubits - subsystem_size)
            
            state_matrix = statevector.reshape((dim_a, dim_b))
            
            # Schmidtåˆ†è§£
            u, s, vh = np.linalg.svd(state_matrix)
            
            # è¨ˆç®—ç³¾çºç†µ
            s_squared = s ** 2
            s_squared = s_squared[s_squared > 1e-10]  # éæ¿¾æ•¸å€¼èª¤å·®
            
            entanglement_entropy = -np.sum(s_squared * np.log2(s_squared + 1e-10))
            
            # æ¨™æº–åŒ–ç³¾çºç†µ
            max_entanglement = min(subsystem_size, n_qubits - subsystem_size)
            normalized_entanglement = entanglement_entropy / max_entanglement if max_entanglement > 0 else 0
            
            return np.clip(normalized_entanglement, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ é‡å­ç³¾çºæ¸¬é‡å¤±æ•—: {e}")
            return 0.0

    def _calculate_entropy_gradient(self, entropy_values: np.ndarray) -> float:
        """è¨ˆç®—é‡å­ç†µçš„æ¢¯åº¦"""
        try:
            if len(entropy_values) < 3:
                return 0.0
            
            # è¨ˆç®—æ•¸å€¼æ¢¯åº¦
            gradient = np.gradient(entropy_values)
            
            # å–æœ€è¿‘çš„æ¢¯åº¦å€¼
            recent_gradient = np.mean(gradient[-3:])
            
            return recent_gradient
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç†µæ¢¯åº¦è¨ˆç®—å¤±æ•—: {e}")
            return 0.0

    def _check_uncertainty_criterion(self, entropy_stability: float, 
                                   coherence_convergence: float, 
                                   improvement: float) -> float:
        """åŸºæ–¼æµ·æ£®å ¡ä¸ç¢ºå®šæ€§åŸç†çš„æ”¶æ–‚åˆ¤æ“š"""
        try:
            # æµ·æ£®å ¡ä¸ç¢ºå®šæ€§é—œä¿‚ï¼šÎ”E * Î”t â‰¥ â„/2
            # åœ¨é‡å­è¨ˆç®—ä¸­ï¼Œèƒ½é‡æ”¹å–„å’Œæ™‚é–“æ”¶æ–‚å­˜åœ¨æ¬Šè¡¡
            
            # èƒ½é‡æ”¹å–„çš„ä¸ç¢ºå®šæ€§
            energy_uncertainty = abs(improvement) if improvement != 0 else 1e-6
            
            # æ™‚é–“æ”¶æ–‚çš„ä¸ç¢ºå®šæ€§ï¼ˆåŸºæ–¼ç†µå’Œç›¸å¹²æ€§ï¼‰
            time_uncertainty = 1.0 - (entropy_stability * coherence_convergence)
            
            # ä¸ç¢ºå®šæ€§ä¹˜ç©
            uncertainty_product = energy_uncertainty * time_uncertainty
            
            # æµ·æ£®å ¡ä¸‹ç•Œï¼ˆæ¨™æº–åŒ–ï¼‰
            h_bar_normalized = 0.01  # æ¨™æº–åŒ–çš„ç´„åŒ–æ™®æœ—å…‹å¸¸æ•¸
            
            # æ»¿è¶³ä¸ç¢ºå®šæ€§åŸç†çš„ç¨‹åº¦
            uncertainty_satisfaction = uncertainty_product / h_bar_normalized
            
            # è½‰æ›ç‚ºæ”¶æ–‚åˆ¤æ“šï¼ˆæ»¿è¶³ä¸ç¢ºå®šæ€§åŸç†æ™‚æ›´å¯èƒ½æ”¶æ–‚ï¼‰
            criterion = 1.0 / (1.0 + uncertainty_satisfaction)
            
            return np.clip(criterion, 0.0, 1.0)
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸ç¢ºå®šæ€§åˆ¤æ“šè¨ˆç®—å¤±æ•—: {e}")
            return 0.0

if __name__ == "__main__":
    """çœŸå¯¦é‡å­è¨ˆç®—ä¸»ç¨‹åºï¼ˆç„¡æ¸¬è©¦æ¨¡å¼ï¼‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description='BTC é‡å­çµ‚æ¥µæ¨¡å‹ - çœŸå¯¦é‡å­è¨ˆç®—ç‰ˆæœ¬')
    parser.add_argument('--backend', choices=['ibm', 'local_hf'], default='local_hf',
                        help='é‡å­å¾Œç«¯é¡å‹ (ibm: IBM Quantum, local_hf: æœ¬åœ°é«˜ä¿çœŸåº¦)')
    parser.add_argument('--symbol', default='BTCUSDT', help='äº¤æ˜“å°ç¬¦è™Ÿ')
    parser.add_argument('--demo', action='store_true', help='é‹è¡Œç”Ÿç”¢ç´šæ¼”ç¤º')
    
    args = parser.parse_args()
    
    if args.demo:
        production_quantum_demo()
    else:
        logger.info("ğŸ”® BTC é‡å­çµ‚æ¥µæ¨¡å‹å·²å°±ç·’")
        logger.info("   ä½¿ç”¨ --demo åƒæ•¸é‹è¡Œç”Ÿç”¢ç´šæ¼”ç¤º")
        logger.info("   ä½¿ç”¨ --backend ibm é€£æ¥ IBM Quantum ç¡¬é«”")
        logger.info("   ç¢ºä¿è¨­ç½® IBM_QUANTUM_TOKEN ç’°å¢ƒè®Šæ•¸")
